<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BENULL</title>
  
  <subtitle>tomorrow is another day</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-12-15T20:11:31.332Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>BENULL</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Feedback Graph Convolutional Network for Skeleton-based Action Recognition</title>
    <link href="http://yoursite.com/2020/12/16/FGCN/"/>
    <id>http://yoursite.com/2020/12/16/FGCN/</id>
    <published>2020-12-15T20:12:22.000Z</published>
    <updated>2020-12-15T20:11:31.332Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Feedback-Graph-Convolutional-Network-for-Skeleton-based-Action-Recognition"><a href="#Feedback-Graph-Convolutional-Network-for-Skeleton-based-Action-Recognition" class="headerlink" title="Feedback Graph Convolutional Network for Skeleton-based Action Recognition"></a>Feedback Graph Convolutional Network for Skeleton-based Action Recognition</h1><blockquote><p>作者          | Hao Yang, Dan Yan<br>单位          | NUCTECH Company Limited<br>论文地址 ｜<a href="https://arxiv.org/abs/2003.07564" target="_blank" rel="noopener">https://arxiv.org/abs/2003.07564</a></p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    近年来，许多学者利用图卷积网络（GCN）对骨架序列进行端到端优化建模。然而，传统的gcn是前馈网络，浅层不能访问到深层的语义信息，在这篇论文中，提出一个新的网络，称为<strong>反馈图卷积网络（FGCN）</strong></p><p>这是<strong>首次</strong>将反馈机制引入GCNs和动作识别中</p><p><strong>与传统的gcn相比，FGCN具有以下优点</strong></p><ol><li><p>设计了一种多阶段的时间采样策略，以从粗到精的渐进过程提取动作识别的时空特征</p></li><li><p>提出了一种基于稠密连接的反馈图卷积块（FGCB）来引入反馈连接,它将高层语义特征传递到底层，并逐级传递时间信息，逐步建立全局时空特征模型，用于动作识别</p></li><li><p>FGCN模型提供了早期预测。在早期阶段，模型接收到关于动作的部分信息。当然，它的预测相对粗糙。将粗预测视为先验知识，指导后期特征学习，实现精确预测</p></li></ol><p>在NTU-RGB+D、NTU-RGB+D120和Northwestern-UCLA的数据集上进行了大量的实验，结果表明所提出的FGCN对动作识别是有效的。它在三个数据集上达到了最先进的性能</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>​    近年来，从不同终端上传的视频数量已经增加。这推动了对基于视频内容的人类行为分析的迫切需求。尤其是与RGB和光流等其他模式相比，骨架人体行为识别因其对动态环境和复杂背景的影响具有较强的适应性而吸引了许多计算机视觉研究者。早期使用骨骼进行动作识别的深度学习方法通常将骨骼数据表示为关节坐标向量序列或伪图像，然后分别由RNN或CNN建模</p><p>​    然而，这些方法并没有显式地利用相关关节之间的空间依赖性，即使空间依赖性对于理解人类行为是有用的。最近，一些方法根据连续帧的自然连接和时间边缘来构造时空图。然后他们利用GCN来模拟时空特征。然而，<strong>传统的gcn都是单个前馈网络</strong>，由整个骨架序列当作输入。这些方法很难提取出有效的时空特征，因为这些有用的信息通常被隐藏在与运动无关或未区分的片段中。例如，在“踢某物”动作中，大多数片段是“直立站立”，而在“穿鞋”动作中，大多数片段都是坐在椅子上。因此，对于低层，单通前馈网络无法访问深层语义信息。同时，输入整个骨架序列增加了模型的计算复杂度。</p><p>​    基于这一点，提出了一种新的神经网络，称为反馈图卷积网络(FGCN)，以粗到精的渐进过程从骨架数据中提取有效的时空特征，用于动作识别。<strong>FGCN是第一个将反馈机制引入GCNs和动作识别的工作</strong>。与传统的gcn相比，FGCN具有多阶段的时间采样策略，该策略将输入的骨架序列在时域内分为多个阶段，并从时域对输入的骨架片段进行稀疏采样，避免了整个骨架序列的输入。对每一级输入的空时图像进行局部卷积提取。提出了一种基于反馈图卷积块（FGCB）融合局部特征的全局时空特征建模方法。FGCB是一个局部稠密图卷积网络，每个级到下一级都有横向连接，它将反馈连接引入到传统的gcn中。从语义角度看，它是自上而下的工作方式，这使得低层卷积层能够在每个阶段访问高层的语义信息。在时域上，FGCB的反馈机制具有一系列因果关系，前一级的输出流入下一级，以调节其输入。</p><p>​    <strong>FGCN的另一个优点是它可以在总推理时间的一小部分时间内提供输出的早期预测</strong>。这在许多应用中都很有价值，例如机器人或自动驾驶，在这些应用中，延迟时间是非常关键的。早期预测是所提出的多阶段从粗到细逐步优化的结果。在早期阶段，FGCN只提供了一部分骨架序列，而且有关该行为的信息有限，因此其推断相对粗糙。这些推理被视为在以后阶段指导特征学习的先验知识。在后期阶段，该模型接收到更完整的行为信息和先前推理的引导者信息，从而输出更精确的推理。提出了几种时域融合策略，将局部预测融合到视频级预测中。这些策略使网络在渐进过程中得到优化。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="反馈图卷积网络"><a href="#反馈图卷积网络" class="headerlink" title="反馈图卷积网络"></a>反馈图卷积网络</h3><p>​    传统的基于GCNs的动作识别方法都是在一个前馈网络中输入整个骨架序列。然而，当输入整个骨架序列时，有用的信息通常隐藏在与运动无关且无差别的片段中。单通前馈网络不能在浅层访问语义信息。为了解决这些问题，提出了一种反馈图卷积网络（FGCN），该网络通过多级递进过程提取时空特征。具体地说，FGCN设计了一种多阶段的时间采样策略来从骨架数据中稀疏地采样一系列输入片段，而不是直接对整个骨架序列进行操作。这些片段首先被输入到图的卷积层中以提取局部时空特征。然后，提出了一种反馈图卷积块（FGCB），通过将前一级的高级信息传输到下一级来调制其输入，从而融合来自多个时间阶段的局部时空特征。最后，提出了几种时间融合策略，将所有时间阶段的局部预测进行融合，给出一个视频级的预测。</p><p><img src="https://img-blog.csdnimg.cn/20201216040747929.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201216040804695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="反馈图卷积块（FGCB"><a href="#反馈图卷积块（FGCB" class="headerlink" title="反馈图卷积块（FGCB)"></a>反馈图卷积块（FGCB)</h3><p>​    反馈模块FGCB是FGCN模型的核心部分。一方面，FGCB将高层语义信息传回低层，以细化其编码特征。另一方面，前一级的输出流入下一级，以调节其输入。为了使FGCB能够有效地将信息从高层传输到低层，以及从前一个阶段传输到下一个阶段，提出了一个密集连接的局部图卷积网络，它增加了从每一层到所有后续层的连接</p><p><img src="https://img-blog.csdnimg.cn/20201216040814776.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>设计了四个消融实验来评估不同超参数、结构和输入对FGCN模型性能的影响。这些消融实验都是在NTU-RGB+D上进行的</p><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><p>​    所有实验均采用PyTorch深度学习框架实现。训练过程中采用随机梯度下降（SGD）优化器，batch-size为32，momentum为0.9，初始学习率为0.1。在第40和60 个epoch，学习率除以10。训练过程在第80 epoch结束</p><p>​    输入的视频在时间上分为五个阶段，每个阶段随机抽取64个连续的帧组成一个输入片段。十个图卷积层堆叠在反馈块FGCB的前面，这些层具有与ST-GCN中的图卷积层相同的配置。FGCB有四个图形卷积层（即L=4）,将它们的时空核大小和输出通道分别设置为ks=3、kt=3和m=256<br><img src="https://img-blog.csdnimg.cn/20201216040830492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="请添加图片描述"><br><img src="https://img-blog.csdnimg.cn/20201216040830441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="请添加图片描述"><br><img src="https://img-blog.csdnimg.cn/20201216040830561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="请添加图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Feedback-Graph-Convolutional-Network-for-Skeleton-based-Action-Recognition&quot;&gt;&lt;a href=&quot;#Feedback-Graph-Convolutional-Network-for-Skele
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="ActionRecognition" scheme="http://yoursite.com/tags/ActionRecognition/"/>
    
  </entry>
  
  <entry>
    <title>Temporal Extension Module for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/2020/12/10/TEM/"/>
    <id>http://yoursite.com/2020/12/10/TEM/</id>
    <published>2020-12-09T22:42:22.000Z</published>
    <updated>2020-12-09T22:35:28.561Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Temporal-Extension-Module-for-Skeleton-Based-Action-Recognition"><a href="#Temporal-Extension-Module-for-Skeleton-Based-Action-Recognition" class="headerlink" title="Temporal Extension Module for Skeleton-Based Action Recognition"></a>Temporal Extension Module for Skeleton-Based Action Recognition</h1><blockquote><p>作者 | Yuya Obinata and Takuma Yamamoto<br>单位         | FUJITSU LABORATORIES LTD<br>论文地址｜<a href="https://arxiv.org/abs/2003.08951" target="_blank" rel="noopener">https://arxiv.org/abs/2003.08951</a><br>ICPR2020</p></blockquote><p>在st-gcn的基础上开发了一个模块，在帧与帧之间相邻的关节点之间也添加连接，好处在于和其他网络结合起来很方便，对于性能也有一定的提高</p><p>但其实很多sota的模型都已经考虑过帧间关节点的链接，同时扩展到了多个尺度，不仅仅是帧间邻居关节点相连</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>提出了一个用于基于骨架的动作识别的的时域扩展模块</p><p><strong>现有的方法试图在帧内表示更合适的空间图，但忽略了帧间时间图的优化</strong></p><p>具体来说，这些方法只连接帧间同一关节对应的顶点。在这篇论文中，着重于在帧间添加与相邻多个顶点的连接</p><p>是提取人体运动中多个关节的相关特征的一种简单而有效的方法</p><p>主要贡献如下</p><ul><li>提出了一个时间扩展模块，用于帧间时态图的扩展。该模块在提取人体运动中连接的多个相邻关节的相关特征时简单而有效。</li><li>在消融实验中的展示了TEM有效性</li><li>达到了SOTA</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>时空图卷积网络（ST-GCN）是第一个使用GCN对骨架序列进行动作识别的方法</p><p><img src="https://img-blog.csdnimg.cn/2020121006314185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>ST-GCN包括一个空间图和一个时间图，直接输入一个骨架序列，并从帧内和帧间的关节处提取特征</p><p>在这项工作中，着重于扩展时间图，连接相邻的多个顶点以及帧间（右）上的同一个顶点。在帧内提出了更合适的空间图，性能得到了显著提高。然而，这些方法忽略了帧间时间图的优化</p><p>传统的GCN方法将只对应于同一关节的顶点之间的时间维连接起来，该方法对于提取同一关节轨迹特征具有一定的效果，然而，由于过于简单不太能够提取帧间各关节间相关运动的特征</p><p><strong>研究目标</strong><br>优化空间图和时间图，以进一步提高性能<br>TEM模块不仅直接将边添加到同一个顶点，而且还直接向相邻的多个顶点添加边，并基于帧间相同的多个顶点计算卷积</p><h2 id="时间扩展模块"><a href="#时间扩展模块" class="headerlink" title="时间扩展模块"></a>时间扩展模块<img src="https://img-blog.csdnimg.cn/20201210063210320.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h2><p>在ST-GCN中加入了这个方法的模块，ST-GCN包括多层时空图卷积操作，将TEM放在空间卷积和时间卷积层中</p><p>以同样的方式很容易地TEM模块实现到基于时空图卷积的网络中</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验<img src="https://img-blog.csdnimg.cn/20201210063232136.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h2><p>加上这个模块后有一定的性能提升</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Temporal-Extension-Module-for-Skeleton-Based-Action-Recognition&quot;&gt;&lt;a href=&quot;#Temporal-Extension-Module-for-Skeleton-Based-Action-Recog
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="ActionRecognition" scheme="http://yoursite.com/tags/ActionRecognition/"/>
    
  </entry>
  
  <entry>
    <title>Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting</title>
    <link href="http://yoursite.com/2020/12/09/CRA%20_for_image_inpanting/"/>
    <id>http://yoursite.com/2020/12/09/CRA%20_for_image_inpanting/</id>
    <published>2020-12-09T10:02:22.000Z</published>
    <updated>2020-12-09T10:02:22.948Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Contextual-Residual-Aggregation-for-Ultra-High-Resolution-Image-Inpainting"><a href="#Contextual-Residual-Aggregation-for-Ultra-High-Resolution-Image-Inpainting" class="headerlink" title="Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting"></a>Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting</h1><blockquote><p>作者 | Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, Zhan Xu<br>单位 | 华为技术有限公司（加拿大）<br>代码 | <a href="https://github.com/Ascend-Huawei/Ascend-Canada/tree/master/Models/Research_HiFIll_Model" target="_blank" rel="noopener">https://github.com/Ascend-Huawei/Ascend-Canada/tree/master/Models/Research_HiFIll_Model</a><br>论文地址｜<a href="https://arxiv.org/abs/2005.09704" target="_blank" rel="noopener">https://arxiv.org/abs/2005.09704</a><br>备注 | CVPR 2020 Oral </p></blockquote><h2 id="图像修复"><a href="#图像修复" class="headerlink" title="图像修复"></a>图像修复</h2><p>自动填充图像中缺失部分</p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul><li>调整目标位置</li><li>移除不想要的元素</li><li>修复损坏的图像</li></ul><p><img src="https://img-blog.csdnimg.cn/2020120917523752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="当前的方法"><a href="#当前的方法" class="headerlink" title="当前的方法"></a>当前的方法</h3><ol><li><p>通过复制来填充</p><ul><li><p>从缺失部分附近“借”像素来进行填充</p></li><li><p>e.g., PatchMatch, diffusion-based</p></li></ul></li><li>通过建模来填充<ul><li>数据驱动的方式来学习缺失的像素</li><li>e.g., PixelRNN,FCN</li></ul></li><li>结合上面两种<ul><li>e.g., DeepFill, Patch-Swap</li><li>这篇文章的方法</li></ul></li></ol><h4 id="当前基于学习的方法的不足"><a href="#当前基于学习的方法的不足" class="headerlink" title="当前基于学习的方法的不足"></a>当前基于学习的方法的不足</h4><ul><li>不能够去处理高分辨率图像<ul><li>训练困难</li><li>GPU/NPU内存的限制</li><li>缺少高分辨率的训练数据集</li></ul></li></ul><p><img src="https://img-blog.csdnimg.cn/20201209175251686.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="论文方法"><a href="#论文方法" class="headerlink" title="论文方法"></a>论文方法</h2><p>提出了一种上下文残差聚合（CRA）机制，该机制可以通过对上下文补丁中的残差进行加权聚合来生成丢失内容的高频残差，因此网络的训练仅需要低分辨率即可</p><p>由于神经网络的卷积层仅需要在低分辨率的输入和输出上进行操作，因此降低了内存和计算能力的成本</p><p>此外，还减轻了对高分辨率训练数据集的需求</p><p><strong>通过3阶段的pipeline实现高分辨率图像的修复</strong></p><ol><li><p>由生成器（Generator）得到低分辨率的修补好的图像</p></li><li><p>通过残差聚合模块得到高频残差</p></li><li><p>合并高频残差和低分辨率修补结果得到高分辨率修补图像</p></li></ol><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://img-blog.csdnimg.cn/20201209175306706.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="生成器（Generator）"><a href="#生成器（Generator）" class="headerlink" title="生成器（Generator）"></a>生成器（Generator）</h4><p><strong>两阶段的coarse-to-fine网络</strong></p><p>coarse network输入下采样到256×256的带mask图像，会产生粗略的缺失内容</p><p>fine network 通过<strong>Attention Computing Module (ACM)</strong>和<strong>Attention Transfer Module (ATM)</strong>得到缺失部分内外的关系得分，输出512×512的修复结果</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://img-blog.csdnimg.cn/20201209175321384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>这篇论文的方法在图片分辨率大于1K的情况下修复效率和质量达到了最好</strong></p><p><img src="https://img-blog.csdnimg.cn/20201209175343504.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="使用预训练好模型的测试结果"><a href="#使用预训练好模型的测试结果" class="headerlink" title="使用预训练好模型的测试结果"></a>使用预训练好模型的测试结果</h4><p>结果在缺失部分很大，且上下文环境复杂的情况下，效果看起来并没有很好</p><p>在背景单一的风景照中效果很不错<br><img src="https://img-blog.csdnimg.cn/20201209175353108.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>提出了一种新颖的上下文残留聚合技术，可对超高分辨率图像进行更高效和高质量的修复</p></li><li><p>把大图下采样到 512×512 ，在分辨率为512×512的小图像上进行图像修复，然后在高分辨率图像上进行推理得到修复效果良好的大图</p></li><li>与其他数据驱动方法不同，分辨率和孔尺寸的增加不会降低修补质量，也不会显着增加我们框架中的处理时间</li><li>到目前为止，是唯一能够在超高分辨率图像(4K至8K)上进行端到端修复的基于学习的技术</li></ul><p>  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Contextual-Residual-Aggregation-for-Ultra-High-Resolution-Image-Inpainting&quot;&gt;&lt;a href=&quot;#Contextual-Residual-Aggregation-for-Ultra-High
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="ImageInpainting" scheme="http://yoursite.com/tags/ImageInpainting/"/>
    
  </entry>
  
  <entry>
    <title>K-means聚类算法</title>
    <link href="http://yoursite.com/2020/11/25/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2020/11/25/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</id>
    <published>2020-11-25T10:34:22.000Z</published>
    <updated>2020-11-25T10:45:24.703Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K-means聚类算法"><a href="#K-means聚类算法" class="headerlink" title="K-means聚类算法"></a>K-means聚类算法</h1><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><blockquote><p>在无监督学习中, 训练样本的标记信息是未知的, 目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律, 为进一步的数据分析提供基础，此类学习任务中研究最多、应用最广的是“聚类”(clustering)</p></blockquote><p>聚类试图将数据集中的样本划分为若干个通常是不相交的子集, 每个子集称为一个“簇”(cluster)</p><p>通过这样的划分, 每个簇可能对应于一些潜在的概念(类别), 需说明的是, 这些概念对聚类算法而言事先是未知的, 聚类过程仅能自动形成簇结构, 簇所对应的概念语义需由使用者来把握</p><p>关于簇的完整定义尚未达成共识，传统的定义如下</p><ul><li><p>同一簇中的实例必须尽可能相似</p></li><li><p>不同簇中的实例必须尽可能不同</p></li><li><p>相似度和相异度的度量必须清楚并具有实际意义</p></li></ul><h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>聚类性能度量大致有两类</p><ul><li><p>将聚类结果与某个参考模型进行比较, 称为“外部指标” (external index)</p></li><li><p>直接考察聚类结果而不利用任何参考模型, 称为“内部指标” (internalindex)</p></li></ul><h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>对数据集 <script type="math/tex">D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\},</script> 假定通过聚类给出的族划分为 <script type="math/tex">\mathcal{C}=\left\{C_{1},C_{2}, \ldots, C_{k}\right\},</script> 参考模型给出的族划分为 <script type="math/tex">\mathcal{C}^{*}=\left\{C_{1}^{*}, C_{2}^{*}, \ldots, C_{s}^{*}\right\} .</script> 相应地, 令 <script type="math/tex">\boldsymbol{\lambda}</script> 与<script type="math/tex">\lambda^{*}</script> 分别表示与 <script type="math/tex">\mathcal{C}</script> 和 <script type="math/tex">\mathcal{C}^{*}</script> 对应的族标记向量. 我们将样本两两配对考虑, 定义</p><script type="math/tex; mode=display">\begin{aligned}a &\left.=|S S|, \quad S S=\left\{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \mid \lambda_{i}=\lambda_{j}, \lambda_{i}^{*}=\lambda_{j}^{*}, i<j\right)\right\} \\b &\left.=|S D|, \quad S D=\left\{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \mid \lambda_{i}=\lambda_{j}, \lambda_{i}^{*} \neq \lambda_{j}^{*}, i<j\right)\right\} \\c &\left.=|D S|, \quad D S=\left\{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \mid \lambda_{i} \neq \lambda_{j}, \lambda_{i}^{*}=\lambda_{j}^{*}, i<j\right)\right\} \\d &\left.=|D D|, \quad D D=\left\{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \mid \lambda_{i} \neq \lambda_{j}, \lambda_{i}^{*} \neq \lambda_{j}^{*}, i<j\right)\right\}\end{aligned}</script><p>其中集合 <script type="math/tex">SS</script> 包含了在 <script type="math/tex">\mathcal{C}</script> 中隶属于相同族且在 <script type="math/tex">\mathcal{C}^{*}</script> 中也隶属于相同族的样本</p><p>集合 <script type="math/tex">SD</script> 包含了在 <script type="math/tex">\mathcal{C}</script> 中隶属于相同族但在 <script type="math/tex">\mathcal{C}^{*}</script> 中隶属于不同族的样本</p><p>集合 <script type="math/tex">DS</script> 包含了在 <script type="math/tex">\mathcal{C}</script> 中隶属于 不同族但在 <script type="math/tex">\mathcal{C}^{*}</script> 中隶属于相同族的样本</p><p>集合 <script type="math/tex">DD</script> 包含了在 <script type="math/tex">\mathcal{C}</script> 中隶属于不同族在 <script type="math/tex">\mathcal{C}^{*}</script> 中也隶属于不同族的样本</p><p>可计算出下面这些聚类性能度量外部指标</p><ul><li>Jaccard系数</li></ul><script type="math/tex; mode=display">\mathrm{JC}=\frac{a}{a+b+c}</script><ul><li><p>FM指数（Fowlkes and Mallows Index,FMI）</p><script type="math/tex; mode=display">\mathrm{FMI}=\sqrt{\frac{a}{a+b} \cdot \frac{a}{a+c}}</script></li><li><p>Rand指数(Rand Index,RI)</p><script type="math/tex; mode=display">\mathrm{RI}=\frac{2(a+d)}{m(m-1)}</script></li></ul><p>显然上述性能度量的结果值均在[0，1]区间，值越大越好</p><h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>考虑聚类结果的族划分 <script type="math/tex">\mathcal{C}=\left\{C_{1}, C_{2}, \ldots, C_{k}\right\},</script> 定义</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{avg}(C) &=\frac{2}{|C|(|C|-1)} \sum_{1 \leqslant i<j \leqslant|C|} \operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \\\operatorname{diam}(C) &=\max _{1 \leqslant i<j \leqslant|C|} \operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \\d_{\min }\left(C_{i}, C_{j}\right) &=\min _{\boldsymbol{x}_{i} \in C_{i}, \boldsymbol{x}_{j} \in C_{j}} \operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \\d_{\operatorname{cen}}\left(C_{i}, C_{j}\right) &=\operatorname{dist}\left(\boldsymbol{\mu}_{i}, \boldsymbol{\mu}_{j}\right)\end{aligned}</script><p>其中dist <script type="math/tex">(\cdot, \cdot)</script> 用于计算两个样本之间的距离<script type="math/tex">\boldsymbol{\mu}</script><br>族 <script type="math/tex">C</script> 的中心点 <script type="math/tex">\boldsymbol{\mu}=</script> <script type="math/tex">\frac{1}{|C|} \sum_{1 \leqslant i \leqslant|C|} \boldsymbol{x}_{i}</script></p><p>族 <script type="math/tex">C</script> 内样本间的平均距离<script type="math/tex">\operatorname{avg}(C)</script> </p><p>族 <script type="math/tex">C</script> 内样本间的最远距离<script type="math/tex">\operatorname{diam}(C)</script> </p><p>对应于族 <script type="math/tex">C_{i}</script> 与族 <script type="math/tex">C_{j}</script> 最近样本间的距离 <script type="math/tex">d_{\min }\left(C_{i}, C_{j}\right)</script></p><p>对应于族 <script type="math/tex">C_{i}</script> 与族 <script type="math/tex">C_{j}</script> 中心点间的距离<script type="math/tex">d_{\mathrm{cen}}\left(C_{i}, C_{j}\right)</script> </p><p>可计算出下面这些聚类性能度量内部指标</p><ul><li>DB 指数(Davies-Bouldin Index, DBI)<script type="math/tex; mode=display">\mathrm{DBI}=\frac{1}{k} \sum_{i=1}^{k} \max _{j \neq i}\left(\frac{\operatorname{avg}\left(C_{i}\right)+\operatorname{avg}\left(C_{j}\right)}{d_{\mathrm{cen}}\left(\mu_{i}, \mu_{j}\right)}\right)</script></li><li>Dunn 指数(Dunn Index, DI)<script type="math/tex; mode=display">\mathrm{DI}=\min _{1 \leqslant i \leqslant k}\left\{\min _{j \neq i}\left(\frac{d_{\min }\left(C_{i}, C_{j}\right)}{\max _{1 \leqslant l \leqslant k} \operatorname{diam}\left(C_{l}\right)}\right)\right\}</script>显然, DBI 的值越小越好, 而 DI 则相反, 值越大越好</li></ul><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>可参考<a href="https://blog.csdn.net/qq_32815807/article/details/109172509" target="_blank" rel="noopener">k-近邻算法（KNN）</a>中距离度量部分</p><h3 id="常见的聚类算法"><a href="#常见的聚类算法" class="headerlink" title="常见的聚类算法"></a>常见的聚类算法</h3><h4 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h4><p>原型聚类亦称“基于原型的聚类” (prototype-based clustering)</p><p>假设聚类结构能通过一组原型（指样本空间中具有代表性的点）刻画, 通常情形下，算法先对原型进行初始化, 然后对原型进行迭代更新求解. 采用不同的原型表示、不同的求解方式将产生不同的算法，下面是几种著名的原型聚类算法</p><ul><li>k-means</li><li>学习向量量化（Learning Vector Quantization,LVQ）</li><li>高斯混合聚类（Mixture-of-Guassian）</li></ul><h4 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h4><p>层次聚类(hierarchical clustering)试图在不同层次对数据集进行划分，从而形成树形的聚类结构</p><p>数据集的划分可采用“自底向上”的聚合策略，它将最相似的两个点合并，直到所有点都合并到一个群集中为止</p><p>也可采用“自顶向下”的分拆策略，它以所有点作为一个簇开始，并在每一步拆分相似度最低的簇，直到仅剩下单个数据点</p><p>在分层聚类中，聚类数（k）通常由用户预先确定，通过在指定深度切割树状图来分配聚类，从而得到k组较小的树状图</p><p>与许多分区聚类技术不同，分层聚类是<strong>确定性</strong>过程，这意味着当您对相同的输入数据运行两次算法时，聚类分配不会改变</p><p>层次聚类方法的<strong>优点</strong>包括</p><ul><li>它们通常会揭示有关数据对象之间的更详细的信息。</li><li>它们提供了<strong>可解释的树状图</strong>。</li></ul><p>层次聚类方法的<strong>缺点</strong>包括</p><ul><li>算法复杂度高</li><li>对<strong>噪音</strong>和<strong>异常值</strong>很敏感</li></ul><h4 id="基于密度的聚类"><a href="#基于密度的聚类" class="headerlink" title="基于密度的聚类"></a>基于密度的聚类</h4><p>基于密度的聚类根据区域中数据点的密度确定聚类分配,在高密度的数据点被低密度区域分隔的地方分配簇</p><p>与其他类别的聚类不同，该方法不需要用户指定群集数量，而是有一个基于距离的参数充当可调阈值来确定是否将接近的点视为簇成员</p><p>DBSCAN是一种著名的密度聚类算法，它基于一组领域参数来刻画样本分布的紧密程度</p><p>基于密度的聚类方法的<strong>优点</strong>包括</p><ul><li>他们擅长识别<strong>非类圆型</strong>簇。</li><li>它们可以抵抗<strong>异常值</strong>。</li></ul><p>基于密度的聚类方法的<strong>缺点</strong>包括</p><ul><li>它们不太适合在<strong>高维空间中聚类</strong></li><li>很难确定<strong>密度不同的</strong>簇。</li></ul><h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><p>k均值聚类算法（k-means clustering algorithm）是一种迭代求解的聚类分析算法</p><p>它试图将数据集划分为K个不同的非重叠子组(簇)，其中每个数据点只属于一个组</p><p>同时使得簇内数据点尽可能相似，还要尽可能保持簇之间的差异</p><p>聚类分配的质量是通过计算质心<strong>收敛</strong>后的平方误差和（sum of the squared error,SSE）来确定的，或者与先前的迭代分配相符</p><p>SSE定义为每个点与其最接近的质心的欧几里德距离平方的总和，k-means的目的是尝试最小化该值</p><p>下图显示了在同一数据集上两次不同的<em>k</em> -means算法运行的前五个迭代中的质心和SSE更新：</p><p><img src="https://img-blog.csdnimg.cn/20201125183206829.gif#pic_center" alt="在这里插入图片描述"></p><p>此图的目的是表明质心的初始化是重要的一步，随机初始化步骤导致<em>k</em> -means算法<strong>不确定</strong>，这意味着如果您在同一数据集上运行两次相同的算法，则簇分配将有所不同</p><p>研究人员通常会对整个<em>k</em>均值算法进行几次初始化，并从具有最低SSE的初始化</p><h3 id="工作方式"><a href="#工作方式" class="headerlink" title="工作方式"></a>工作方式</h3><ol><li>指定簇数K</li><li>首先对数据集shuffle，然后为质心随机选择<em>K个</em>数据点</li><li>不断迭代，直到质心没有变化，即数据点分配给群集的操作没有改变<ul><li>计算数据点和所有质心之间的平方距离之和</li><li>将每个数据点分配给最近的群集（质心）</li><li>通过获取属于每个群集的所有数据点的平均值，计算群集的质心</li></ul></li></ol><p>k-means解决问题的方法称为期望最大化（Expectation Maximization,EM）<br>目标函数是</p><script type="math/tex; mode=display">E=\sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_{i}}\left\|\boldsymbol{x}-\boldsymbol{\mu}_{i}\right\|_{2}^{2}</script><p>其中 <script type="math/tex">\mu_{i}=\frac{1}{|C_{i}|} \sum_{x \in C_{i}} x</script> 是族 <script type="math/tex">C_{i}</script> 的均值向量<br>最小化目标函数并不容易, 找到它的最优解需考察样本集 <script type="math/tex">D</script> 所有可能的族划分, 这是一个 <script type="math/tex">\mathrm{NP}</script> 难问题</p><p>因此k-means算法采用了贪心策略, 通过迭代优化来近似求解</p><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><ul><li>类别数 <script type="math/tex">k</script> 值需要预先指定，而在实际应用中最优的 <script type="math/tex">k</script> 值是不知道的,解决这个问题的一个方法是尝试用不同的 <script type="math/tex">k</script> 值聚类, 检验各自得到聚类结果的质量, 推测最优的 <script type="math/tex">k</script> 值</li><li><p>由于包括kmeans在内的聚类算法使用基于距离的度量来确定数据点之间的相似性，因此建议对数据进行标准化</p></li><li><p>由于kmeans算法可能停留在局部最优而不收敛于全局最优，因此不同的初始化可能导致不同的聚类，建议使用不同的质心初始化来运行算法，并选择产生较低SSE的运行结果</p></li></ul><h3 id="找到合适的K"><a href="#找到合适的K" class="headerlink" title="找到合适的K"></a>找到合适的K</h3><p>下面将介绍两个指标，这些指标可能使我们更直接的观察k的取值</p><h4 id="肘部法则（Elbow-Method"><a href="#肘部法则（Elbow-Method" class="headerlink" title="肘部法则（Elbow Method)"></a>肘部法则（Elbow Method)</h4><p>k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散</p><p>畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点</p><p> <img src="https://img-blog.csdnimg.cn/2020112518295967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>存在当曲线是单调递减的时仍然很难找出合适的簇数的可能</p><h4 id="轮廓系数（Silhouette-Coefficient）"><a href="#轮廓系数（Silhouette-Coefficient）" class="headerlink" title="轮廓系数（Silhouette Coefficient）"></a>轮廓系数（Silhouette Coefficient）</h4><p>轮廓系数是聚类结合和分离程序的评价指标，它基于两个因素来量化数据点适合其分配的簇的程度</p><ol><li><p>数据点与簇中其他点的距离有多近</p></li><li><p>数据点与其他簇中的点有多远</p></li></ol><p>计算距同一簇中所有数据点的平均距离<script type="math/tex">a_i</script><br>计算到最近的簇中所有数据点的平均距离<script type="math/tex">b_i</script></p><script type="math/tex; mode=display">\frac{b^{i}-a^{i}}{\max \left(a^{i}, b^{i}\right)}</script><p>轮廓系数值介于-1和1之间，越接近1表示样本所在簇合理</p><p>若近似为0，则说明样本在两个簇的边界上</p><p><img src="https://img-blog.csdnimg.cn/2020112518301011.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>如果簇是类圆形的，那么Kmeans算法是可以胜任的，它总是尝试在质心周围构造一个不错的球形</p><p>但这这意味着，当群集具有复杂的几何形状时，Kmeans的表现不好，如下<br><img src="https://img-blog.csdnimg.cn/20201125183024585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>不出所料，kmeans无法找出两个数据集的正确聚类</p><p>但是，如果我们使用核方法，将其转换为高维表示从而使数据线性可分离，则可以帮助kmeans完美地聚类此类数据集<br><img src="https://img-blog.csdnimg.cn/20201125183139392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding = utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMeans</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,n_clusters, max_iters=<span class="number">100</span>, random_state=<span class="number">666</span>)</span>:</span></span><br><span class="line">        <span class="string">"""初始化Kmeans模型"""</span></span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.max_iters = max_iters</span><br><span class="line">        self.random_state = random_state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initializ_centroids</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        np.random.RandomState(self.random_state)</span><br><span class="line">        random_idx = np.random.permutation(X.shape[<span class="number">0</span>])</span><br><span class="line">        centroids = X[random_idx[:self.n_clusters]]</span><br><span class="line">        <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_centroids</span><span class="params">(self, X, labels)</span>:</span></span><br><span class="line">        centroids = np.zeros((self.n_clusters, X.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">            centroids[k, :] = np.mean(X[labels == k, :], axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distance</span><span class="params">(self, X, centroids)</span>:</span></span><br><span class="line">        distance = np.zeros((X.shape[<span class="number">0</span>], self.n_clusters))</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">            row_norm = np.linalg.norm(X - centroids[k, :], axis=<span class="number">1</span>)</span><br><span class="line">            distance[:, k] = np.square(row_norm)</span><br><span class="line">        <span class="keyword">return</span> distance</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find_closest_cluster</span><span class="params">(self, distance)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.argmin(distance, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_sse</span><span class="params">(self, X, labels, centroids)</span>:</span></span><br><span class="line">        distance = np.zeros(X.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">            distance[labels == k] = np.linalg.norm(X[labels == k] - centroids[k], axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> np.sum(np.square(distance))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        self.centroids = self.initializ_centroids(X)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.max_iters):</span><br><span class="line">            old_centroids = self.centroids</span><br><span class="line">            distance = self.compute_distance(X, old_centroids)</span><br><span class="line">            self.labels = self.find_closest_cluster(distance)</span><br><span class="line">            self.centroids = self.compute_centroids(X, self.labels)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> np.all(old_centroids == self.centroids):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        self.error = self.compute_sse(X, self.labels, self.centroids)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_predict)</span>:</span></span><br><span class="line">        distance = self.compute_distance(X, old_centroids)</span><br><span class="line">        <span class="keyword">return</span> self.find_closest_cluster(distance)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">    <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">    features, true_labels = make_blobs(</span><br><span class="line">        n_samples=<span class="number">200</span>,</span><br><span class="line">        n_features=<span class="number">2</span>,</span><br><span class="line">        centers=<span class="number">3</span>,</span><br><span class="line">        cluster_std=<span class="number">2.75</span>,</span><br><span class="line">        random_state=<span class="number">42</span></span><br><span class="line">    )</span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    scaled_features = scaler.fit_transform(features)</span><br><span class="line"></span><br><span class="line">    kmeans = KMeans(</span><br><span class="line">        n_clusters=<span class="number">3</span>,</span><br><span class="line">        max_iters=<span class="number">300</span>,</span><br><span class="line">        random_state=<span class="number">42</span></span><br><span class="line">    )</span><br><span class="line">    kmeans.fit(scaled_features)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>机器学习-周志华</p><p><a href="https://realpython.com/k-means-clustering-python/#writing-your-first-k-means-clustering-code-in-python" target="_blank" rel="noopener">K-Means Clustering in Python: A Practical Guide</a></p><p><a href="https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a" target="_blank" rel="noopener">K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;K-means聚类算法&quot;&gt;&lt;a href=&quot;#K-means聚类算法&quot; class=&quot;headerlink&quot; title=&quot;K-means聚类算法&quot;&gt;&lt;/a&gt;K-means聚类算法&lt;/h1&gt;&lt;h2 id=&quot;聚类&quot;&gt;&lt;a href=&quot;#聚类&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="Algorithms" scheme="http://yoursite.com/tags/Algorithms/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="KMeans" scheme="http://yoursite.com/tags/KMeans/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy豆瓣搜索页爬虫</title>
    <link href="http://yoursite.com/2020/11/23/Scrapy%E8%B1%86%E7%93%A3%E6%90%9C%E7%B4%A2%E9%A1%B5%E7%88%AC%E8%99%AB/"/>
    <id>http://yoursite.com/2020/11/23/Scrapy%E8%B1%86%E7%93%A3%E6%90%9C%E7%B4%A2%E9%A1%B5%E7%88%AC%E8%99%AB/</id>
    <published>2020-11-23T10:07:22.000Z</published>
    <updated>2020-11-23T10:07:20.812Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Scrapy-豆瓣搜索页爬虫"><a href="#Scrapy-豆瓣搜索页爬虫" class="headerlink" title="Scrapy 豆瓣搜索页爬虫"></a>Scrapy 豆瓣搜索页爬虫</h1><p>使用scrapy爬虫框架对豆瓣图书搜索结果进行爬取</p><h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架</p><p>可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序</p><p>它提供了多种类型爬虫的基类，如BaseSpider、CrawlSpider等</p><h3 id="主要组件"><a href="#主要组件" class="headerlink" title="主要组件"></a>主要组件</h3><p>Scrapy框架主要由<strong>五大组件</strong>组成</p><ol><li><p><strong>调度器(Scheduler)</strong><br> 调度器，说白了把它假设成为一个URL的优先队列，由它来决定下一个要抓取的网址是什么，同时去除重复的网址，用户可以自己的需求定制调度器。</p></li><li><p><strong>下载器(Downloader)</strong><br> 下载器，是所有组件中负担最大的，它用于高速地下载网络上的资源<br> Scrapy的下载器代码不会太复杂，但效率高，主要的原因是Scrapy下载器是建立在twisted这个高效的        异步模型上的</p></li><li><p><strong>爬虫(Spider)</strong></p><p> 爬虫，是用户最关心的部份。用户定制自己的爬虫(通过定制正则表达式等语法)，用于从特定的网页中提取自己需要的信息，即所谓的实体(Item)。 用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</p></li><li><p><strong>实体管道(Item Pipeline)</strong></p><p> 实体管道，用于处理爬虫(spider)提取的实体(Item)<br>主要的功能是持久化实体、验证实体的有效性、清除不需要的信息</p></li><li><p><strong>Scrapy引擎(Scrapy Engine)</strong></p><p> Scrapy引擎是整个框架的核心<br> 它用来控制调试器、下载器、爬虫。实际上，引擎相当于计算机的CPU,它控制着整个流程</p></li></ol><p><img src="https://img-blog.csdnimg.cn/20201123180151632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="数据流-Data-flow"><a href="#数据流-Data-flow" class="headerlink" title="数据流(Data flow)"></a>数据流(Data flow)</h3><p>Scrapy中的数据流由执行引擎控制，其过程如下:</p><ol><li>引擎打开一个网站，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)</li><li>引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度</li><li>引擎向调度器请求下一个要爬取的URL</li><li>调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(request方向)转发给下载器(Downloader)</li><li>一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(response方向)发送给引擎</li><li>引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理</li><li>Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎</li><li>引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器</li><li>(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站</li></ol><h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><blockquote><p>创建项目    <code>scrapy startproject xxx</code><br>创建爬虫    <code>scrapy genspider xxx（爬虫名） xxx.com （爬取域）</code><br>生成文件    <code>scrapy crawl xxx -o xxx.json (生成json/csv文件)</code><br>运行爬虫    <code>scrapy crawl XXX</code><br>列出所有爬虫    <code>scrapy list</code></p></blockquote><h3 id="scrapy项目目录结构"><a href="#scrapy项目目录结构" class="headerlink" title="scrapy项目目录结构"></a>scrapy项目目录结构</h3><p>通过命令<code>scrapy startproject tutorial</code>创建一个新的项目<code>tutorial</code></p><p>将会创建包含下列内容的 <code>tutorial</code> 目录</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tutorial/</span><br><span class="line">    scrapy.cfg    <span class="comment"># 项目的配置文件</span></span><br><span class="line">    tutorial/<span class="comment"># 该项目的python模块之后将在此加入代码</span></span><br><span class="line">        __init__.py</span><br><span class="line">        items.py<span class="comment"># 项目中的item文件</span></span><br><span class="line">        pipelines.py<span class="comment"># 项目中的pipelines文件</span></span><br><span class="line">        settings.py<span class="comment"># 项目的设置文件</span></span><br><span class="line">        spiders/<span class="comment"># 放置spider代码的目录</span></span><br><span class="line">            __init__.py</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure><h2 id="使用scrapy爬取豆瓣搜索页"><a href="#使用scrapy爬取豆瓣搜索页" class="headerlink" title="使用scrapy爬取豆瓣搜索页"></a>使用scrapy爬取豆瓣搜索页</h2><p><strong>分析</strong></p><p><code>https://search.douban.com/movie/subject_search?search_text={search_text}&amp;cat=1002&amp;start={start}</code></p><p>search_text 搜索关键字</p><p>cat 搜索类别</p><p>start 开始的条数</p><p>url规则可以适用到图书电影搜索页面，后面的爬取也一样</p><p><strong>爬取后发现页面信息都无法获取</strong>，但是可以找到有个<code>window.__DATA__</code>猜测数据都被加密成了这串字符串<br><img src="https://img-blog.csdnimg.cn/20201123180017804.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>一轮百度发现有大佬把加密的js代码提取出来了！</p><p>于是直接给出大佬的链接<a href="https://mp.weixin.qq.com/s/2mpu_oY2-M0wcLvf1eU7Sw" target="_blank" rel="noopener">豆瓣读书搜索页的window.<strong>DATA</strong>的解密</a></p><p>解决了这个问题其他的就很好爬取了</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>完整代码见<a href="https://github.com/MorreInfo/MoreInfo_Crawler/blob/master/moreinfo_crawler/spiders/douban_book_search.py" target="_blank" rel="noopener">github仓库</a></p><p>提取出的js在<code>third_party/main.js</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubanBookSearchSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'douban_book_search'</span></span><br><span class="line">    allowed_domains = [<span class="string">'douban.com'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,keyword=None,start=None,*args, **kwargs)</span>:</span></span><br><span class="line">        super(DoubanBookSearchSpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.keyword = keyword</span><br><span class="line">        self.start = start</span><br><span class="line">        self.start_urls.append(<span class="string">f'https://search.douban.com/book/subject_search?search_text=<span class="subst">&#123;self.keyword&#125;</span>&amp;cat=1001&amp;start=<span class="subst">&#123;self.start&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        r = re.search(<span class="string">'window.__DATA__ = "([^"]+)"'</span>, response.text).group(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 导入js</span></span><br><span class="line">        file_path = pathlib.Path.cwd() / <span class="string">'third_party/main.js'</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">'gbk'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            decrypt_js = f.read()</span><br><span class="line">        ctx = execjs.compile(decrypt_js)</span><br><span class="line">        data = ctx.call(<span class="string">'decrypt'</span>, r)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data[<span class="string">'payload'</span>][<span class="string">'items'</span>]:</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'rating'</span>, <span class="literal">None</span>):</span><br><span class="line">                cover_url = item[<span class="string">'cover_url'</span>]</span><br><span class="line">                score = item[<span class="string">'rating'</span>][<span class="string">'value'</span>]</span><br><span class="line">                score_num = item[<span class="string">'rating'</span>][<span class="string">'count'</span>]</span><br><span class="line">                url = item[<span class="string">'url'</span>]</span><br><span class="line">                abstract = item[<span class="string">'abstract'</span>]</span><br><span class="line">                title = item[<span class="string">'title'</span>]</span><br><span class="line">                id = item[<span class="string">'id'</span>]</span><br><span class="line">                <span class="keyword">yield</span> DouBanBookSearchItem(</span><br><span class="line">                    cover_url=cover_url,</span><br><span class="line">                    score=score,</span><br><span class="line">                    score_num=score_num,</span><br><span class="line">                    url=url,</span><br><span class="line">                    abstract=abstract,</span><br><span class="line">                    title=title,</span><br><span class="line">                    id=id)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/cecb29c04cd2" target="_blank" rel="noopener">爬虫框架Scrapy个人总结（详细）熟悉</a></p><p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/architecture.html" target="_blank" rel="noopener">架构概览</a></p><p><a href="https://blog.csdn.net/ck784101777/article/details/104468780/" target="_blank" rel="noopener">Scrapy爬虫框架，入门案例（非常详细）</a></p><p><a href="https://mp.weixin.qq.com/s/2mpu_oY2-M0wcLvf1eU7Sw" target="_blank" rel="noopener">豆瓣读书搜索页的window.<strong>DATA</strong>的解密</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Scrapy-豆瓣搜索页爬虫&quot;&gt;&lt;a href=&quot;#Scrapy-豆瓣搜索页爬虫&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 豆瓣搜索页爬虫&quot;&gt;&lt;/a&gt;Scrapy 豆瓣搜索页爬虫&lt;/h1&gt;&lt;p&gt;使用scrapy爬虫框架对豆瓣图书搜索结果进
      
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="crawler" scheme="http://yoursite.com/tags/crawler/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>使用crontab完成定时任务</title>
    <link href="http://yoursite.com/2020/11/23/%E4%BD%BF%E7%94%A8crontab%E5%AE%8C%E6%88%90%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    <id>http://yoursite.com/2020/11/23/%E4%BD%BF%E7%94%A8crontab%E5%AE%8C%E6%88%90%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/</id>
    <published>2020-11-22T17:34:39.000Z</published>
    <updated>2020-11-22T17:34:11.383Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用crontab完成定时任务"><a href="#使用crontab完成定时任务" class="headerlink" title="使用crontab完成定时任务"></a>使用crontab完成定时任务</h1><p>crond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">usage:  crontab [-u user] file</span><br><span class="line">        crontab [-u user] [ -e | -l | -r ]</span><br><span class="line">        -e      (执行文字编辑器来设定时程表，内定的文字编辑器是 vi)</span><br><span class="line">        -l      (列出user的时间表)</span><br><span class="line">        -r      (删除user的时间表)</span><br></pre></td></tr></table></figure><p>root用户的任务调度操作可以通过<code>crontab –u root –e</code>来设置，也可以将调度任务直接写入<code>/etc/crontab</code>文件</p><h2 id="cron表达式"><a href="#cron表达式" class="headerlink" title="cron表达式"></a>cron表达式</h2><p>cron表达式是一个字符串，包含五个到七个由空格分隔的字段，表示一组时间，通常作为执行某个程序的时间表</p><p>minute  hour  day  month  week  command</p><p>minute： 表示分钟，可以是从0到59之间的任何整数</p><p>hour：表示小时，可以是从0到23之间的任何整数</p><p>day：表示日期，可以是从1到31之间的任何整数</p><p>month：表示月份，可以是从1到12之间的任何整数</p><p>week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日</p><p>command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">*    *    *    *    * <span class="built_in">command</span></span><br><span class="line">-    -    -    -    -</span><br><span class="line">|    |    |    |    |</span><br><span class="line">|    |    |    |    +----- 星期中星期几 (0 - 7) (星期天 为0)</span><br><span class="line">|    |    |    +---------- 月份 (1 - 12) </span><br><span class="line">|    |    +--------------- 一个月中的第几天 (1 - 31)</span><br><span class="line">|    +-------------------- 小时 (0 - 23)</span><br><span class="line">+------------------------- 分钟 (0 - 59)</span><br></pre></td></tr></table></figure><p>星号（*):代表所有可能的值，如month字段为星号，则表示每月都执行该命令操作</p><p>逗号（,):可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”</p><p>中杠（-):可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”</p><p>正斜线（/):可以用正斜线指定时间的间隔频率，例如“*/2”表示每两小时执行一次</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><ol><li>每一分钟执行一次 /bin/ls</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * /bin/ls</span><br></pre></td></tr></table></figure><ol><li>在 12 月内, 每天的早上 6 点到 12 点，每隔 3 个小时 0 分钟执行一次 <code>/usr/bin/backup</code></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 6-12/3 * 12 * /usr/bin/backup</span><br></pre></td></tr></table></figure><ol><li>每天22：50关闭ssh服务</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">50 22 * * * /sbin/service sshd stop</span><br></pre></td></tr></table></figure><ol><li>在 <code>/etc/crontab</code> 中添加环境变量，在可执行命令之前添加命令 <code>. /etc/profile;/bin/sh</code>，使得环境变量生效</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20 03 * * * . /etc/profile;/bin/sh test.sh</span><br></pre></td></tr></table></figure><h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><ol><li>crontab有2种编辑方式：直接编辑/etc/crontab文件与crontab –e，其中/etc/crontab里的计划任务是系统中的计划任务，而用户的计划任务需要通过crontab –e来编辑</li><li>crontab中的command尽量使用绝对路径，否则会经常因为路径错误导致任务无法执行</li><li>新创建的 cron 任务不会马上执行，至少要过 2 分钟后才可以，可以重启 cron 来马上执行</li><li>%在crontab文件中表示换行，因此假如脚本或命令含有%,需要使用\%来进行转义</li></ol><h3 id="Mac-下使用crontab遇到的问题"><a href="#Mac-下使用crontab遇到的问题" class="headerlink" title="Mac 下使用crontab遇到的问题"></a>Mac 下使用crontab遇到的问题</h3><p>我有一个Python爬虫脚本，在命令行时可以正常工作，但在crontab下报错</p><p><strong>can’t open file …  [Errno 1] Operation not permitted</strong></p><p>cron表达式如下</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">30</span> <span class="number">7</span> * * * /usr/local/bin/python3 script.py &gt;&gt; script.log <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure><p>尝试了许多不同的方法，包括尝试过赋予文件权限，以root用户身份创建cron作业，不同的Python路径，都不能正常运行</p><p>最后在Stack Overflow找到解决方案</p><p>赋予<code>cron</code>全磁盘访问权限，方法如下</p><ol><li>系统偏好设置-&gt;安全和隐私-&gt;完整磁盘访问</li><li><p>解除锁定以允许更改</p></li><li><p>单击 +</p></li><li>单击Command + Shift + G输入<code>/ usr / sbin</code></li><li>找到<code>cron</code> 添加</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.runoob.com/linux/linux-comm-crontab.html" target="_blank" rel="noopener">Linux crontab 命令</a></p><p><a href="https://www.linuxprobe.com/how-to-crontab.html" target="_blank" rel="noopener">crontab用法与实例</a></p><p><a href="https://www.cnblogs.com/ftl1012/p/crontab.html" target="_blank" rel="noopener">Linux crontab命令详解</a></p><p><a href="https://stackoverflow.com/questions/58844669/trying-to-run-a-python-script-with-cron-getting-errno-1-operation-not-permitt/62152555#62152555" target="_blank" rel="noopener">Trying to run a Python script with cron  Operation not permitted</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用crontab完成定时任务&quot;&gt;&lt;a href=&quot;#使用crontab完成定时任务&quot; class=&quot;headerlink&quot; title=&quot;使用crontab完成定时任务&quot;&gt;&lt;/a&gt;使用crontab完成定时任务&lt;/h1&gt;&lt;p&gt;crond是linux下用来周期性的
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>A Dynamic Directed Graph Convolutional Network for Action Recognition(DDGCN)</title>
    <link href="http://yoursite.com/2020/11/17/DDGCN%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/11/17/DDGCN%E7%AC%94%E8%AE%B0/</id>
    <published>2020-11-17T13:12:35.000Z</published>
    <updated>2020-11-17T15:53:03.180Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DDGCN-A-Dynamic-Directed-Graph-Convolutional-Network-for-Action-Recognition"><a href="#DDGCN-A-Dynamic-Directed-Graph-Convolutional-Network-for-Action-Recognition" class="headerlink" title="DDGCN: A Dynamic Directed Graph Convolutional Network for Action Recognition"></a>DDGCN: A Dynamic Directed Graph Convolutional Network for Action Recognition</h1><blockquote><p>作者 | Matthew Korban, Xin Li<br>单位 | 路易斯安那州立大学<br>论文地址 | <a href="https://link.zhihu.com/?target=https%3A//www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650749.pdf" target="_blank" rel="noopener">https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650749.pdf</a><br>会议 | ECCV 2020</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出了一种动态有向图卷积网络(DDGCN)，从人体行为的骨架表示出发，对人体行为的时空特征进行建模</p><p>DDGCN由三个新的特征建模模块组成：</p><ol><li>动态卷积采样(DCS)</li><li>动态卷积权重(DCW)</li><li>有向图时空(DGST)特征提取</li></ol><p>DCS和DCW模块可以有效地捕捉动态的非相邻关节之间的时空相关性</p><p>DSTG特征提取模块，通过包含时空的有序信息来增强动作的特征</p><h2 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构<img src="https://img-blog.csdnimg.cn/20201117212723550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></h2><h2 id="动态卷积采样-DCS"><a href="#动态卷积采样-DCS" class="headerlink" title="动态卷积采样(DCS)"></a>动态卷积采样(DCS)</h2><p>人体非相邻的子部分在人类行为中往往是相互关联的，且这种关联是动态的<br><img src="https://img-blog.csdnimg.cn/20201117212757212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>DCS算法可以总结如下</p><ol><li><p>按照骨架模板初始化静态图<script type="math/tex">G_S</script>，并相应地初始化所有节点的索引</p></li><li><p>初始化邻居采样：对于∀vi∈GS，分两步创建其初始有序近邻集合pi(B(Vi))</p><ul><li>创建包括图中所有其他节点的有序节点集合Oi，该有序节点集合Oi包括根据图到vi的图距离排序的图中的所有其他节点。 当两个节点Vj和Vr具有相同的图距离(例如，都离Vi有r跳距离)时，则根据它们的初始化索引对它们进行排序</li><li>给定核大小r，从Oi中选取前r个节点，这些节点在此步骤pi(B(Vi))中形成有序的邻集</li></ul></li><li><p>更新采样邻域：∀vi，通过学习减少识别损失的最优偏移量∆pi来更新索引偏移量和邻域采样</p></li></ol><p>最后，在<script type="math/tex">G_{ST}</script>上，通过如下公式(1)的图形卷积计算特征图<script type="math/tex">f_{ST}</script></p><script type="math/tex; mode=display">f_{S T}\left(v_{i}\right)=\sum_{v_{j} \in B\left(v_{i}\right)} w\left(v_{i}\right) \cdot\left(p_{i}\left(v_{j}\right)+\Delta p_{i}\left(v_{j}\right)\right)</script><p>其中i和j分别是中心采样节点和相邻采样节点的索引，B是动态相邻采样节点集合，w是动态权重函数，pi是动态相邻采样函数，∆pi是偏移采样函数</p><h2 id="动态卷积权重-DCW"><a href="#动态卷积权重-DCW" class="headerlink" title="动态卷积权重(DCW)"></a>动态卷积权重(DCW)</h2><p>DCW权重分配模块动态地将权重<script type="math/tex">w_i</script>分配给<script type="math/tex">v_i</script>的相邻节点</p><p>使用动态时间规整(DTW)算法来计算 <script type="math/tex">P_v=DTW_{path} (W,B(v))</script></p><p>其中<script type="math/tex">P_v</script>中的第一列定义了W中元素的排序索引，第二列表示所选元素及其在<script type="math/tex">B(V)</script>中的顺序</p><p><img src="https://img-blog.csdnimg.cn/20201117212842409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="有向时空图特征-DSTG"><a href="#有向时空图特征-DSTG" class="headerlink" title="有向时空图特征(DSTG)"></a>有向时空图特征(DSTG)</h2><p><img src="https://img-blog.csdnimg.cn/20201117212858577.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>骨骼特征 <script type="math/tex">f_{i}^{B}=\overline{f_{i-1} f_{i}}=f_{i-1}-f_{i}</script></p><p>时间特征 <script type="math/tex">f_{i}^{T}=f_{i}^{t}-f_{i}^{t-1}</script></p><p>串联得到节点v<em>i的特征向量 $$F</em>{i}=\left{f<em>{i}^{J}, f</em>{i}^{B}, f_{i}^{T}\right}$$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在NTURGB-D 60和Kinetics数据集上性能均优于其他方法<br><img src="https://img-blog.csdnimg.cn/20201117213008955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>DSTG模块对于性能提升最大，完整的DDC模块可得到最高的准确率</p><p><img src="https://img-blog.csdnimg.cn/20201117212959696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="识别不完整的动作"><a href="#识别不完整的动作" class="headerlink" title="识别不完整的动作"></a>识别不完整的动作</h3><p>对丢失帧的动作识别进行的实验，分为以下3中情况</p><ul><li><p>运动开始时丢失帧</p></li><li><p>运动结束时丢失帧</p></li><li><p>序列中随机丢失的帧</p></li></ul><p>结论是<strong>运动开始时的序列存在大部分特征</strong><br><img src="https://img-blog.csdnimg.cn/20201117213022474.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyODE1ODA3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>提出了一种基于骨架图的动态有向图卷积网络(DDGCN)动作识别算法</p><p>DDGCN由三个新模块组成，动态卷积抽样(DCS)、动态卷积权重分配(DCW)和有向图时空(DGST)特征提取</p><p>这些新模块有助于更好地捕捉时空依赖关系，骨架的层次结构和时序特征。 实验表明，DDGCN在多个公共数据集上的动作识别准确率优于最先进的算法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DDGCN-A-Dynamic-Directed-Graph-Convolutional-Network-for-Action-Recognition&quot;&gt;&lt;a href=&quot;#DDGCN-A-Dynamic-Directed-Graph-Convolutional-
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="ActionRecognition" scheme="http://yoursite.com/tags/ActionRecognition/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机(SVM)</title>
    <link href="http://yoursite.com/2020/11/06/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM)/"/>
    <id>http://yoursite.com/2020/11/06/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM)/</id>
    <published>2020-11-06T08:39:23.000Z</published>
    <updated>2020-11-06T08:40:10.348Z</updated>
    
    <content type="html"><![CDATA[<h1 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h1><p>支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的<strong>间隔最大的线性分类器</strong></p><p>SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题</p><ul><li><p>优点:泛化错误率低，计算开销不大，结果易解释</p></li><li><p>缺点:对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题</p></li></ul><h2 id="分隔超平面"><a href="#分隔超平面" class="headerlink" title="分隔超平面"></a>分隔超平面</h2><p><img src="https://github.com/benull/Resource/raw/master/svm1.png" alt="pic1"></p><p>上面将数据集分隔开来的直线称为分隔超平面 ( separating hyperplane)</p><p>在上面给出的例子中，由于数据点都在二维平面上，所 以此时分隔超平面就只是一条直线。但是，如果所给的数据集是三的，那么此时用来分隔数据的就是一个平面</p><p>显而易见，更高维的情况可以依此类推。如果数据集是1024维的，那么就需要一个1023维的对象来对数据进行分隔，该对象被称之为超平面(hyperplane)，也就是分类的决策边界</p><p>分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别</p><h2 id="线性可划分"><a href="#线性可划分" class="headerlink" title="线性可划分"></a>线性可划分</h2><p>存在一个划分超平面能将训练样本正确分类</p><p>而现实人物中，原始样本空间内也许并不在一个能正确划分两类样本的超平面，如下图</p><p><img src="https://github.com/benull/Resource/raw/master/svm2.png" alt="pic1"></p><p>对这样的问题, 可将样本从原始空间映射到一个更高维的特征空间, 使得样本在这个特征空间内线性可分</p><p>例如在上图中， 若将原始的二维空间映射到一个合适的三维空间, 就能找到一个合适的划分超平面</p><p>比如想要将下图的红点和蓝点变成线性可分的，那么就将映射<script type="math/tex">y = x</script>变成映射<script type="math/tex">y=x^2</script>，这样就线性可分了</p><p><img src="https://img-blog.csdnimg.cn/20190303081111808.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20190303081101648.png" alt="在这里插入图片描述"></p><h2 id="间隔-margin"><a href="#间隔-margin" class="headerlink" title="间隔(margin)"></a>间隔(margin)</h2><p>对于任意一个超平面，其两侧数据点都距离它有一个最小距离（垂直距离），这两个最小距离的和就是间隔margin</p><p><img src="https://img-blog.csdnimg.cn/20190302091934374.png" alt="img"></p><h2 id="Hard-Margin-SVM"><a href="#Hard-Margin-SVM" class="headerlink" title="Hard Margin SVM"></a>Hard Margin SVM</h2><blockquote><p>当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机</p></blockquote><p><strong>svm尝试寻找一个最优决策边界，距离两个类别最近的样本距离最远</strong></p><p><img src="https://github.com/benull/Resource/raw/master/svm3.png" alt="pic1"></p><p>在样本空间中，划分超平面可用如下线性方程来描述：</p><script type="math/tex; mode=display">w^Tx+b=0</script><p>其中 <script type="math/tex">\boldsymbol{w}=\left(w_{1} ; w_{2} ; \ldots ; w_{d}\right)</script> 为法向量, 决定了超平面的方向; <script type="math/tex">b</script> 为位移项, 决定了超平面与原点之间的距离</p><p>样本空间中任意点 <script type="math/tex">\boldsymbol{x}</script> 到超平面 <script type="math/tex">(\boldsymbol{w}, b)</script> 的距离可写为</p><script type="math/tex; mode=display">r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|}</script><p>假设超平面 <script type="math/tex">(\boldsymbol{w}, b)</script> 能将训练样本正确分类, 即对于 <script type="math/tex">\left(\boldsymbol{x}_{i}, y_{i}\right) \in D,</script> 若 <script type="math/tex">y_{i}=</script> <script type="math/tex">+1,</script> 则有 <script type="math/tex">\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b>0 ;</script> 若 <script type="math/tex">y_{i}=-1,</script> 则有 <script type="math/tex">\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b<0 .</script> 令</p><script type="math/tex; mode=display">\left\{\begin{array}{ll}\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \geqslant+1, & y_{i}=+1 \\\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \leqslant-1, & y_{i}=-1\end{array}\right.</script><p>如上图所示, 距离超平面最近的这几个训练样本点使上式的等号成立, 它们被称为<strong>“支持向量”(support vector)</strong>, 两个异类支持向量到超平面的距离之和为</p><script type="math/tex; mode=display">\gamma=\frac{2}{\|\boldsymbol{w}\|}</script><p>显然, 为了最大化间隔, 仅需最大化 <script type="math/tex">\|\boldsymbol{w}\|^{-1},</script> 这等价于最小化 <script type="math/tex">\|\boldsymbol{w}\|^{2}</script></p><script type="math/tex; mode=display">\begin{array}{l}\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2} \\\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m\end{array}</script><p>这就是支持向量机(Support Vector Machine, 简称 SVM)的基本型</p><h2 id="Soft-Margin-SVM"><a href="#Soft-Margin-SVM" class="headerlink" title="Soft Margin SVM"></a>Soft Margin SVM</h2><blockquote><p>当训练数据近似线性可分时，通过软间隔最大化，学习一个线性支持向量机，又称为软间隔支持向量机</p></blockquote><p>前面我们是假定所有的训练样本在样本空间或特征空间中是严格线性可分的，即存在一个超平面能把不同类的样本完全分开，然而现实任务中很难确定这样的超平面（不管是线性超平面还是经过核变换到高维空间的超平面），所以引入松弛变量，允许一些样本出错，但我们希望出错的样本越少越好，所以松弛变量也有限制</p><p>具体来说，前面介绍的支持向量机的形式要求所有样本满足约束，即所有的样本必须划分正确，这称为硬间隔，而软间隔则是允许某些样本不满足约束</p><script type="math/tex; mode=display">y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1</script><p>如下图红色圈出了不满足约束的样本<br><img src="https://github.com/benull/Resource/raw/master/svm4.png" alt="pic1"></p><p>当然, 在最大化间隔的同时, 不满足约束的样本应尽可能少. 于是, 优化目标可写为</p><script type="math/tex; mode=display">\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right)</script><p>其中 <script type="math/tex">C>0</script> 是一个常数, <script type="math/tex">\ell_{0 / 1}</script> 是“0/1损失函数”</p><script type="math/tex; mode=display">\ell_{0 / 1}(z)=\left\{\begin{array}{ll}1, & \text { if } z<0 \\0, & \text { otherwise }\end{array}\right.</script><p>引入“松他变量” (slack variables) <script type="math/tex">\xi_{i} \geqslant 0,</script> 可将上式重写为</p><script type="math/tex; mode=display">\min _{\boldsymbol{w}, b, \xi_{i}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}</script><h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题</p><p>这个问题可以用现成的QP (Quadratic Programming) 优化包进行求解</p><p>此外，由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法</p><p>这样做的优点在于：</p><ul><li>一者对偶问题往往更容易求解</li><li>二者可以自然的引入核函数，进而推广到非线性分类问题</li></ul><p>具体来说, 对每条约束添加拉格朗日乘子 <script type="math/tex">\alpha_{i} \geqslant 0,</script> 则该问题的拉格朗日函数可写为</p><script type="math/tex; mode=display">L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)</script><p>其中 <script type="math/tex">\alpha=\left(\alpha_{1} ; \alpha_{2} ; \ldots ; \alpha_{m}\right) .</script> 令 <script type="math/tex">L(\boldsymbol{w}, b, \boldsymbol{\alpha})</script> 对 <script type="math/tex">\boldsymbol{w}</script> 和 <script type="math/tex">b</script> 的偏导为零可得</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{w} &=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\0 &=\sum_{i=1}^{m} \alpha_{i} y_{i}\end{aligned}</script><p>即可将 <script type="math/tex">L(\boldsymbol{w}, b, \boldsymbol{\alpha})</script> 中的 <script type="math/tex">\boldsymbol{w}</script> 和 <script type="math/tex">b</script> 消去, 就得到的目标函数对偶问题</p><script type="math/tex; mode=display">\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}</script><h2 id="非线形SVM"><a href="#非线形SVM" class="headerlink" title="非线形SVM"></a>非线形SVM</h2><blockquote><p>当训练数据线性不可分时，通过使用核技巧(kernel trick)及软间隔最大化，学习非线形支持向量机</p></blockquote><h3 id="非线性SVM算法原理"><a href="#非线性SVM算法原理" class="headerlink" title="非线性SVM算法原理"></a>非线性SVM算法原理</h3><p>对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机</p><p>令 <script type="math/tex">\phi(x)</script> 表示将 <script type="math/tex">x</script> 映射后的特征向量, 于是, 在特征空间中划分超平面所对<br>应的模型可表示为</p><script type="math/tex; mode=display">f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b</script><p>其中 <script type="math/tex">\boldsymbol{w}</script> 和 <script type="math/tex">b</script> 是模型参数，有</p><script type="math/tex; mode=display">\begin{array}{l}\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2} \\\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \phi\left(\boldsymbol{x}_{i}\right)+b\right) \geqslant 1, \quad i=1,2, \ldots, m\end{array}</script><p>其对偶问题是</p><script type="math/tex; mode=display">\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)</script><script type="math/tex; mode=display">\begin{array}{ll}\text { s.t. } & \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\& \alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m\end{array}</script><p>由于在线性支持向量机学习的对偶问题里，目标函数和分类决策函数都<strong>只涉及实例和实例之间的内积，所以不需要显式地指定非线性变换，而是用核函数替换当中的内积</strong></p><p>有了这样的函数，我们就不必直接去计算高维甚至无穷维特征空间中的内积，于是上式可重写为</p><script type="math/tex; mode=display">\begin{array}{ll}\max _{\boldsymbol{\alpha}} & \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \\\text { s.t. } & \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\& \alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m\end{array}</script><h3 id="常见的核函数"><a href="#常见的核函数" class="headerlink" title="常见的核函数"></a>常见的核函数</h3><script type="math/tex; mode=display">\begin{aligned}&\text { 线性核 } \quad \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}\\&\text { 多项式核 } \quad \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}\right)^{d} \quad d \geqslant 1 \text { 为多项式的次数 }\\&\text { 高斯核 } \quad \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\exp \left(-\frac{\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|^{2}}{2 \sigma^{2}}\right) \quad \sigma>0 \text { 为高斯核的带宽(width) }\\&\text { 拉普拉斯核 } \quad \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\exp \left(-\frac{\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|}{\sigma}\right) \quad \sigma>0\\&\text { Sigmoid 核 } \quad \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\tanh \left(\beta \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}+\theta\right) \quad \text { tanh 为双曲正切函数, } \beta>0, \theta<0\end{aligned}</script><h2 id="二分类扩展到多分类问题"><a href="#二分类扩展到多分类问题" class="headerlink" title="二分类扩展到多分类问题"></a>二分类扩展到多分类问题</h2><p>SVM 扩展可解决多个类别分类问题</p><p><strong>one-vs-rest</strong></p><p>对于每个类，有一个当前类和其他类的二类分类器（one-vs-rest）<br>将多分类问题转化为 n 个二分类问题</p><h2 id="svm解决回归问题"><a href="#svm解决回归问题" class="headerlink" title="svm解决回归问题"></a>svm解决回归问题</h2><p>现在我们来考虑回归问题，传统回归模型通常直接基于模型输出 <script type="math/tex">f(x)</script> 与真实输出 <script type="math/tex">y</script> 之 间的差别来计算损失, 当且仅当 <script type="math/tex">f(\boldsymbol{x})</script> 与 <script type="math/tex">y</script> 完全相同时, 损失才为零</p><p>与此不同 <script type="math/tex">,</script> <strong>支持向量回归(Support Vector Regression, 简称 SVR)</strong>假设我们能容忍 <script type="math/tex">f(x)</script> 与 <script type="math/tex">y</script> 之间最多有 <script type="math/tex">\epsilon</script> 的偏差, 即仅当 <script type="math/tex">f(\boldsymbol{x})</script> 与 <script type="math/tex">y</script> 之间的差别绝对值大于 <script type="math/tex">\epsilon</script> 时才计算损失</p><p>如图所示, 这相当于以 <script type="math/tex">f(x)</script> 为中心, 构建了一个宽度为 <script type="math/tex">2 \epsilon</script> 的间隔带, 若训练样本落入此间隔带, 则认为是被预测正确的</p><p><img src="https://github.com/benull/Resource/raw/master/svm5.png" alt="pic1"></p><p>于是, SVR 问题可形式化为</p><script type="math/tex; mode=display">\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{c}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)</script><p>其中 <script type="math/tex">C</script> 为正则化常数, <script type="math/tex">\ell_{\epsilon}</script> 是图 6.7 所示的 <script type="math/tex">\epsilon</script> -不敏感损失 <script type="math/tex">(\epsilon</script> -insensitive loss <script type="math/tex">)</script> 函数</p><script type="math/tex; mode=display">\ell_{\epsilon}(z)=\left\{\begin{array}{ll}0, & \text { if }|z| \leqslant \epsilon \\|z|-\epsilon, & \text { otherwise }\end{array}\right.</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href>机器学习算法（一）SVM</a></p><p>机器学习-周志华</p><p><a href="https://zhuanlan.zhihu.com/p/31886934" target="_blank" rel="noopener">支持向量机（SVM）——原理篇</a></p><p>Machine Learning in Action by Peter Harrington</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;支持向量机（SVM）&quot;&gt;&lt;a href=&quot;#支持向量机（SVM）&quot; class=&quot;headerlink&quot; title=&quot;支持向量机（SVM）&quot;&gt;&lt;/a&gt;支持向量机（SVM）&lt;/h1&gt;&lt;p&gt;支持向量机（support vector machines, SVM）是一种
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="Algorithms" scheme="http://yoursite.com/tags/Algorithms/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>分类算法评估指标</title>
    <link href="http://yoursite.com/2020/11/05/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E8%AF%84%E4%BB%B7/"/>
    <id>http://yoursite.com/2020/11/05/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E8%AF%84%E4%BB%B7/</id>
    <published>2020-11-05T06:26:23.000Z</published>
    <updated>2020-11-05T06:26:03.282Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分类算法评估指标"><a href="#分类算法评估指标" class="headerlink" title="分类算法评估指标"></a>分类算法评估指标</h1><h2 id="精度（Accuracy）"><a href="#精度（Accuracy）" class="headerlink" title="精度（Accuracy）"></a>精度（Accuracy）</h2><p>即正确预测的正反例数 /预测总数</p><p>对于样例集D,分类错误率定义为</p><script type="math/tex; mode=display">E(f ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right)</script><p>精度则定义为</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{acc}(f ; D) &=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right)=y_{i}\right) \\&=1-E(f ; D)\end{aligned}</script><ul><li><p>对于样本类别数量严重不均衡的情况（skewed)不能用精度指标来衡量</p><p>例如：有A类1000个，B类5个，如果我把这1005个样本都预测成A类，正确率=1000/1005=99.5%</p></li><li><p>对于有倾向性的问题，往往不能用精度指标来衡量</p><p>例如：判断这个病人是不是病危，如果不是病危错误判断为病危，那只是损失一点医务人员的时间和精力，如果是把病危的人判断为非病危状态，那损失的就是一条人命</p></li></ul><p>对于以上两种情况，单纯根据Accuracy来衡量算法的优劣已经失效，这个时候就需要对目标变量的真实值和预测值做更深入的分析</p><h2 id="混淆矩阵（confusion-matrix）"><a href="#混淆矩阵（confusion-matrix）" class="headerlink" title="混淆矩阵（confusion matrix）"></a>混淆矩阵（confusion matrix）</h2><p>对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为</p><ul><li><p>真正例（True Positive)</p></li><li><p>假正例（False Positive)</p></li><li>真反例（True Negative）</li><li>假反例（False Negative）</li></ul><p>分类结果的混淆矩阵如下</p><div class="table-container"><table><thead><tr><th></th><th>预测值0</th><th>预测值1</th></tr></thead><tbody><tr><td>真实值0</td><td>TN</td><td>FP</td></tr><tr><td>真实值1</td><td>FN</td><td>TP</td></tr></tbody></table></div><h2 id="精准率（precision）"><a href="#精准率（precision）" class="headerlink" title="精准率（precision）"></a>精准率（precision）</h2><script type="math/tex; mode=display">\text {precision}=\frac{T P}{T P+F P}</script><p>精准率就是预测为正例的那些数据里预测正确的数据个数</p><h2 id="召回率（recall）"><a href="#召回率（recall）" class="headerlink" title="召回率（recall）"></a>召回率（recall）</h2><script type="math/tex; mode=display">\text {recall}=\frac{T P}{T P+F N}</script><p>召回率就是真实为正例的那些数据里预测正确的数据个数</p><p>精准率和查全率是一对矛盾的度量，一般来说，精准率高时，召回率则偏低；而召回率高时，精准率则偏低</p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>F1 Score同时关注精准率和召回率，是precision 和 recall 的调和平均值</p><p>它的值更接近于Precision与Recall中较小的值</p><script type="math/tex; mode=display">\frac{1}{F 1}=\frac{1}{2}\left(\frac{1}{\text {precision}}+\frac{1}{\text {recall}}\right)</script><script type="math/tex; mode=display">F 1=\frac{2 \cdot \text {precision } \cdot \text { recall}}{\text {precision }+\text {recall}}</script><h2 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h2><p>根据学习器的预测结果对样例进行排序, 排在前面的是学习器认为“最可能”是正例的样本, 排在最后的则是学习器认为“最 不可能”是正例的样本. 按此顺序逐个把样本作为正例进行预测, 则每次可以计算出当前的精准率、召回率</p><p>以精准率为纵轴、召回率为横轴作图, 就得到了精准率-召回率曲线, 简称“P-R曲线”，显示该曲线的图称为“P-R图”</p><p><img src="https://github.com/benull/Resource/raw/master/ev1.png" alt="ev1"></p><p>与ROC曲线左上凸效果好不同的是，PR曲线是右上凸效果越好</p><p>若一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者</p><h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p>ROC(Receiver Operating Characteristic)受试者工作特性曲线</p><p>True Postitve Rate(真正例率)：正样本中被预测对比例</p><script type="math/tex; mode=display">\text {TPR}=\frac{TP}{TP+FN}</script><p>False Positive Rate(假正例率)：负样本被预测错的比例</p><script type="math/tex; mode=display">\text {FPR}=\frac{FP}{TN+FP}</script><p>ROC是一个以TPR为纵坐标，FPR为横坐标构造出来的一幅图</p><p><img src="https://github.com/benull/Resource/raw/master/ev2.png" alt="ev2"></p><p>在ROC空间，ROC曲线越凸向左上方向效果越好，因为这说明精确率高且覆盖率大</p><p>进行学习器的比较时, 与 P-R 图相似, 若一个学习器的 ROC 曲线被另一 个学习器的曲线完全“包住”, 则可断言后者的性能优于前者; 若两个学习器的 ROC 曲线发生交叉, 则难以一般性地断言两者熟优熟劣. 此时如果一定要进行比较, 则较为合理的判据是比较 ROC曲线下的面积, 即 AUC</p><h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>AUC（Area Under Curve）是一种模型分类指标，且仅仅是二分类模型的评价指标</p><p>AUC 值为 ROC 曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好</p><p>AUC的物理意义正样本的预测结果大于负样本的预测结果的概率。所以AUC反应的是分类器对样本的排序能力。另外值得注意的是，AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价分类器性能的一个原因</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/36305931" target="_blank" rel="noopener">机器学习评估指标</a></p><p>机器学习-周志华</p><p><a href="https://blog.csdn.net/chocolate_chuqi/article/details/81162244" target="_blank" rel="noopener">回顾及总结—评价指标（分类指标)</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;分类算法评估指标&quot;&gt;&lt;a href=&quot;#分类算法评估指标&quot; class=&quot;headerlink&quot; title=&quot;分类算法评估指标&quot;&gt;&lt;/a&gt;分类算法评估指标&lt;/h1&gt;&lt;h2 id=&quot;精度（Accuracy）&quot;&gt;&lt;a href=&quot;#精度（Accuracy）&quot; cla
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Fashion Image Retrieval Based on Regional Representation for Design Protection</title>
    <link href="http://yoursite.com/2020/11/03/PsNet/"/>
    <id>http://yoursite.com/2020/11/03/PsNet/</id>
    <published>2020-11-03T13:20:32.000Z</published>
    <updated>2020-11-04T16:24:33.842Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Which-Is-Plagiarism-Fashion-Image-Retrieval-Based-on-Regional-Representation-for-Design-Protection"><a href="#Which-Is-Plagiarism-Fashion-Image-Retrieval-Based-on-Regional-Representation-for-Design-Protection" class="headerlink" title="Which Is Plagiarism: Fashion Image Retrieval Based on Regional Representation for Design Protection"></a>Which Is Plagiarism: Fashion Image Retrieval Based on Regional Representation for Design Protection</h1><blockquote><p>作者 | Yining Lang, Yuan He, Fan Yang, Jianfeng Dong, Hui Xue</p><p>单位 | 阿里；浙江工商大学；AZFT</p><p>会议｜CVPR2020</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Lang_Which_Is_Plagiarism_Fashion_Image_Retrieval_Based_on_Regional_Representation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper地址</a></p></blockquote><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在服装领域，虽然打假一直不断，但盗版抄袭问题依旧普遍存在，而且从线上线下，抄袭手段越来越刁钻。目前来看，服装领域的抄袭有以下三类</p><p>•<strong>图片盗用</strong></p><p>​    抄袭成本很低，很容易被平台的图片检索系统锁定</p><p>•<strong>创意盗版</strong>  </p><p>​    抄袭成本稍高，但基于相似度度量的算法，可以对它们进行召回和治理</p><p>•<strong>对服装的某些区域进行修改</strong></p><p>   抄袭成本高，需要人工审核发现，打假成本也高</p><p><img src="https://github.com/benull/Resource/raw/master/psnet1.png" alt="pic1"></p><blockquote><p>两组盗版示例，其中每组中左图为正版服装，右图为盗版服装</p></blockquote><h3 id="盗版服装检索的难点"><a href="#盗版服装检索的难点" class="headerlink" title="盗版服装检索的难点"></a>盗版服装检索的难点</h3><p>盗版服装的形式层出不穷，有些盗版服装跟原图比较相似，但是有些并不相似</p><p>而且有些盗版服装与原创服装属于不同的类型，提高了网络训练时的要求</p><p><img src="https://github.com/benull/Resource/raw/master/psnet2.png" alt="pic2"></p><h3 id="盗版服装的定义"><a href="#盗版服装的定义" class="headerlink" title="盗版服装的定义"></a>盗版服装的定义</h3><p>作为盗版服装检索领域的首次工作，作者对盗版服装的定义是整体上抄袭原版服装设计和风格，服装修改的局部区域数小于等于2</p><p><img src="https://github.com/benull/Resource/raw/master/psnet3.png" alt="pic3"></p><blockquote><p>将图像中的服装分为五个区域，包括领子、胸部、腰部和两个袖子区域</p></blockquote><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="基于三元组的损失函数（for-相似性检索）"><a href="#基于三元组的损失函数（for-相似性检索）" class="headerlink" title="基于三元组的损失函数（for 相似性检索）"></a>基于三元组的损失函数（for 相似性检索）</h3><script type="math/tex; mode=display">\begin{array}{c}\mathcal{L}_{t r i}\left(I, I^{+}, I^{-}\right)=\sum_{r=1}^{R} \max \left(D_{r}^{I, I^{+}}-D_{r}^{I, I^{-}}+m, 0\right) \\\mathcal{L}_{t r a}=\sum_{n=1}^{N} \mathcal{L}_{t r i}\left(I, I^{+}, I^{-}\right)\end{array}</script><h3 id="基于三元组的损失函数（for-盗版检索）"><a href="#基于三元组的损失函数（for-盗版检索）" class="headerlink" title="基于三元组的损失函数（for 盗版检索）"></a>基于三元组的损失函数（for 盗版检索）</h3><script type="math/tex; mode=display">\begin{array}{c}\mathcal{L}_{t r i}^{\prime}\left(I, I^{+}, I^{-}\right)=\sum_{r=1}^{R} \max \left(D_{r}^{I, I^{+}}-D_{r}^{I, I^{-}}+m, 0\right) \cdot \lambda_{r} \\\alpha_{t r i}=\frac{\operatorname{avg}\left\{\left\|f_{r}(I)-f_{r}\left(I^{+}\right)\right\|_{2} ; r=1,2, \ldots R\right\}}{\max \left\{\left\|f_{r}(I)-f_{r}\left(I^{+}\right)\right\|_{2} ; r=1,2, \ldots R\right\}} \\\mathcal{L}_{p l a}=\sum_{n=1}^{N}\left[\mathcal{L}_{t r i}^{\prime}\left(I, I^{+}, I^{-}\right) \cdot \alpha_{t r i}\right]\end{array}</script><h2 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h2><h3 id="PS-Net总体框架"><a href="#PS-Net总体框架" class="headerlink" title="PS-Net总体框架"></a>PS-Net总体框架</h3><p><img src="https://github.com/benull/Resource/raw/master/psnet4.png" alt="pic4"></p><h3 id="Network-Backbone"><a href="#Network-Backbone" class="headerlink" title="Network Backbone"></a>Network Backbone</h3><p><strong>HR-Net提取图片的特征</strong></p><p>​    HR-Net 的多分辨率子网并行连接，使得每一个高分辨率到低分辨率的表征都从其它并行表示中反复接受信息，从而得到丰富的高分辨率表征</p><p>​    但HR-Net不是必须的，可以用ResNet、VGG-Net 等替代</p><h3 id="Landmark-Branch"><a href="#Landmark-Branch" class="headerlink" title="Landmark Branch"></a>Landmark Branch</h3><p>关键点估计分支，为划分区域做准备，通过反卷积进行上采样</p><h3 id="Retrieval-Branch"><a href="#Retrieval-Branch" class="headerlink" title="Retrieval Branch"></a>Retrieval Branch</h3><p>聚合局部区域的特征进行检索</p><p>根据 Landmark Branch 得到的关键点预测和 输出的热力图，得到特定局部区域在特征图上的位置</p><p>再根据特定区域在特征图上的位置，通过ROI pooling得到 Retrieval Branch 的特征图中该区域相应的局部特征图</p><h3 id="Plagiarized-Fashion-数据集"><a href="#Plagiarized-Fashion-数据集" class="headerlink" title="Plagiarized Fashion 数据集"></a>Plagiarized Fashion 数据集</h3><p>•总共60,000张图片,其中40,000用于训练 20,000用于测试</p><p>•包括4类服装：短袖T恤、长袖上衣、外套以及连衣裙</p><p>•图片从淘宝网爬取并经过专家标注</p><p><img src="https://github.com/benull/Resource/raw/master/psnet5.png" alt="pic5"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>•提出了一个新的抄袭服装检索问题</p><p>•建立了新的用于抄袭服装检索的数据集Plagiarism  Fashion</p><p>•提出了一种基于区域表示的多任务网络PS-Net且达到了SOTA</p><p>•PS-Net还可以用于传统的服装检索和关键点估计任务</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Which-Is-Plagiarism-Fashion-Image-Retrieval-Based-on-Regional-Representation-for-Design-Protection&quot;&gt;&lt;a href=&quot;#Which-Is-Plagiarism-Fa
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="Retrieval" scheme="http://yoursite.com/tags/Retrieval/"/>
    
  </entry>
  
  <entry>
    <title>集成学习</title>
    <link href="http://yoursite.com/2020/10/30/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/10/30/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-10-30T05:01:23.000Z</published>
    <updated>2020-10-30T04:58:22.550Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><p>集成学习(ensemble learning)通过构建并结合多学习器来完成学习任务，常可获得比单一学习器显著优越的泛化性能</p><p>要获得好的集成，个体学习器应好而不同，即个体学习器要有一定的准确性，并且要有多样性，即学习器间要有差异</p><p>根据个体学习器的生成方式, 目前的集成学习方法大致可分为两大类</p><ul><li><p>学习器间存在强依赖关系、必须串行生成的序列化方法，如Boosting</p></li><li><p>学习器间不存在强依赖关系、可同时生成的并行化方法，如 Bagging 和随机森林</p></li></ul><p>集成学习在各个规模的数据集上都有很好的策略</p><ul><li><p>数据集大：划分成多个小数据集，学习多个模型进行组合</p></li><li><p>数据集小：利用Bootstrap方法(也称为自助法，一种有放回的抽样方法)进行抽样，得到多个数据集，分别训练多个模型再进行组合</p></li></ul><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting 是一族可将弱学习器提升为强学习器的算法</p><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><ol><li>先从初始训练集训练出一个基学习器</li><li>根据基学习器的表现对训练样本分布进行调整, 使得先前基学习器做错的训练样本在后续受到更多关注</li><li>基于调整后的样本分布来训练下一个基学习器;</li><li>重复进行, 直至基学习器数目达到事先指定的值 $T,$ 最终将这 $T$ 个基学习器进行加权结合</li></ol><p>从偏差-方差分解的角度看, Boosting 主要关注降低偏差, 因此 Boosting 能基于泛化性能相当弱的学习器构建出很强的集成</p><p>Boosting 族算法最著名的代表是 AdaBoost,是通过集中关注被已有分类器错分的那些数据来获得新的分类器</p><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><ul><li>优点:泛化错误率低，易编码，可以应用在大部分分类器上</li><li>缺点:对离群点敏感</li></ul><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><ol><li>训练数据中的每个样本，并赋予其一个权重，这些权重构成了向量<em>D</em>，权重都初始化成相等值</li><li>在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器</li><li>在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本 的权重将会提高</li><li>计算出<em>D</em>之后，AdaBoost又开始进入下一轮迭代</li><li>AdaBoost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止</li></ol><p><img src="https://github.com/benull/Resource/raw/master/adaboost.png" alt="adaboost"></p><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging 是并行式集成学习方法最著名的代表</p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><ol><li><p>给定包含 $m$ 个样本的数据集, 我们先随机取出一个样本放入采样集中, 再把该样本放回初始数据集, 使得下次采样时该样本仍有可能被选中</p></li><li><p>经过 $m$ 次随机采样操作, 我们得到含 $m$ 个样本的采样集, 初始训练集中有的样本在采样集里多次出现, 有的则从未出现</p></li><li><p>采样出 $T$ 个含 $m$ 个训练样本的采样集, 然后基于每个采样集训练出一个基学习器</p></li><li><p>将这些基学习器进行结合</p></li></ol><p>在对预测输出进行结合时, Bagging 通常对分类任务使用简单投票法, 对回归任务使用简单平均法</p><ul><li>Bagging通过降低基分类器的方差，改善了泛化误差</li><li>其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起</li><li>由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例</li></ul><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林(Random Forest) 是 Bagging 的一个扩展变体</p><p>随即森林在以决策树为基学习器构建 Bagging 集成的基础上, 进一步在决策树的训练过程中引入了随机属性选择</p><p>具体来说, 传统决策树在选择划分属性时是在当前结点的属性集合(假定有 $d$ 个属性)中选择一个最优属性</p><p>而在随即森林中, 对基决策树的每个结点, 先从该结点的属性集合中随机选择一个包含 $k$个属性的子集, 然后再从这个子集中选择一个最优属性用于划分</p><p>随机森林的训练效率常优于 Bagging， 因为在个体决策树的构建过程中, Bagging 使用的是“确定型”决策树, 在选择划分属性时要对结点的所有属性进行考察,而随机森林使用的“随机型”决策树则只需考察一个属性子集</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>具有极好的准确率</li><li>能够有效地运行在大数据集上</li><li>能够处理具有高维特征的输入样本，而且不需要降维</li><li>能够评估各个特征在分类问题上的重要性</li><li>对于缺省值问题也能够获得很好得结果</li></ul><h2 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h2><p>每个基学习器之间不存在很强的依赖性，为了提高集成的泛化能力在最终预测结果时，需要一定的策略对多个结果进行结合</p><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h4><p>对数值型输出，最常见的结合策略是使用平均法，又可分为</p><ul><li>简单平均法</li><li>加权平均法</li></ul><p>一般而言，在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法</p><h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h4><p>对分类任务来说, 学习器将从类别标记集合 中预测出一个标记, 最常见的结合策略是使用投票法</p><ul><li><p>绝对多数投票法</p><p>若某标记得票过半数，则预测为该标记；否则拒绝预测</p></li><li><p>相对多数投票法</p><p>预测为得票最多的标记。若同时有多个标记获得最高票，则从中随机选取一个。</p></li><li><p>加权投票法</p></li></ul><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>当训练数据很多时, 一种更为强大的结合策略是使用“学习法”, 即通过另一个学习器来进行结合</p><p>Stacking 是学习法的典型代表</p><h5 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h5><p>Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术</p><p>基础模型通常包含不同的学习算法,利用整个训练集做训练</p><p>元模型将基础模型的特征作为特征进行训练</p><p><img src="https://github.com/benull/Resource/raw/master/stacking.png" alt="stacking"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/zongfa/p/9304353.html" target="_blank" rel="noopener">机器学习—集成学习(Ensemble Learning)</a></p><p><a href="https://blog.csdn.net/zwqjoy/article/details/80431496" target="_blank" rel="noopener">集成学习—bagging、boosting、stacking</a></p><p>机器学习-周志华</p><p>Machine Learning in Action by Peter Harrington</p><p><a href="https://blog.csdn.net/yangyin007/article/details/82385967" target="_blank" rel="noopener">随机森林算法及其实现（Random Forest)</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;集成学习&quot;&gt;&lt;a href=&quot;#集成学习&quot; class=&quot;headerlink&quot; title=&quot;集成学习&quot;&gt;&lt;/a&gt;集成学习&lt;/h1&gt;&lt;p&gt;集成学习(ensemble learning)通过构建并结合多学习器来完成学习任务，常可获得比单一学习器显著优越的泛化性能&lt;
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="Algorithms" scheme="http://yoursite.com/tags/Algorithms/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="集成学习" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2020/10/28/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2020/10/28/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2020-10-28T15:20:29.000Z</published>
    <updated>2020-10-28T15:21:24.176Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一颗熵值下降最快的树，到叶子节点处，熵值为0</p><p>具有非常好的可解释性、分类速度快的优点，是一种有监督学习</p><p>最早提及决策树思想的是Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法</p><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p>一般的，一颗决策树包含一个根结点、若干个内部节点和若干个叶节点</p><h3 id="构造"><a href="#构造" class="headerlink" title="构造"></a>构造</h3><p>构造就是生成一棵完整的决策树。简单来说，构造的过程就是选择什么属性作为节点的过程</p><p>叶结点对应于决策结果, 其他每个结点则对应于一个属性测试，每个结点包含的样本集合根据属性测试的结果被划分到子结点中; </p><p>根结点包含样本全集，从根结点到每个叶结点的路径对应了一个判定测试序列. 决策树学习的目的是为了产生一棵泛化能力强, 即处理未见示例能力强的决策树，其基本流程遵循简单且直观的分而治之策略</p><p>显然, 决策树的生成是一个递归过程. 在决策树基本算法中, 有三种情形会导致递归返回:</p><ol><li>当前结点包含的样本全属于同一类别, 无需划分</li><li>当前属性集为空, 或是所有样本在所有属性上取值相同, 无法划分</li><li>当前结点包含的样本集合为空, 不能划分</li></ol><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h3><p>决策树学习的关键是如何选择最优划分属性</p><p>随着划分过程不断进行, 我们希望决策树的分支结点所包含的样本尽可能属于同一类别, 即结点的“纯度”越来越高</p><h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>信息熵在信息论中代表随机变量不确定度的度量</p><p>熵越大，数据的不确定性越高，纯度越低</p><p>熵越小，数据的不确定性越低，纯度越高</p><p>假定当前样本集合 <script type="math/tex">D</script> 中第 <script type="math/tex">k</script> 类样本所占的比例为 <script type="math/tex">p_{k}(k=1,2, \ldots,|\mathcal{Y}|),</script> 则<script type="math/tex">D</script>的信息嫡定义为</p><script type="math/tex; mode=display">\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}</script><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>假定离散属性 <script type="math/tex">a</script> 有 <script type="math/tex">V</script> 个可能的取值 <script type="math/tex">\left\{a^{1}, a^{2}, \ldots, a^{V}\right\},</script> 若使用 <script type="math/tex">a</script> 来对样本集 <script type="math/tex">D</script> 进行划分, 则会产生 <script type="math/tex">V</script> 个分支结点, 其中第 <script type="math/tex">v</script> 个分支结点包含了 <script type="math/tex">D</script> 中所有在 属性 <script type="math/tex">a</script> 上取值为 <script type="math/tex">a^{v}</script> 的样本, 记为 <script type="math/tex">D^{v} .</script> 我们可根据式 (4.1) 计算出 <script type="math/tex">D^{v}</script> 的信息嫡, 再考虑到不同的分支结点所包含的样本数不同, 给分支结点赋予权重 <script type="math/tex">\left|D^{v}\right| /|D|,</script> 即样本数越多的分支结点的影响越大, 于是可计算出用属性 <script type="math/tex">a</script> 对样本集 <script type="math/tex">D</script> 进行 划分所获得的“信息增益” </p><script type="math/tex; mode=display">\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)</script><p>一般而言, 信息增益越大, 则意味着使用属性 <script type="math/tex">a</script> 来进行划分所获得的“纯度提升”越大. 因此, 我们可用信息增益来进行决策树的划分属性选择,  著名的 ID3 决策树学习算法就是以信息增益为准则来选择划分属性</p><p>ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误</p><h4 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h4><script type="math/tex; mode=display">\text {GainRatio}(D, a)=\frac{\operatorname{Gain}(D, a)}{Ent(D)}</script><p>C4.5 在 IID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低</p><h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>基尼指数是经典决策树CART用于分类问题时选择最优特征的指标</p><p>假设有<script type="math/tex">K</script>个类，样本点属于第<script type="math/tex">k</script>类的概率为<script type="math/tex">p_k</script>，则概率分布的基尼指数定义为</p><script type="math/tex; mode=display">G(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}</script><p>在信息增益、增益率、基尼指数之外, 人们还设计了许多其他的准则用于决策树划分选择</p><p>然而有实验研究表明这些准则虽然对决策树的尺寸有较大影响, 但对泛化性能的影响很有限</p><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>决策树的缺点包括对未知的测试数据未必有好的分类、泛化能力，即可能发生过拟合现象，此时可采用剪枝或随机森林</p><p>剪枝是决策树学习算法对付“过拟合”的主要手段</p><p>在决策树学习中, 为了尽可能正确分类训练样本, 结点划分过程将不断重复, 有时会造成决 策树分支过多, 这时就可能因训练样本学得太好了, 以致于把训练集自身 的一些特点当作所有数据都具有的一般性质而导致过拟合</p><h2 id="ID3-决策树代码"><a href="#ID3-决策树代码" class="headerlink" title="ID3 决策树代码"></a>ID3 决策树代码</h2><p>参考 Machine Learning in Action by Peter Harrington</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding = utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span>:</span></span><br><span class="line">    <span class="string">"""ID3 DecisionTree</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.decisionTree = <span class="literal">None</span></span><br><span class="line">        self._X = <span class="literal">None</span></span><br><span class="line">        self._y = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算信息熵</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(self,y)</span>:</span></span><br><span class="line">        lablesCounter = Counter(y)</span><br><span class="line">        shannonEnt = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> lablesCounter.values():</span><br><span class="line">            p = num / len(y)</span><br><span class="line">            shannonEnt += -p * log(p,<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self._X = X</span><br><span class="line">        self._y = y</span><br><span class="line">        self.decisionTree = self.createTree(self._X,self._y)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">splitDataset</span><span class="params">(self,X,y,d,value)</span>:</span></span><br><span class="line">        features = X[X[:,d]==value]</span><br><span class="line">        labels = y[X[:,d]==value]</span><br><span class="line">        <span class="keyword">return</span> np.concatenate((features[:,:d],features[:,d+<span class="number">1</span>:]),axis=<span class="number">1</span>), labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(self,X,y)</span>:</span></span><br><span class="line">        numFeatures = X.shape[<span class="number">1</span>]</span><br><span class="line">        baseEntropy = self.calcShannonEnt(y)</span><br><span class="line">        bestInfoGain, bestFeature = <span class="number">0.0</span>, <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">            <span class="comment"># 创建唯一的分类标签列表</span></span><br><span class="line">            uniqueVals = np.unique(X[:,i])</span><br><span class="line">            newEntropy =<span class="number">0.0</span></span><br><span class="line">            <span class="comment"># 计算每种划分方式的信息熵</span></span><br><span class="line">            <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">                _x, _y = self.splitDataset(X,y,i,value)</span><br><span class="line">                prob = len(_x)/len(X)</span><br><span class="line">                newEntropy += prob * self.calcShannonEnt(_y)</span><br><span class="line">            infoGain = baseEntropy - newEntropy</span><br><span class="line">            <span class="keyword">if</span> infoGain&gt;bestInfoGain:</span><br><span class="line">                bestInfoGain = infoGain</span><br><span class="line">                bestFeature = i</span><br><span class="line">            <span class="keyword">return</span> bestFeature</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(self,y)</span>:</span></span><br><span class="line">        lablesCounter = Counter(y)</span><br><span class="line">        <span class="keyword">return</span> lablesCounter.most_common(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(self,X,y)</span>:</span></span><br><span class="line">        <span class="comment"># 类别完全相同则停止继续划分</span></span><br><span class="line">        <span class="keyword">if</span> y[y == y[<span class="number">0</span>]].size == y.size :</span><br><span class="line">            <span class="keyword">return</span> y[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 遍历完所有特征时返回出现次数最多的类别</span></span><br><span class="line">        <span class="keyword">if</span> X.shape[<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> self.majorityCnt(y)</span><br><span class="line">        bestFeat = self.chooseBestFeatureToSplit(X,y)</span><br><span class="line">        decisionTree = &#123;bestFeat: &#123;&#125;&#125;</span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> np.unique(X[:,bestFeat]):</span><br><span class="line">            decisionTree[bestFeat][value] = self.createTree(*self.splitDataset(X,y,bestFeat, value))</span><br><span class="line">        <span class="keyword">return</span> decisionTree</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    dataSet = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]])</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>, <span class="string">'flippers'</span>]</span><br><span class="line">    dt = DecisionTree()</span><br><span class="line">    X = dataSet[:, :<span class="number">2</span>]</span><br><span class="line">    X = X.astype(np.int)</span><br><span class="line">    y = dataSet[:,<span class="number">-1</span>]</span><br><span class="line">    dt.fit(X,y)</span><br><span class="line">    print(dt.decisionTree)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://shuwoom.com/?p=1452" target="_blank" rel="noopener">机器学习之-常见决策树算法(ID3、C4.5、CART)</a></p><p><a href="https://www.cnblogs.com/molieren/articles/10664954.html" target="_blank" rel="noopener">决策树</a></p><p><a href="https://zhuanlan.zhihu.com/p/30059442" target="_blank" rel="noopener">决策树(Decision Tree)：通俗易懂之介绍</a></p><p>机器学习-周志华</p><p>Machine Learning in Action by Peter Harrington</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;p&gt;决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一颗熵值下降最快的树，到叶子节点处，熵值为0&lt;/p&gt;
&lt;p&gt;具有
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="Algorithms" scheme="http://yoursite.com/tags/Algorithms/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu设置时区</title>
    <link href="http://yoursite.com/2020/10/22/ubuntu%E8%AE%BE%E7%BD%AE%E6%97%B6%E5%8C%BA/"/>
    <id>http://yoursite.com/2020/10/22/ubuntu%E8%AE%BE%E7%BD%AE%E6%97%B6%E5%8C%BA/</id>
    <published>2020-10-21T16:12:39.000Z</published>
    <updated>2020-10-21T16:09:11.108Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ubuntu设置时区"><a href="#ubuntu设置时区" class="headerlink" title="ubuntu设置时区"></a>ubuntu设置时区</h1><p>查看现在时区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">benull@37c7dedb7a13:~<span class="comment"># date -R</span></span><br><span class="line">Wed, 21 Oct 2020 23:46:05 +0800</span><br></pre></td></tr></table></figure><p>执行<code>tzselect</code>查看时区(只能查看不能修改）</p><p>遇到tzselect报错如下    </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">benull@37c7dedb7a13:~<span class="comment"># tzselect</span></span><br><span class="line">/usr/bin/tzselect: line 180: /usr/share/zoneinfo/iso3166.tab: No such file or directory</span><br><span class="line">/usr/bin/tzselect: time zone files are not <span class="built_in">set</span> up correctly</span><br></pre></td></tr></table></figure><p>解决方案一：安装tzdata</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install tzdata</span><br></pre></td></tr></table></figure><p>解决方案二：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/bin/tzselect</span><br><span class="line">将</span><br><span class="line"><span class="variable">$&#123;TZDIR=pwd&#125;</span></span><br><span class="line">改为</span><br><span class="line"><span class="variable">$&#123;TZDIR=/usr/share/zoneinfo&#125;</span></span><br></pre></td></tr></table></figure><p>继续执行 tzselect</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">benull@37c7dedb7a13:~# tzselect</span><br><span class="line">Please identify a location so that time zone rules can be set correctly.</span><br><span class="line">Please select a continent, ocean, &quot;coord&quot;, or &quot;TZ&quot;.</span><br><span class="line"> 1) Africa</span><br><span class="line"> 2) Americas</span><br><span class="line"> 3) Antarctica</span><br><span class="line"> 4) Asia</span><br><span class="line"> 5) Atlantic Ocean</span><br><span class="line"> 6) Australia</span><br><span class="line"> 7) Europe</span><br><span class="line"> 8) Indian Ocean</span><br><span class="line"> 9) Pacific Ocean</span><br><span class="line">10) coord - I want to use geographical coordinates.</span><br><span class="line">11) TZ - I want to specify the time zone using the Posix TZ format.</span><br><span class="line">#? 4</span><br><span class="line">Please select a country whose clocks agree with yours.</span><br><span class="line"> 1) Afghanistan  18) Israel    35) Palestine</span><br><span class="line"> 2) Armenia  19) Japan    36) Philippines</span><br><span class="line"> 3) Azerbaijan  20) Jordan    37) Qatar</span><br><span class="line"> 4) Bahrain  21) Kazakhstan    38) Russia</span><br><span class="line"> 5) Bangladesh  22) Korea (North)    39) Saudi Arabia</span><br><span class="line"> 6) Bhutan  23) Korea (South)    40) Singapore</span><br><span class="line"> 7) Brunei  24) Kuwait    41) Sri Lanka</span><br><span class="line"> 8) Cambodia  25) Kyrgyzstan    42) Syria</span><br><span class="line"> 9) China  26) Laos    43) Taiwan</span><br><span class="line">10) Cyprus  27) Lebanon    44) Tajikistan</span><br><span class="line">11) East Timor  28) Macau    45) Thailand</span><br><span class="line">12) Georgia  29) Malaysia    46) Turkmenistan</span><br><span class="line">13) Hong Kong  30) Mongolia    47) United Arab Emirates</span><br><span class="line">14) India  31) Myanmar (Burma)    48) Uzbekistan</span><br><span class="line">15) Indonesia  32) Nepal    49) Vietnam</span><br><span class="line">16) Iran  33) Oman    50) Yemen</span><br><span class="line">17) Iraq  34) Pakistan</span><br><span class="line">#? 9</span><br><span class="line">Please select one of the following time zone regions.</span><br><span class="line">1) Beijing Time</span><br><span class="line">2) Xinjiang Time</span><br><span class="line">#? 1</span><br><span class="line"></span><br><span class="line">The following information has been given:</span><br><span class="line"></span><br><span class="line">China</span><br><span class="line">Beijing Time</span><br><span class="line"></span><br><span class="line">Therefore TZ=&apos;Asia/Shanghai&apos; will be used.</span><br><span class="line">Local time is now:Thu Oct 22 00:02:14 CST 2020.</span><br><span class="line">Universal Time is now:Wed Oct 21 16:02:14 UTC 2020.</span><br><span class="line">Is the above information OK?</span><br><span class="line">1) Yes</span><br><span class="line">2) No</span><br><span class="line">#? 1</span><br><span class="line"></span><br><span class="line">You can make this change permanent for yourself by appending the line</span><br><span class="line">TZ=&apos;Asia/Shanghai&apos;; export TZ</span><br><span class="line">to the file &apos;.profile&apos; in your home directory; then log out and log in again.</span><br><span class="line"></span><br><span class="line">Here is that TZ value again, this time on standard output so that you</span><br><span class="line">can use the /usr/bin/tzselect command in shell scripts:</span><br><span class="line">Asia/Shanghai</span><br></pre></td></tr></table></figure><p>复制文件到 <code>/etc/localtime</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/TonyZhao/p/11169840.html" target="_blank" rel="noopener">Ubuntu设置时区和更新时间</a></p><p><a href="https://my.oschina.net/u/914655/blog/3078043" target="_blank" rel="noopener">ubuntu设置时区</a></p><p><a href="https://blog.csdn.net/mashuai720/article/details/78662607" target="_blank" rel="noopener">ubuntu 时区 修改时间 保存 重启 变化等</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ubuntu设置时区&quot;&gt;&lt;a href=&quot;#ubuntu设置时区&quot; class=&quot;headerlink&quot; title=&quot;ubuntu设置时区&quot;&gt;&lt;/a&gt;ubuntu设置时区&lt;/h1&gt;&lt;p&gt;查看现在时区&lt;/p&gt;
&lt;figure class=&quot;highlight ba
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Ubuntu" scheme="http://yoursite.com/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ascii codec can&#39;t encode characters in position</title>
    <link href="http://yoursite.com/2020/10/21/&#39;ascii&#39;%20codec%20can&#39;t%20encode%20characters%20in%20position/"/>
    <id>http://yoursite.com/2020/10/21/&#39;ascii&#39;%20codec%20can&#39;t%20encode%20characters%20in%20position/</id>
    <published>2020-10-21T15:45:32.000Z</published>
    <updated>2020-10-21T15:42:28.769Z</updated>
    
    <content type="html"><![CDATA[<h1 id="‘ascii’-codec-can’t-encode-characters-in-position"><a href="#‘ascii’-codec-can’t-encode-characters-in-position" class="headerlink" title="‘ascii’ codec can’t encode characters in position"></a>‘ascii’ codec can’t encode characters in position</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在ubuntu中运行python3脚本输出到命令行出错</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol><li><p>将<code>LANG</code> 或 <code>LC_ALL</code> 设置为 <code>en_US.utf8</code> 或 <code>zh_CN.utf8</code></p><ul><li>列出当前设置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">benull@37c7dedb7a13:~<span class="comment"># locale</span></span><br><span class="line">LANG=zh_CN.UTF-8</span><br><span class="line">LANGUAGE=</span><br><span class="line">LC_CTYPE=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_NUMERIC=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_TIME=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_COLLATE=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_MONETARY=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_MESSAGES=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_PAPER=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_NAME=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_ADDRESS=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_TELEPHONE=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_MEASUREMENT=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_IDENTIFICATION=<span class="string">"en_US.UTF-8"</span></span><br><span class="line">LC_ALL=en_US.UTF-8</span><br></pre></td></tr></table></figure><ul><li>列出已安装的语言环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">benull@37c7dedb7a13:~<span class="comment"># locale -a</span></span><br><span class="line">C</span><br><span class="line">C.UTF-8</span><br><span class="line">en_US.utf8</span><br><span class="line">POSIX</span><br><span class="line">zh_CN.utf8</span><br></pre></td></tr></table></figure></li><li><p>如果不存在上述的 <code>locale</code>，先用 <code>locale-gen</code> 生成</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt-get install locales</span><br><span class="line">locale-gen <span class="string">'zh_CN.UTF-8'</span></span><br><span class="line">update-locale LC_ALL=<span class="string">"zh_CN.UTF-8"</span></span><br></pre></td></tr></table></figure><ol><li>将<code>export LC_ALL=zh_CN.UTF-8&quot;</code> 加到的<code>~/.bashrc</code></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.v2ex.com/amp/t/422301" target="_blank" rel="noopener">关于’ascii’ codec can’t encode characters in position 的问题</a></p><p><a href="https://blog.csdn.net/wangzhenling/article/details/104781048" target="_blank" rel="noopener">ubuntu locales 设置中文</a></p><p><a href="https://help.ubuntu.com/community/Locale" target="_blank" rel="noopener">Locale</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;‘ascii’-codec-can’t-encode-characters-in-position&quot;&gt;&lt;a href=&quot;#‘ascii’-codec-can’t-encode-characters-in-position&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>k近邻算法(KNN)</title>
    <link href="http://yoursite.com/2020/10/20/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2020/10/20/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</id>
    <published>2020-10-19T16:25:29.000Z</published>
    <updated>2020-10-19T17:09:06.638Z</updated>
    
    <content type="html"><![CDATA[<h1 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h1><p>k近邻算法( k-Nearest Neighbor)是一种监督学习<br>优点:精度高、对异常值不敏感、无数据输入假定<br>缺点:计算复杂度高、空间复杂度高</p><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p>给定测试样本, 基于某种距离度量找出训练集中与其最靠近的 <script type="math/tex">k</script> 个训练样本, 然后基于这 <script type="math/tex">k</script> 个邻居的信息来进行预测</p><p>一般流程包括：</p><ol><li>计算已知类别数据集中的点与当前点之间的距离</li><li>按照距离递增次序排序</li><li>选取与当前点距离最小的k个点</li><li>确定前k个点所在类别的出现频率</li><li>返回前k个点出现频率最高的类别作为当前点的预测分类</li></ol><p>分类任务中可选择这 <script type="math/tex">k</script> 个样本中出现最多的类别标记作为预测结果</p><p>回归任务中可将这 <script type="math/tex">k</script> 个样本的实值输出标记的平均值作为预测结果</p><h2 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h2><p>当 <script type="math/tex">k</script>取不同值时, 分类结果会有显著不同</p><p>K值过小：特征空间被划分为更多子空间，整体模型变复杂，预测结果会对近邻点十分敏感，预测就会出错容易发生过拟合<br>K值过大：近邻误差会偏大，距离较远的点也会同样对预测结果产生影响，使得预测结果产生较大偏差，此时模型容易发生欠拟合</p><h2 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h2><p>若采用不同的距离计算方式也会导致分类结果有显著不同</p><p>对函数 <script type="math/tex">\operatorname{dist}(\cdot, \cdot)</script>，若它是一个距离度量，则需满足一些基本性质</p><script type="math/tex; mode=display">非负性:\operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \geqslant 0</script><script type="math/tex; mode=display">\text { 同一性: } \operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=0 \text { 当且仅当 } \boldsymbol{x}_{i}=\boldsymbol{x}_{j}</script><script type="math/tex; mode=display">\begin{aligned}&\text { 对称性: } \operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\operatorname{dist}\left(\boldsymbol{x}_{j}, \boldsymbol{x}_{i}\right)\\&\text { 直递性: } \operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \leqslant \operatorname{dist}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{k}\right)+\operatorname{dist}\left(\boldsymbol{x}_{k}, \boldsymbol{x}_{j}\right)\end{aligned}</script><p>给定样本 <script type="math/tex">\boldsymbol{x}_{i}=\left(x_{i 1} ; x_{i 2} ; \ldots ; x_{i n}\right)</script> 与 <script type="math/tex">\boldsymbol{x}_{j}=\left(x_{j 1} ; x_{j 2} ; \ldots ; x_{j n}\right)</script><br>最常用的是闵可夫斯基距离(Minkowski distance）</p><script type="math/tex; mode=display">\operatorname{dist}_{\mathrm{mk}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|^{p}\right)^{\frac{1}{p}}</script><p>当 <script type="math/tex">p=2</script> 时, 闵可夫斯基距离即欧氏距离 (Euclidean distance)</p><p>欧几里得空间中两点间直线距离、真实距离或者向量的自然长度</p><script type="math/tex; mode=display">\operatorname{dist}_{\mathrm{ed}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{2}=\sqrt{\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|^{2}}</script><p>当 <script type="math/tex">p=1</script> 时, 闵可夫斯基距离即曼哈顿距离(Manhattan distance)</p><p>在欧几里德空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和，也称街区距离</p><script type="math/tex; mode=display">\operatorname{dist}_{\operatorname{man}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{1}=\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|</script><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>FROM <a href="https://coding.imooc.com/class/169.html" target="_blank" rel="noopener">IMOOC机器学习</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNNClassifier</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        <span class="string">"""初始化kNN分类器"""</span></span><br><span class="line">        <span class="keyword">assert</span> k &gt;= <span class="number">1</span>, <span class="string">"k must be valid"</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self._X_train = <span class="literal">None</span></span><br><span class="line">        self._y_train = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">        <span class="string">"""根据训练数据集X_train和y_train训练kNN分类器"""</span></span><br><span class="line">        <span class="keyword">assert</span> X_train.shape[<span class="number">0</span>] == y_train.shape[<span class="number">0</span>], \</span><br><span class="line">            <span class="string">"the size of X_train must be equal to the size of y_train"</span></span><br><span class="line">        <span class="keyword">assert</span> self.k &lt;= X_train.shape[<span class="number">0</span>], \</span><br><span class="line">            <span class="string">"the size of X_train must be at least k."</span></span><br><span class="line"></span><br><span class="line">        self._X_train = X_train</span><br><span class="line">        self._y_train = y_train</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_predict)</span>:</span></span><br><span class="line">        <span class="string">"""给定待预测数据集X_predict，返回表示X_predict的结果向量"""</span></span><br><span class="line">        <span class="keyword">assert</span> self._X_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> self._y_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>, \</span><br><span class="line">                <span class="string">"must fit before predict!"</span></span><br><span class="line">        <span class="keyword">assert</span> X_predict.shape[<span class="number">1</span>] == self._X_train.shape[<span class="number">1</span>], \</span><br><span class="line">                <span class="string">"the feature number of X_predict must be equal to X_train"</span></span><br><span class="line"></span><br><span class="line">        y_predict = [self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> X_predict]</span><br><span class="line">        <span class="keyword">return</span> np.array(y_predict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""给定单个待预测数据x，返回x的预测结果值"""</span></span><br><span class="line">        <span class="keyword">assert</span> x.shape[<span class="number">0</span>] == self._X_train.shape[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">"the feature number of x must be equal to X_train"</span></span><br><span class="line"></span><br><span class="line">        distances = [sqrt(np.sum((x_train - x) ** <span class="number">2</span>))</span><br><span class="line">                     <span class="keyword">for</span> x_train <span class="keyword">in</span> self._X_train]</span><br><span class="line">        nearest = np.argsort(distances)</span><br><span class="line"></span><br><span class="line">        topK_y = [self._y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:self.k]]</span><br><span class="line">        votes = Counter(topK_y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, X_test, y_test)</span>:</span></span><br><span class="line">        <span class="string">"""根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""</span></span><br><span class="line"></span><br><span class="line">        y_predict = self.predict(X_test)</span><br><span class="line">        <span class="keyword">return</span> self.accuracy_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"KNN(k=%d)"</span> % self.k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">        <span class="string">"""计算y_true和y_predict之间的准确率"""</span></span><br><span class="line">        <span class="keyword">assert</span> len(y_true) == len(y_predict), \</span><br><span class="line">            <span class="string">"the size of y_true must be equal to the size of y_predict"</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> np.sum(y_true == y_predict) / len(y_true)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/pxhdky/article/details/85067808" target="_blank" rel="noopener">LP距离、欧式距离、曼哈顿距离、切比雪夫距离</a></p><p><a href="https://blog.csdn.net/hajk2017/article/details/82862788" target="_blank" rel="noopener">什么是KNN算法？</a></p><p><a href="https://coding.imooc.com/class/169.html" target="_blank" rel="noopener">IMOOC机器学习</a></p><p>机器学习-周志华</p><p>Machine Learning in Action by Peter Harrington</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;k近邻算法&quot;&gt;&lt;a href=&quot;#k近邻算法&quot; class=&quot;headerlink&quot; title=&quot;k近邻算法&quot;&gt;&lt;/a&gt;k近邻算法&lt;/h1&gt;&lt;p&gt;k近邻算法( k-Nearest Neighbor)是一种监督学习&lt;br&gt;优点:精度高、对异常值不敏感、无数据输入假
      
    
    </summary>
    
    
      <category term="AI" scheme="http://yoursite.com/categories/AI/"/>
    
    
      <category term="Algorithms" scheme="http://yoursite.com/tags/Algorithms/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="KNN" scheme="http://yoursite.com/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/2020/10/13/%E5%9F%BA%E4%BA%8E%E9%AA%A8%E6%9E%B6%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E7%9A%84%E7%A9%BA%E9%97%B4%E6%AE%8B%E5%B7%AE%E5%B1%82%E5%92%8C%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E5%9D%97%E5%A2%9E%E5%BC%BA%E7%9A%84%E6%97%B6%E7%A9%BA%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2020/10/13/%E5%9F%BA%E4%BA%8E%E9%AA%A8%E6%9E%B6%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E7%9A%84%E7%A9%BA%E9%97%B4%E6%AE%8B%E5%B7%AE%E5%B1%82%E5%92%8C%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E5%9D%97%E5%A2%9E%E5%BC%BA%E7%9A%84%E6%97%B6%E7%A9%BA%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/</id>
    <published>2020-10-13T11:23:35.000Z</published>
    <updated>2020-11-04T16:25:02.205Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spatial-Residual-Layer-and-Dense-Connection-Block-Enhanced-Spatial-Temporal-Graph-Convolutional-Network-for-Skeleton-Based-Action-Recognition"><a href="#Spatial-Residual-Layer-and-Dense-Connection-Block-Enhanced-Spatial-Temporal-Graph-Convolutional-Network-for-Skeleton-Based-Action-Recognition" class="headerlink" title="Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition"></a>Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition</h1><h2 id="基于骨架动作识别的空间残差层和密集连接块增强的时空图卷积网络"><a href="#基于骨架动作识别的空间残差层和密集连接块增强的时空图卷积网络" class="headerlink" title="基于骨架动作识别的空间残差层和密集连接块增强的时空图卷积网络"></a>基于骨架动作识别的空间残差层和密集连接块增强的时空图卷积网络</h2><h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><div class="table-container"><table><thead><tr><th>作者单位</th><th>会议</th><th>论文地址</th><th>代码</th></tr></thead><tbody><tr><td>江南大学</td><td>ICCV 2019</td><td><a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/SGRL/Wu_Spatial_Residual_Layer_and_Dense_Connection_Block_Enhanced_Spatial_Temporal_ICCVW_2019_paper.pdf" target="_blank" rel="noopener">论文地址</a></td><td>暂无</td></tr></tbody></table></div><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><p>引入了空间残差层来捕获和融合时空特征<br>在先前的工作中，时空层包括空间图卷积和时间卷积。但是不同卷积的序列叠加会混合不同域的信息，从而导致识别不准确。通过引入跨域空间残差卷积，可以增强时空信息</p><p>此外，提出了一个密集连接块来提取全局信息</p><p>它由多个空间残差层组成。在这些层中，可以通过密集连接来传递信息。</p><p>结合上面提到的两个组件来创建了一个时空图卷积网络(ST-GCN),称为SDGCN</p><p><img src="https://pic3.zhimg.com/80/v2-d226e3e1c29a4dcce46c3bb947bd51ee_1440w.jpg" alt="img"></p><blockquote><p>图1.该方法将2D空间卷积与1D时间卷积集成在一起，用于时空特征表示，用于基于骨骼的动作识别。 蓝色方块代表空间图卷积，黄色代表时间卷积。</p></blockquote><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>主要工作：空间残差层和密集连接块增强的时空图卷积网络</p><h3 id="空间残差层Spatial-Residual-Layer-SRL"><a href="#空间残差层Spatial-Residual-Layer-SRL" class="headerlink" title="空间残差层Spatial Residual Layer (SRL)"></a>空间残差层Spatial Residual Layer (SRL)</h3><p>​    ResNet 首先通过引入残差结构来提出残差连接的概念，其中输入节点信息通过恒等映射传递。残差映射的想法是删除相同的主要部分，从而突出显示较小的更改。通过引入残差映射，整个结构对输出的变化更加敏感。残差层可以看作是一个放大器，经过合理的设置，敏感信息会被放大，因此残差连接只需要关心它需要学习的内容即可。</p><p><img src="https://pic3.zhimg.com/80/v2-e3a662c73a31025e27f3dd0e1607462d_1440w.jpg" alt="img"></p><blockquote><p>图2.空间残差ST-GCN层。 下部是ST-GCN层，由空间图卷积和时间卷积组成，上部是空间图卷积。 就像在ResNet中一样，残差连接的输入与ST-GCN层相同，然后将从残差连接获得的输出添加到ST-GCN层的输出中，相加的结果是最终的输出。</p></blockquote><p>​    这里的空间残差连接是跨域的。时空融合网络由空间图卷积分支和时空卷积分支组成。恒等映射是图中的下流。与原始的ResNet不同，此处的恒等映射由图卷积组成，该图卷积也可以视为特殊的双流结构，其中一个流学习静态特征，而另一个流学习时空特征。通过2D空间图卷积，可以提取静态空间特征。由于残差连接，残差图将注意静态空间信息。原始层只需要重视时空信息。这种设计使GCN可以更有效地从视频中学习重要信息。</p><h3 id="密集连接块Dense-Connection-Block-DCB"><a href="#密集连接块Dense-Connection-Block-DCB" class="headerlink" title="密集连接块Dense Connection Block (DCB)"></a>密集连接块Dense Connection Block (DCB)</h3><p>它的结构非常简单，由几个密集的连接块组成。在每个块中，每个图层的特征图都与相同尺度的所有先前特征连接在一起。通过引入密集连接，将重用每一层的特征。一方面，使用少量的计算，可以获得更丰富的特征图。另一方面，重用特征更强大，因此减少了不同层之间的依赖性。</p><p><img src="https://pic2.zhimg.com/80/v2-e536267e21d4c87281fd0a36521bc12a_1440w.jpg" alt="img"></p><blockquote><p>图3.每个密集的连接块都包含几个空间残差层。 在这里，除了第一层或最后一层，每层输入特征的大小与上一层的输出大小完全相同。 </p></blockquote><p>​    在每个密集连接块中，每一层都已连接到所有后续层。 <strong>在通道中将它们全部串联</strong>在一起。 这样，可以在以后的层中重用先前层所提取的大多数信息。 就像DenseNet一样，此块允许整个网络充分利用全局信息。 最重要的是，从特征的角度来看，通过特征重用和旁路设置，可以大大减少网络参数的数量，并在一定程度上缓解了梯度消失的问题。 另一方面，每一层的输入不仅包括前一层的输出，而且还包括其他先前的层。 这也提高了网络的健壮性。</p><h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><p>在这里，将空间残差层和密集连接块组合在一起，以形成最终的体系结构，称为SDGCN。注意，几个空间残差层构成一个块。引入密集连接来连接每个块中的这些层。整个网络结构由3个密集连接块组成。在每个块中，通道大小的设置可以充分利用密集连接。采用原始ST-GCN的设置，这确保了所提出的方法可以应用于常用的ST-GCN结构。</p><p><img src="https://pic2.zhimg.com/80/v2-680b3f6df425460d7b2562e65b59d784_1440w.jpg" alt="img"></p><blockquote><p>图4.左：基于DCB1的SDGCN。 遵循标准的ST-GCN模型来设计模型。右：基于DCB2的SDGCN。 与左图相比，为了充分探讨密集连接的作用，引入了更多的层和密集连接。空心圆表示SRL。</p></blockquote><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p><img src="https://pic2.zhimg.com/80/v2-8ef7b0719590ec00a1b813e71ea54f11_1440w.jpg" alt="img"></p><blockquote><p>表1.在Kinetics数据集上的消融实验。 SRL表示空间残差层，DCB表示密集连接块(DCB1和DCB2)，如图4所示。 报告了在Kinetics数据集上的TOP-1和TOP-5准确率。</p><p>表2.在NTU-RGB+D数据集的消融实验。 我们报告交叉对象和交叉视图数据上的准确性。 其他符号与Tab相同。 只上报TOP-1准确率。</p></blockquote><p>对Kinetics和NTU-RGB + D进行详细的实验比较</p><h4 id="空间残差层"><a href="#空间残差层" class="headerlink" title="空间残差层"></a>空间残差层</h4><p>首先以STGCN 为基准，探索跨域空间残差层的有效性</p><p>与原始结构相比，对于由空间图卷积运算和时间卷积串联组成的每个时空结构，我们向原始网络引入空间残差连接，简称为SRL，并保留其他条件不变</p><p>发现与基线方法相比，带有SRL的ST-GCN表现出明显的改进</p><p>在Kinetics上，性能提高了2.61％</p><p>对于NTU-RGB + D，Cross-Subject和Cross-View的性能分别提高了1.75％和2.76％</p><h4 id="密集连接块"><a href="#密集连接块" class="headerlink" title="密集连接块"></a>密集连接块</h4><p>DCB1基于原始结构，包含10层。为了演示密集连接的作用，设计了DCB2，它包含12层</p><p>具有DCB1的ST-GCN的性能提高了1.51％，具有DCB2的ST-GCN的性能提高了2.48％</p><p>显然，密集的连接为我们的网络做出了很大的贡献，但是，随着密集连接的增加，网络参数的数量迅速增加。此外，盲目的累积网络复杂性可能导致某些数据集的模型过度拟合。</p><h3 id="结果比较"><a href="#结果比较" class="headerlink" title="结果比较"></a>结果比较</h3><p>为了进行全面的比较，选择将我们的方法与两个重要的基线相比：ST-GCN 和2s-AGCN 。第一个基准是基于骨骼的动作识别方面的开创性工作，而2s-AGCN是最新的最佳方法。将SRL和DCB合并在一起以报告最终结果。最终模型表示为SDGCN。</p><p>与基于ST-GCN的方法相比，在Kinetics上的准确性提高到34.06％，在Cross-Subject和Cross-View测试中分别提高到84.04％和91.43％。</p><p>当2s-AGCN作为基准时，在Kinetics上，所提出的方法达到了37.35％的精度。在NTU-RGB + D上，Cross-Subject和Cross-View数据的准确度分别为89.58％和95.74％</p><p><img src="https://pic2.zhimg.com/80/v2-4a022227b16f1873c12a12c5a722e186_1440w.jpg" alt="img"></p><blockquote><p>表3 与目前最先进的方法在Kinetics数据集上的比较</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-86a0945703f3e4cf63505c7d98460068_1440w.jpg" alt="img"></p><blockquote><p>表4.在NTURGB+D上与现有方法的比较</p></blockquote><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>​    提出了一个统一的时空图卷积网络框架，称为SDGCN，以提高基于骨架的动作识别的性能。通过引入跨域空间图残差层和密集连接块，充分利用了时空信息，提高了时空信息处理的效率。可以很容易地将其合并到主流的时空图网络中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Spatial-Residual-Layer-and-Dense-Connection-Block-Enhanced-Spatial-Temporal-Graph-Convolutional-Network-for-Skeleton-Based-Action-Re
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="ActionRecognition" scheme="http://yoursite.com/tags/ActionRecognition/"/>
    
  </entry>
  
  <entry>
    <title>PyQt5 画图板</title>
    <link href="http://yoursite.com/2020/09/29/PyQt5%E7%94%BB%E5%9B%BE%E6%9D%BF/"/>
    <id>http://yoursite.com/2020/09/29/PyQt5%E7%94%BB%E5%9B%BE%E6%9D%BF/</id>
    <published>2020-09-29T12:15:32.000Z</published>
    <updated>2020-11-04T16:26:18.070Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyQt5-画图板"><a href="#PyQt5-画图板" class="headerlink" title="PyQt5 画图板"></a>PyQt5 画图板</h1><h2 id="主要技术"><a href="#主要技术" class="headerlink" title="主要技术"></a>主要技术</h2><ul><li>PyQt5</li><li>qtDesigner</li><li>openCV</li></ul><h2 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h2><ul><li>绘画<ul><li>画笔</li><li>油漆桶</li><li>直线</li><li>矩形</li><li>椭圆</li><li>橡皮擦</li></ul></li><li>图片处理<ul><li>旋转、翻转</li><li>亮度、饱和度、对比度、色调调节</li><li>灰度化</li><li>二值化</li><li>反相（反色）</li><li>浮雕</li><li>边缘检测</li><li>模糊</li><li>锐化</li></ul></li></ul><h2 id="详细代码"><a href="#详细代码" class="headerlink" title="详细代码"></a>详细代码</h2><p><a href="https://github.com/BENULL/Paint" target="_blank" rel="noopener">github仓库</a></p><h2 id="实现过程遇到的问题"><a href="#实现过程遇到的问题" class="headerlink" title="实现过程遇到的问题"></a>实现过程遇到的问题</h2><h3 id="在pycharm上使用qtDesigner"><a href="#在pycharm上使用qtDesigner" class="headerlink" title="在pycharm上使用qtDesigner"></a>在pycharm上使用qtDesigner</h3><p>配置qtDesigner</p><p>配置UIC</p><p>参考 <a href="https://blog.csdn.net/u013667527/article/details/97657621" target="_blank" rel="noopener">Mac下pycharm+qtdesigner环境搭建</a></p><h3 id="绘图时图像不能留存或重影问题"><a href="#绘图时图像不能留存或重影问题" class="headerlink" title="绘图时图像不能留存或重影问题"></a>绘图时图像不能留存或重影问题</h3><p>采取双缓冲绘图方法</p><p>我们再添加一个辅助画布，如果正在绘图，也就是鼠标按键还没有释放的时候，就在这个辅助画布上绘图，只有当鼠标按键释放的时候，才在真正的画布上绘图</p><p>参考<a href="http://shouce.jb51.net/qt-beginning/22.html" target="_blank" rel="noopener">2D绘图（八）双缓冲绘图</a></p><h3 id="油漆桶Flood-Fill算法问题"><a href="#油漆桶Flood-Fill算法问题" class="headerlink" title="油漆桶Flood Fill算法问题"></a>油漆桶Flood Fill算法问题</h3><p>泛洪算法—Flood Fill，用于确定连接到多维数组中给定节点的区域。</p><p>基本原理就是从一个像素点出发，以此向周边的像素点扩充着色，直到图形的边界。</p><p>实现方法包括传统递归方式dfs、bfs和描绘线算法(Scanline Fill)等</p><p>在QImage上实现效率很低，因为getPixel操作很慢，可以进一步优化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPixel</span><span class="params">(x,y,pixels,w)</span>:</span></span><br><span class="line">    i = (x + (y * w)) * <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> pixels[i:i + <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 油漆桶</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">floodFill</span><span class="params">(image,pos)</span>:</span></span><br><span class="line">    fillPositions = []</span><br><span class="line">    w, h = image.width(), image.height()</span><br><span class="line">    pixels = image.bits().asstring(w * h * <span class="number">4</span>)</span><br><span class="line">    targetColor = getPixel(pos.x(), pos.y(), pixels, w)</span><br><span class="line"></span><br><span class="line">    haveSeen = set()</span><br><span class="line">    queue = [(pos.x(), pos.y())]</span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        x, y = queue.pop()</span><br><span class="line">        <span class="keyword">if</span> getPixel(x, y,pixels,w) == targetColor:</span><br><span class="line">            fillPositions.append((x,y))</span><br><span class="line">            queue.extend(getCardinalPoints(haveSeen, (x, y),w,h))</span><br><span class="line">    <span class="keyword">return</span> fillPositions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCardinalPoints</span><span class="params">(haveSeen, centerPos,w,h)</span>:</span></span><br><span class="line">    points = []</span><br><span class="line">    cx, cy = centerPos</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">-1</span>)]:</span><br><span class="line">        xx, yy = cx + x, cy + y</span><br><span class="line">        <span class="keyword">if</span> (xx &gt;= <span class="number">0</span> <span class="keyword">and</span> xx &lt; w <span class="keyword">and</span> yy &gt;= <span class="number">0</span> <span class="keyword">and</span> yy &lt; h <span class="keyword">and</span> (xx, yy) <span class="keyword">not</span> <span class="keyword">in</span> haveSeen):</span><br><span class="line">            points.append((xx, yy))</span><br><span class="line">            haveSeen.add((xx, yy))</span><br><span class="line">    <span class="keyword">return</span> points</span><br></pre></td></tr></table></figure><p>参考<a href="https://www.learnpyqt.com/blog/implementing-qpainter-flood-fill-pyqt5pyside/" target="_blank" rel="noopener">Implementing QPainter flood fill in PyQt5/PySide</a></p><p><a href="https://www.pianshen.com/article/172962944/" target="_blank" rel="noopener">图像分割经典算法—《泛洪算法》（Flood Fill)</a></p><h3 id="Mac上pyqt5-与-cv库冲突问题"><a href="#Mac上pyqt5-与-cv库冲突问题" class="headerlink" title="Mac上pyqt5 与 cv库冲突问题"></a>Mac上pyqt5 与 cv库冲突问题</h3><p>问题<code>You might be loading **two sets of Qt binaries** into the same process</code></p><p>删除原有的opencv       </p><p><code>pip3 uninstall opencv-python</code></p><p>安装opencv–headless版本</p><p><code>pip3 install opencv-contrib-python-headless</code></p><p>参考<a href="https://blog.csdn.net/qq_43444349/article/details/106602543" target="_blank" rel="noopener">Mac下使用opencv与pyqt发生冲突</a></p><h3 id="pyqt-QImage-与opencv-MAT格式转化问题"><a href="#pyqt-QImage-与opencv-MAT格式转化问题" class="headerlink" title="pyqt QImage 与opencv MAT格式转化问题"></a>pyqt QImage 与opencv MAT格式转化问题</h3><p>在使用opencv过程中需要传入QImage对象进行处理</p><p>QImage转化成opencv下的 MAT(numpy ndarray) 对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CvMatToQImage</span><span class="params">(cvMat)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(cvMat.shape) == <span class="number">2</span>:</span><br><span class="line">        rows, columns = cvMat.shape</span><br><span class="line">        bytesPerLine = columns</span><br><span class="line">        <span class="keyword">return</span> QImage(cvMat.data, columns, rows, bytesPerLine, QImage.Format_Indexed8)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        rows, columns, channels = cvMat.shape</span><br><span class="line">        bytesPerLine = channels * columns</span><br><span class="line">        <span class="keyword">return</span> QImage(cvMat.data, columns, rows, bytesPerLine, QImage.Format_RGBA8888)</span><br></pre></td></tr></table></figure><p>MAT(numpy ndarray) 转QImage</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">QImageToCvMat</span><span class="params">(incomingImage)</span>:</span></span><br><span class="line">    incomingImage = incomingImage.convertToFormat(QImage.Format_RGBA8888)</span><br><span class="line">    width = incomingImage.width()</span><br><span class="line">    height = incomingImage.height()</span><br><span class="line">    ptr = incomingImage.bits()</span><br><span class="line">    ptr.setsize(height * width * <span class="number">4</span>)</span><br><span class="line">    arr = np.frombuffer(ptr, np.uint8).reshape((height, width, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure><p>参考<a href="https://blog.csdn.net/lch551218/article/details/104882183/" target="_blank" rel="noopener">Python 中如何将 Pyqt5 下的 QImage 对象转换成 PIL image 或 opencv MAT (numpy ndarray) 对象</a></p><h2 id="效果预览"><a href="#效果预览" class="headerlink" title="效果预览"></a>效果预览</h2><h3 id="绘画"><a href="#绘画" class="headerlink" title="绘画"></a>绘画</h3><p><img src="https://b1.sbimg.org/file/chevereto-jia/2020/09/30/GZ6Oo.png" alt></p><h3 id="油漆桶效果"><a href="#油漆桶效果" class="headerlink" title="油漆桶效果"></a>油漆桶效果</h3><p><img src="https://sbimg.cn/images/2020/09/30/GZpxa.png" alt></p><h3 id="图像处理部分展示"><a href="#图像处理部分展示" class="headerlink" title="图像处理部分展示"></a>图像处理部分展示</h3><h3 id="原图"><a href="#原图" class="headerlink" title="原图"></a>原图</h3><p><img src="https://b1.sbimg.org/file/chevereto-jia/2020/09/30/GZGDl.png" alt></p><h3 id="亮度调节"><a href="#亮度调节" class="headerlink" title="亮度调节"></a>亮度调节</h3><p><img src="https://b1.sbimg.org/file/chevereto-jia/2020/09/30/GZ3zw.png" alt></p><h3 id="色调调节"><a href="#色调调节" class="headerlink" title="色调调节"></a>色调调节</h3><p><img src="https://sbimg.cn/images/2020/09/30/GZmuM.png" alt></p><h3 id="反相"><a href="#反相" class="headerlink" title="反相"></a>反相</h3><p><img src="https://b1.sbimg.org/file/chevereto-jia/2020/09/30/GZoGT.png" alt></p><h3 id="灰度化"><a href="#灰度化" class="headerlink" title="灰度化"></a>灰度化</h3><p><img src="https://sbimg.cn/images/2020/09/30/GZ0mR.png" alt></p><h3 id="二值化"><a href="#二值化" class="headerlink" title="二值化"></a>二值化</h3><p><img src="https://sbimg.cn/images/2020/09/30/GZ2dI.png" alt></p><h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3><p><img src="https://sbimg.cn/images/2020/09/30/GZCZK.png" alt></p><h2 id="部分参考"><a href="#部分参考" class="headerlink" title="部分参考"></a>部分参考</h2><p><a href="https://www.cnblogs.com/lfri/p/10599420.html" target="_blank" rel="noopener">https://www.cnblogs.com/lfri/p/10599420.html</a><a href="https://blog.csdn.net/qq_43444349/article/details/106602543" target="_blank" rel="noopener">https://blog.csdn.net/qq_43444349/article/details/106602543</a><br><a href="https://www.pianshen.com/article/172962944/" target="_blank" rel="noopener">https://www.pianshen.com/article/172962944/</a><br><a href="https://blog.csdn.net/lzwarhang/article/details/93209166" target="_blank" rel="noopener">https://blog.csdn.net/lzwarhang/article/details/93209166</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PyQt5-画图板&quot;&gt;&lt;a href=&quot;#PyQt5-画图板&quot; class=&quot;headerlink&quot; title=&quot;PyQt5 画图板&quot;&gt;&lt;/a&gt;PyQt5 画图板&lt;/h1&gt;&lt;h2 id=&quot;主要技术&quot;&gt;&lt;a href=&quot;#主要技术&quot; class=&quot;headerli
      
    
    </summary>
    
    
      <category term="Project" scheme="http://yoursite.com/categories/Project/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Project" scheme="http://yoursite.com/tags/Project/"/>
    
      <category term="Qt" scheme="http://yoursite.com/tags/Qt/"/>
    
  </entry>
  
  <entry>
    <title>Spatio-Temporal Graph Routing for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/2020/08/21/%E5%9F%BA%E4%BA%8E%E9%AA%A8%E6%9E%B6%E7%9A%84%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E6%97%B6%E7%A9%BA%E5%9B%BE%E8%B7%AF%E7%94%B1%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2020/08/21/%E5%9F%BA%E4%BA%8E%E9%AA%A8%E6%9E%B6%E7%9A%84%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E6%97%B6%E7%A9%BA%E5%9B%BE%E8%B7%AF%E7%94%B1%E6%96%B9%E6%B3%95/</id>
    <published>2020-08-21T10:22:35.000Z</published>
    <updated>2020-11-04T16:25:56.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spatio-Temporal-Graph-Routing-for-Skeleton-Based-Action-Recognition"><a href="#Spatio-Temporal-Graph-Routing-for-Skeleton-Based-Action-Recognition" class="headerlink" title="Spatio-Temporal Graph Routing for Skeleton-Based Action Recognition"></a>Spatio-Temporal Graph Routing for Skeleton-Based Action Recognition</h1><blockquote><p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4875/4748" target="_blank" rel="noopener">paper: https://www.aaai.org/ojs/index.php/AAAI/article/view/4875/4748</a></p></blockquote><h1 id="基于骨架的动作识别时空图路由方法"><a href="#基于骨架的动作识别时空图路由方法" class="headerlink" title="基于骨架的动作识别时空图路由方法"></a>基于骨架的动作识别时空图路由方法</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    基于骨架的人体动作识别由于其表征的有效性，受到了相当多的研究关注，并具有广泛的实际应用。 在这一领域，现有的许多方法通常依赖于固定的骨架结构物理连通进行识别，这不能很好地捕捉到骨架关节之间的内在高阶相关性。 本文提出了一种新的基于骨架的动作识别时空图路由(STGR)方案，该方案自适应地学习物理上分离的骨架关节的内在高阶连通性关系。具体而言，该方案由空间图路由器(SGR)和时间图路由器(TGR)两部分组成。 SGR的目标是基于空间维度上的子群聚类来发现关节之间的连通关系，而TGR则通过测量时间关节节点轨迹之间的关联度来探索结构信息。该方案自然、无缝地融入到图卷积网络(GCNS)的框架中，生成一组骨架-关节连通性图，并进一步馈送到分类网络中。 此外，还对图节点的感受野进行了深入的分析，说明了该方法的必要性。 在两个基准数据集(NTU-RGB+D和Kinetics)上的实验结果证明了该方法的有效性。</p><p><img src="https://wx1.sbimg.cn/2020/08/10/oHIXk.png" alt="Figure 1"></p><blockquote><p>图1：三种路由方式示意图：（a）物理连接固定路由；（b）考虑局部聚类的空间路由；（c）通过节点轨迹关联度建模的时间路由。</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>​    作为计算机视觉中一个具有挑战性的问题，基于骨骼的人体动作识别以三维人体坐标为输入，输出动作类别，近年来受到越来越多的关注。 典型地，人体骨骼将几何身体结构表征为精准的身体，它们的动力学以连续的方式捕捉运动模式。这种动态的几何结构不仅在空间上而且在时间上表达了关节之间的关系。通过这种方式，图形表示是表达人的内在结构的自然方式。因此，在给定的图上表示关节是至关重要的。最近时空图卷积网络（ST-GCN）的成功证明了基于物理人体骨架的图聚合方案的有效性，并与pseduo图像等现有文献进行了对比。</p><p>​    一般情况下，基于图的方法将固定的人体骨架应用于图的卷积运算，并将隐藏特征与邻域特征进行迭代聚合。然而，在复杂的场景中捕捉多变的人体结构是一个挑战。这给进一步改进带来了三方面的问题：1）骨架本身是可变的，并且依赖于特定的数据集，例如NTURGB+D中的25个关节而Kinetics中有18个关节，导致对真实人体骨骼的混淆；2）关节连接高度不平衡。当躯干关节过度平滑时，肢体关节可能仍然处于欠平滑状态，这给两个肢体关节的特征共享带来极大困难；3)对每个样本应用全局图结构，提出了“一刀切”的问题，这可能是次优的。 在固定图的情况下，数据流被限制在预定义的条目中，这大大降低了模型的灵活性。 类似于计算机网络，我们将其称为“静态路由”。</p><p>​    相比之下，我们更注重寻求更灵活的连接方案，它针对特定的样本自适应地学习骨架关节之间固有的高阶连通性，称为“动态路由”。 在现实世界的场景中，动态骨架本身嵌入了丰富的信息，这些信息隐含地显示了物理上分开的两个关节之间的强连接，例如动作类“拍手”中的两个手关节。 因此，我们将这个动态路由问题描述为一个图拓扑学问题，它自动为所有节点选择信息最丰富的连接。 我们证明了动态路由方案和静态路由方案在该任务中同等重要。</p><p>​    基于这个观察结果，我们把这个问题描述成一个关节学习问题。我们首先通过骨架的位置和运动来学习动态图拓扑，然后将其作为GCN识别框架的先验知识。特别地，我们提出了一种新的时空图路由（STGR）方案，以分离的方式对节点之间的语义连接进行建模。两个子网络不使用固定的人体骨架，而是负责捕捉每两个节点之间的空间和时间依赖性，充当所有节点的路由器。如图1所示，一个空间图路由器（SGR）基于沿空间维度的子组聚类来发现节点之间的连通关系。时态图路由器（TGR）通过测量时间节点轨迹之间的关联度来挖掘结构信息。然后将时空骨架连接图以多种路由方式输入ST-GCN。</p><p>​    为了说明这一必要性，我们进一步引入了“图上的感受野”，类比于CNNs中的同义词。 图上的感受野是指节点可以从中提取信息的覆盖范围。 通过引入这一概念，我们证明了固定的人体骨骼会导致高度不平衡的问题，这是可以通过我们的工作来解决的。</p><p>我们的贡献可以总结如下：</p><ul><li>我们提出了一种新的时空图路由方案，用于利用骨骼关节之间固有的高阶关系。 该模块与分类网络联合学习，更好地匹配动作识别任务。</li><li>我们在图节点上提出了感受野，证明了以前模型的瓶颈是不同节点的感受野的不平衡，从而说明了该时空图路由方案的有效性。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><img src="https://wx1.sbimg.cn/2020/08/10/oHVnj.png" alt="Figure 2"></p><blockquote><p>图2：时空图路由器概述。首先将输入的三维骨架序列分别转换为逐帧骨架和节点轨迹。然后空间图路由器（SGR）和时间图路由器（TGR）分别生成新的骨架连接图。ST-GCN接收该图并输出动作类。</p></blockquote><h3 id="基于骨架的动作识别"><a href="#基于骨架的动作识别" class="headerlink" title="基于骨架的动作识别"></a>基于骨架的动作识别</h3><p>​    传统的基于骨架的动作识别方法主要集中在手工制作的特征来捕捉关节运动的动态，如轨迹的协方差矩阵、Lie Group。</p><p>​    随着深度学习的成功，许多基于CNN的方法以端到端的方式被提出，为了更好地利用已有的强大结构，人们通过多种方式将原始骨架转换为pseduo图像，包括skepxels、temporal-then-spatial recalibration scheme，以及jointwise co-occurence。</p><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>另一方面，有效地对时间依赖进行建模。 LSTM和GRU被提出用于学习序列的时间上下文。 为了更好地处理复杂的时空变化因素，提出了注意力机制来保证健壮性的要求，如关键帧的选择和全球信息关节挖掘。</p><h3 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h3><p>图神经网络（GCN）可以大致分为两个流</p><h4 id="谱域"><a href="#谱域" class="headerlink" title="谱域"></a>谱域</h4><p>它基于图傅立叶变换（GFT），在基图进行变换。 通过Klocalized卷积的参数化，最近实现了计算有效和局部滤波。为了降低计算拉普拉斯矩阵特征值的昂贵成本，引入了切比雪夫多项式作为截断展开</p><h4 id="空域"><a href="#空域" class="headerlink" title="空域"></a>空域</h4><p>另一方面，学习迭代地聚合每个节点的邻域作为其新的隐藏表示。提出了一种一阶近似成功地应用于半监督分类。与此同时，一些文献探索了将图快速转换为序列并利用一维卷积网络的方法。</p><p>​    最近，有一些工作试图用任一度量学习或跳跃知识网络来揭示GCNS的机制。 然而，目前的方法主要集中在半监督分类问题上。 在这项工作中，我们首先借助图上的感受野对基于骨架的动作识别进行了分析。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>​    在这一部分中，我们首先阐述我们的问题，然后通过描述两个子网络SGR和TGR来介绍我们的时空图路由（STGR）方案。稍后我们将描述总体架构和优化。最后，我们在图上讨论感受野，进一步验证STGR的必要性。</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>​    三维人体骨架表示为 <script type="math/tex">X=\left\{x _ {n}^{t}\right\} \in {\mathbb{R}^{C_{\text {in }} \times T \times N}}</script> 其中有 <script type="math/tex">T</script> 帧 和<script type="math/tex">N</script> 个关节点。 在第<script type="math/tex">t</script>个时间步的第<script type="math/tex">n</script>个关节，每个个体表示为 <script type="math/tex">xyz</script>-坐标特征向量 ，因此 <script type="math/tex">C_{\mathrm{in}}=3</script>。为了更加方便，我们将单个帧骨架表示为 <script type="math/tex">X^{t} \in \mathbb{R}^{C_{\text {in}} \times N }</script> 对于第 <script type="math/tex">t</script>  帧和关节轨迹表示为  <script type="math/tex">X_{n} \in \mathbb{R}^{C_{\text {in }} \times T}</script> 对于第 <script type="math/tex">n</script> 个关节。</p><p>​    <script type="math/tex">A \in \mathbb{R}^{N \times N}</script> 是简单的领接矩阵， <script type="math/tex">A_{i j}</script> 表示关节 <script type="math/tex">i</script>和 关节 <script type="math/tex">j</script> 是相连的。<script type="math/tex">D</script> 是相应的度矩阵。默认的图由胰以下表示:</p><script type="math/tex; mode=display">\mathcal{G}^{\text {default }}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}</script><p>其中 <script type="math/tex">\tilde{A}=A+I</script> 是包括节点自环的广义邻接矩阵。 <script type="math/tex">\tilde{D}</script> 是<script type="math/tex">\tilde{A}</script> 相应的度矩阵。因此， <script type="math/tex">\mathcal{G}^{\text {default }}</script> 是<script type="math/tex">\tilde{A}</script>的对角规范化矩阵。 如前所述，固定的人体骨架不足以模拟复杂场景中多变的人体结构。我们的目标是学习从原始骨架到图拓扑表示的映射 <script type="math/tex">X \rightarrow \mathcal{G}</script> 在多个视图中，例如姿势和运动。因此，我们有：</p><script type="math/tex; mode=display">\left\{\mathcal{G}^{\text {spat }}, \mathcal{G}^{\text {temp }}\right\}=f_{\text {STGR }}\left(X ; \theta^{\text {spat }}, \theta^{\text {temp }}\right)</script><p>其中<script type="math/tex">\mathcal{G}^{\text {spat }}</script> 和 <script type="math/tex">\mathcal{G}^{\text {temp }}</script>是时空图拓扑表示。 <script type="math/tex">\theta^{\text {spat }}</script> 和 <script type="math/tex">\theta^{\text {temp }}</script>代表相应的参数。 <script type="math/tex">\mathcal{G}^{\text {spat }}</script> 和 <script type="math/tex">\mathcal{G}^{\text {temp }}</script> 将与默认图 <script type="math/tex">\mathcal{G}^{\text {default }}</script> 连接，形成一个图集 <script type="math/tex">\mathcal{S}=\left\{\mathcal{G}^{\text {default }}, \mathcal{G}^{\text {spat }}, \mathcal{G}^{\text {temp }}\right\}</script> 。在下面的部分中，我们将对两个子网进行详细的描述。</p><h3 id="空间图路由器子网"><a href="#空间图路由器子网" class="headerlink" title="空间图路由器子网"></a>空间图路由器子网</h3><p><img src="https://wx1.sbimg.cn/2020/08/10/oHyLN.png" alt="Figure 3"></p><blockquote><p>图3：空间图路由器(SGR)的网络结构。</p></blockquote><p>​    在现实世界中，关节通常聚集在一个组中以表达特定动作。 换句话说，每个关节的位置和成对关节之间的距离编码了关系的强度，这对引导信息流至关重要。</p><h4 id="空间图池"><a href="#空间图池" class="headerlink" title="空间图池"></a>空间图池</h4><p>​    为了提取空间连通图，我们首先对每个帧骨架使用非参数图割聚类方法(Shii和Malik2000) <script type="math/tex">X^{t} \in \mathbb{R}^{C_{\text {in }} \times N}</script>  形成了<script type="math/tex">K</script>个子组。对于每个子组，我们将其视为一个完全连通图，即每个子群中的每两个节点都是连通的。这样，我们为每个帧<script type="math/tex">t</script>定义一个空间连接的图，并将所有这些图聚集在一起以形成“空间图池”:</p><script type="math/tex; mode=display">\mathcal{G}=\left\{\mathcal{G}^{1}, \ldots, \mathcal{G}^{T}\right\}</script><p>其中对于每个<script type="math/tex">\mathcal{G}^{t}</script> :</p><script type="math/tex; mode=display">\mathcal{G}_{i j}^{t}=\left\{\begin{array}{ll}1, & \text { if } i \text { and } j \text { in the same sub-group } \\0, & \text { otherwise }\end{array}\right.</script><h4 id="挤压和激励注意力"><a href="#挤压和激励注意力" class="headerlink" title="挤压和激励注意力"></a>挤压和激励注意力</h4><p>​    由于我们已经获得了一系列空间连通图，因此我们的目标是选择信息最丰富的一个作为代表。<br> 为此，提出了一种用于图融合的联合学习帧重要性的帧注意方案 。</p><p>​    如图3所示，我们以挤压和激发的方式对框架注意力进行建模。首先采用较大的<script type="math/tex">7\times7</script>卷积来聚合局部特征。 然后通过全局平均汇聚层进行挤压操作，以获得中间特征：</p><script type="math/tex; mode=display">m_{t}^{\mathrm{in}}=\frac{1}{N \times N} \sum_{i=1}^{N} \sum_{j=1}^{N} f_{\mathrm{Conv}}\left(\mathcal{G}^{t}\right)_{i j}</script><p>其中 <script type="math/tex">m^{\text {in }}=\left(m_{1}^{\mathrm{in}}, \ldots, m_{T}^{\mathrm{in}}\right)</script> 表示时间上的中间特征的集合。 由于    <script type="math/tex">m^{\text {in }}</script>包含整个图的完整信息，因此激励操作可以对帧之间的内部依赖性进行建模。</p><script type="math/tex; mode=display">\mu=\operatorname{Sigmoid}\left(W_{2} \cdot \sigma\left(W_{1} \cdot m^{\mathrm{in}}\right)\right)</script><p>其中<script type="math/tex">\mu=\left(\mu_{1}, \ldots, \mu_{T}\right)</script>表示每帧的重要性分数。 <script type="math/tex">W_{1} \in \mathbb{R}^{\frac{T}{r} \times T}</script>和<script type="math/tex">W_{2} \in \mathbb{R}^{T \times \frac{T}{r}}</script>是<script type="math/tex">1 \times 1</script>变换矩阵。 <script type="math/tex">r</script>是降维参数，<script type="math/tex">\sigma</script>是RELU激活函数。 该降维方案主要用于挖掘时间维之间的关系。 我们对每个帧重要性<script type="math/tex">\mu_{t}</script>进行加权融合，以形成<script type="math/tex">\mathcal{G}^{\text {spat }}</script> :</p><script type="math/tex; mode=display">\mathcal{G}^{\mathrm{spat}}=\frac{1}{T} \sum_{t=1}^{T} \mu_{t} \cdot \mathcal{G}^{t}</script><h3 id="时间图路由器子网"><a href="#时间图路由器子网" class="headerlink" title="时间图路由器子网"></a>时间图路由器子网</h3><p><img src="https://wx1.sbimg.cn/2020/08/10/oHTCD.png" alt="Figure 4"></p><blockquote><p>图4：时态图路由器（TGR）的网络结构。</p></blockquote><p>​    与SGR子网不同，TGR子网从全局的角度考虑时空图。 根据简单的观察，关联度高的关节通常暗示着密切的关系。 例如，在“行走”类中，手和脚高度相关(向相反方向摆动)，表明了区别关系。 在这一思想的启发下，TGR首先用LSTM编码器对每个关节的轨迹进行编码，然后以一种自关注的方式对每对关节的关系进行建模。</p><h4 id="LSTM编码器"><a href="#LSTM编码器" class="headerlink" title="LSTM编码器"></a>LSTM编码器</h4><p>​    如上所述，TGR首先将输入序列重新排列为<script type="math/tex">N</script>个独立的节点轨迹 <script type="math/tex">X_{n}, n=1, \ldots, N,</script>，每个都被视为。如<script type="math/tex">X_{n} \in {\mathbb{R}^{C_{\text {in }} \times T} }</script>。如图4所示，LSTM单元首先对每个输入节点轨迹进行编码，并在最后一个时间步输出隐藏状态：</p><script type="math/tex; mode=display">h_{n, t}=\psi\left(X_{n}, h_{n, t-1}\right)</script><p>其中<script type="math/tex">\psi</script>表示LSTM模块。 <script type="math/tex">h_{n, t}</script>是最后一个时间点的输出。 然后将<script type="math/tex">h_{n, t}</script>作为关系建模网络的输入，以捕获两个节点之间的交互。为清楚起见，对于每个节点，我们将<script type="math/tex">h_{n, t}</script>表示为<script type="math/tex">v_{n}</script>。</p><h4 id="关系建模"><a href="#关系建模" class="headerlink" title="关系建模"></a>关系建模</h4><p>​    我们在编码特征空间中建立成对节点关系模型。类似于最近的工作，我们用标准化点积来度量这种关系。特别地，给定每个轨迹的编码特征<script type="math/tex">v=\left[v_{1}, \ldots, v_{N}\right]</script>，成对相似性建议为：</p><script type="math/tex; mode=display">D\left(v _ {i}, v _ {j}\right)=\theta\left(v _ {i}\right)^{\top} \cdot \varphi\left(v _ {j}\right)</script><p>其中，<script type="math/tex">\theta</script>和<script type="math/tex">\varphi</script>是两个<script type="math/tex">1 \times 1</script>转换操作。 通过执行点积，我们通过两个节点的余弦距离来检查它们。 在计算了每个成对距离之后，我们在每一行中进一步应用softmax操作，确保单个节点的所有条目之和将被设置为1。</p><script type="math/tex; mode=display">\mathcal{G} _ {i j}^{\mathrm{temp}}=\frac{\exp D\left(v _ {i}, v _ {j}\right)}{\sum _ {k=1}^{N} \exp D\left(v _ {k}, v _ {j}\right)}</script><h3 id="网络体系结构与优化"><a href="#网络体系结构与优化" class="headerlink" title="网络体系结构与优化"></a>网络体系结构与优化</h3><p>​    在本节中，我们将介绍整个网络体系结构。我们的模型是由STGR和ST-GCN构造的。特别地，STGR负责探索空间和时间域中语义相关关节的内在连接关系。ST-GCN以三维骨架和图形为输入输出动作类。具体而言，ST-GCN堆叠多个“GCN-TCN”单元用于表示学习，其中每个“GCN-TCN”单元被视为一层。每个GCN单元在空间维上用默认图<script type="math/tex">\mathcal{G}^{\text {default}}</script>和学习图<script type="math/tex">\mathcal{G}^{\text {spat }}</script>和<script type="math/tex">\mathcal{G}^{\text {temp }}</script>进行图形卷积运算，在时间维上用TCN单元得到高级特征图。</p><p>​    为了清楚起见，假设第<script type="math/tex">l</script>层中特定节点<script type="math/tex">n</script>的隐藏特征用<script type="math/tex">h_{v}^{l} \in \mathbb{R}^{d_{l}}</script>表示。为了一致性，我们假设<script type="math/tex">h^{0}=X</script>，<script type="math/tex">d_{0}=C_{\text {in}}</script>。原始ST-GCN可解释为：</p><script type="math/tex; mode=display">h_{v}^{l+1}=\sigma\left(\left(M \otimes \mathcal{G}^{\text {default }}\right) h_{v}^{l} w^{l}\right)+h_{v}^{l}</script><p>其中<script type="math/tex">M</script>表示一个可学习的掩码，以进一步扩大模型的表达能力。 <script type="math/tex">\otimes</script>是基于元素的乘积，并且<script type="math/tex">w^{l}</script>表示紧接在图形卷积之后的规则卷积运算。 与我们的STGR一起，我们分别为每一层生成了一系列的空间和时间图<script type="math/tex">\mathcal{G}_{i}^{\text {spat }}, \mathcal{G}_{i}^{\text {temp }}, i=1, \ldots, L</script>。 我们不共享STGR的每个单元的权重，因为该模型是轻量级的。</p><p>​    因此，时空图被嵌入到每个“GCN-TCN”单元中。关节不仅可以从固定骨架聚合特征，还可以从这些学习的语义连接中聚合特征：</p><script type="math/tex; mode=display">h _ {v}^{l+1}=\sigma\left(\sum _ {\mathcal{G} \in \mathcal{S}}\left(M _ {\mathcal{G}} \otimes \mathcal{G}\right) h _ {v}^{l} w _ {\mathcal{G}}^{l}\right)+h _ {v}^{l}</script><p>其中<script type="math/tex">\mathcal{S}=\left\{\mathcal{G}^{\text {default }}, \mathcal{G}^{\text {spat }}, \mathcal{G}^{\text {temp }}\right\} .M_{\mathcal{G}}</script> 和 <script type="math/tex">w_{\mathcal{G}}^{l}</script>是特定图的对应掩码和卷积。我们堆叠多个GCN-TCN单元，然后应用全局平均池化和全连接层来获得动作分数<script type="math/tex">\hat{y}</script>：</p><script type="math/tex; mode=display">\hat{y}=f_{\mathrm{ST}-\mathrm{GCN}}\left(X ; \mathcal{G}_{1}^{\mathrm{spat}}, \mathcal{G}_{1}^{\mathrm{temp}}, \ldots \mathcal{G}_{L}^{\mathrm{spat}}, \mathcal{G}_{L}^{\mathrm{temp}}\right)</script><p>我们采用标准交叉熵损失进行分类。 对于这两个子网，为了保证图的稀疏性，采用了<script type="math/tex">L1</script>损失：</p><script type="math/tex; mode=display">\mathcal{L}_{c l s}=-\sum_{i=1}^{M} y_{c} \log \left(\hat{y}_{i}\right)</script><script type="math/tex; mode=display">\mathcal{L}_{\text {sparse}}=\sum_{i=1}^{L}\left\|\mathcal{G}_{i}^{\text {spat }}\right\|_{1}+\left\|\mathcal{G}_{i}^{\text {temp }}\right\|_{1}</script><script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{c l s}+\lambda \mathcal{L}_{\text {sparse}}+\|\Theta\|_{2}</script><p>其中， <script type="math/tex">M</script>是动作类别的总数，<script type="math/tex">y_{c}</script>表示基本事实标签。  <script type="math/tex">\Theta</script>是ST-GCN和STGR的总体参数。 <script type="math/tex">\lambda</script> 用于平衡分类损失和稀疏损失的权重。</p><h3 id="探讨"><a href="#探讨" class="headerlink" title="探讨"></a>探讨</h3><p><img src="https://wx1.sbimg.cn/2020/08/10/oHOle.png" alt="Figure 5"></p><blockquote><p>图5：3种连接类型的比较。 (A)物理连接；(B)SGR学习到的空间上的连接；(C)TGR学习到的时间上的连接。 上面是图的矩阵表示法。 下面是关节连接的相应可视化效果。 为了更好去看，在可视化中使用阈值0.05对连接进行二值化。</p></blockquote><p>​    在这一部分中，我们用解析的方法验证了STGR的必要性，首先直观地给出了“感受野”的定义，然后指出人体骨骼的“星形结构”使得两个肢体关节之间的特征共享变得困难。</p><p>​    感受野是CNNs中一个重要的提法，它揭示了单个神经元的空间环境。通过类比这一思想，我们引入了“图上感受野”的概念，它是指单个节点可以提取信息的覆盖范围。</p><p>​    图5演示了3种连接模式。（a）表示预定义的人体骨骼，而（b）和（c）分别显示了与我们的SGR和TGR的学习到的连接。显而易见，预定义的骨骼会自行组织形成一个“星型结构”，其中躯干连接头部和所有四肢。这样，躯干中心关节的扩展速度会比四肢边缘关节快得多，从而导致严重的不平衡。</p><p><img src="https://wx2.sbimg.cn/2020/08/10/oHfcn.png" alt="Figure 6"></p><blockquote><p>图6：躯干关节（下背部）和非躯干关节（右手）的感受性比较。（a） “右手”关节感受野，左侧:扩散3步后。右：8步扩散后；（b）“下背部”关节感受野。左：三步扩散后。右图：经过8步扩散。红色表示高概率，紫色表示低概率。</p></blockquote><p>​    为了说明，我们在图6中检查肢体关节(右手)和躯干关节(下背部)的感受野。遵循以前的文献(Xu et al.。 2018)，我们将图卷积的展开转化为k步随机游走过程。 颜色表示节点接收的信息的比例。 如图6所示，在3个步长扩散之后，两个关节都从相对较小的范围接收信息。 经过8个步长后，躯干关节几乎可以接收到全局信息，而右手关节仍然在一个小区域内挣扎。</p><p>​    从另一个角度来看，我们提出的STGR方案通过关节的位置或运动来学习成对连接，从而打破了上述限制。 如图5所示，SGR学习图主要关注局部聚集，其中紧密的关节具有很强的连接。 另一方面，TGR学习图长期主要关注长期的相关关节。 这样，我们的STGR方案有效地扩大了每个关节的接受范围，进一步促进了试验过程。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>​    在本节中，我们将在基于骨架的动作识别数据集中评估我们的STGR方案。 我们在NTU-RGB+D和Kinetics两个大规模数据集上进行了实验，首先介绍了我们的实现细节，然后对时空图路由方案的各种设置进行了消融研究。 最后，我们将我们的完整模型与其他最先进的方法进行比较。 所有实验都在4个GTX 1080Ti GPU上进行。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><h4 id="NTU-RGB-D"><a href="#NTU-RGB-D" class="headerlink" title="NTU-RGB+D"></a>NTU-RGB+D</h4><p>​    NTU-RGB+D是一个广泛使用的大规模基于骨骼的人体动作识别数据集。它包含56880个骨架序列和60个动作类。总体行动类别大致分为日常行动、医疗状况和交互行动。每一个动作都是由同一高度的摄像机从三个不同的水平角度捕捉到的，即：-45 ，0 和45 。每个人体骨架都被表示为25个关节的三维坐标。交互作用类包含两个对象，而其他类仅包含一个对象。NTU-RGB+D推荐两种评估方法：1）交叉对象（X-Sub）：根据实验对象的不同，训练集和测试集分别分为40320个片段和16560个片段。2） 交叉视图（X-view）：训练集通过摄像 2和摄像3收集到37920个片段，而评估集是从摄像 1收集的，包含18960个片段。通过遵循现有工作关于基于骨架的动作识别的惯例，我们报告了两个方法的top-1。</p><h4 id="Kinetics"><a href="#Kinetics" class="headerlink" title="Kinetics"></a>Kinetics</h4><p>​    DeepMind Kinetics是最近最大的人类行动数据集之一。 该数据集包含近30万个视频剪辑，持续时间约为10秒。 为了尽可能多地报道真实场景，Kinetics从YouTube上收集视频，组成400个动作类。 请注意，原始运动学数据集仅包含原始视频剪辑。 遵循之前的做法，我们首先在OpenPose工具箱的帮助下提取原始2D坐标。然后应用我们的模型。 与NTU RGB+D相似，我们提取了18个人体关节，选取平均关节置信度最高的2个人作为主要研究对象。 在实践中，我们使用发布的数据来评估我们的模型。 数据集被分为具有240,000个片段的训练集和具有20,000个片段的测试集。在这个实验中，我们同时报告了TOP-1和TOP-5的准确性。</p><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>​    为了减轻计算负担，与以往的零填充不同，我们首先将序列长度下采样到固定大小的64帧，当序列长度大于64帧时，我们使用双线性插值进行均匀采样，而当给定序列小于64帧时，则进行零填充。在实践中，我们还对每一帧进行规范化处理，使训练过程更加稳定。在训练方面，采用SGD优化器对整个网络进行训练，ST-GCN的学习率为0.1，STGR学习率为0.01。权重衰减为<script type="math/tex">1 \mathrm{e}-4</script>，批次大小设置为32。 分类损失和<script type="math/tex">L1</script>损失的平衡参数设置为0.2，因为我们主要关注分类结果。 当监测验证损失停止减少超过5个周期时，我们将两个模块的学习率除以10。受到最近在基于骨架的动作识别上成功的启发，采用“双流”方法融合骨架特征和运动特征，模型收敛需要60个训练周期。</p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>​    在这一部分，我们将检验我们提出的STGR方法的有效性，并进行实验来测试各种参数设置。所有实验均在NTU-RGB+D数据集上的X-Sub基准测试上进行。</p><h4 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h4><p><img src="https://wx2.sbimg.cn/2020/08/11/oY8cw.png" alt="Figure 7"></p><blockquote><p>图7：（a）躯干关节和非躯干关节的感受野随层数增加的趋势比较；（b）识别精度随层数增加的趋势。</p></blockquote><p>​    在这项工作中，我们研究了STGR的必要性。 根据图7(A)，我们首先将所有25个关节分为躯干关节(根部、下背部、上背部)和非躯干关节(其他)。 然后，我们分别计算了两个集合从1层(向前1步)到10层(我们在模型中使用)的平均感受野。 如图所示，两组都用更多的层扩大了它们的接受视野。<br> 而躯干关节的增长速度明显加快，验证了我们对人体骨骼结构的分析。</p><p>​    另外，我们对不同层数的原生ST-GCN进行了测试。 我们发现，识别准确率与平均感受野的变化趋势是一致的。 如图7(B)所示，精度最初增长很快。 经过5层后，增长速度变慢。 当接近10层时，精度趋于稳定。 堆叠更多层不会影响总体精度。</p><p>​    当达到一定阶段时，躯干和四肢关节都被限制在一个相对固定的范围内，这限制了进一步的改进。 这对非常深层的结构是有利的。但是，这样的模型带来了较大的计算负担。相比之下，STGR通过直接学习关节连接有效地解决了这一问题。</p><h4 id="分组数"><a href="#分组数" class="headerlink" title="分组数"></a>分组数</h4><p>​    如前所述，在SGR中，我们首先将每个帧中的骨架聚类成K个子群。该方法通过直接探测相对距离来挖掘隐含的先验信息。在实践中，通过实验将聚类数K设为5。结果见表1。</p><p><img src="https://wx2.sbimg.cn/2020/08/11/oYinG.png" alt="Table 1"></p><blockquote><p>表1：不同簇数精度的比较。</p></blockquote><p>​    从理论上讲，太多的簇通常会导致过度分裂，而不足分裂的主要原因是簇太少。在取值范围在3到7之间，我们发现性能总体上是稳健的。因此，我们的选择正好保持了良好的平衡。</p><h4 id="时空图路由器"><a href="#时空图路由器" class="headerlink" title="时空图路由器"></a>时空图路由器</h4><p>​    如上所述，STGR由SGR和TGR两个子网络组成。表2表明，我们提出的STGR可以为原生ST-GCN带来好处。</p><p><img src="https://wx2.sbimg.cn/2020/08/11/oYxMT.png" alt="Table 2"></p><blockquote><p>表2:在NTU-RGB+D X-Sub上STGR模块的识别精度</p></blockquote><p>​    在这一部分，我们检查了STGR-GCN的四个变体，以测试STGR模块的有效性。这四个变体包括：1）带空间图路由器的GCN；2）带时间图路由器的GCN；3）带时空图路由器的GCN；4）带时空图路由器的双流GCN。</p><p>​    该方法使SGR和TGR比基线精度分别提高了1.84%和1.32%。从结果来看，SGR的性能略优于TGR。通过“双流技巧”，该模型进一步提高了1.18%，是基于骨架的动作识别的有效实践。</p><h4 id="与最先进技术的比较"><a href="#与最先进技术的比较" class="headerlink" title="与最先进技术的比较"></a>与最先进技术的比较</h4><p>​    在这一节中，我们评估了我们的完整的STGR-GCN模型和NTU-RGB+D中已有的基于骨架的动作识别模型和动力学数据集。</p><h4 id="NTU-RGB-D-1"><a href="#NTU-RGB-D-1" class="headerlink" title="NTU-RGB+D"></a>NTU-RGB+D</h4><p>我们将以前的先进方法大致分为四类：1）手工制作的方法：Lie group；2）基于RNN的方法：STA-LSTM，RNN-T/ACH，GCA-LSTM；3）基于CNN的方法：Joint Trajectory Maps，Skepxels，Temporal Conv，HCN；4）基于图的方法：ST-GCN。</p><p><img src="https://wx2.sbimg.cn/2020/08/11/oYEYo.png" alt="Table 3"></p><blockquote><p>表3:在NTU-RGB+D数据集的识别性能。<br>在cross-sub (X-Sub)和 cross-view（X-view）方面，我们将模型与先前的先进技术进行了比较。</p></blockquote><p>​    我们的STGR-GCN模型具有简单的时空路由方法，与原生的ST-GCN相比，具有更好的效果，进一步达到了先进的水平，说明了动态路由方案在图卷积层之间的有效性。</p><h4 id="Kinetics-1"><a href="#Kinetics-1" class="headerlink" title="Kinetics"></a>Kinetics</h4><p><img src="https://wx1.sbimg.cn/2020/08/11/oZ5FA.png" alt="Table 4"></p><blockquote><p>表4：在Kinetics数据集上的识别性能。 我们报告Top-1和Top-5的准确率。</p></blockquote><p>​    在Kinetics上，我们比较了我们的方法同一种手工制作的方法：Feature encoding；一种RNN方法：Deep LSTM；基于时间的CNN方法和ST-GCN。 按照常规，我们同时报告Top-1和Top-5的准确率。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    本文提出了一种新的路由方案，用于在基于骨架的动作识别中为物理上分开的关节生成时空相关图，解决了预定义的人类结构的不足。 此外，我们还通过在图上引入感受野来说明建立必要连接的重要性，并通过我们的工作有效地扩大了感受野。 给出定性和定量结果，验证了该方法的有效性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Spatio-Temporal-Graph-Routing-for-Skeleton-Based-Action-Recognition&quot;&gt;&lt;a href=&quot;#Spatio-Temporal-Graph-Routing-for-Skeleton-Based-Acti
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="ActionRecognition" scheme="http://yoursite.com/tags/ActionRecognition/"/>
    
  </entry>
  
  <entry>
    <title>Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</title>
    <link href="http://yoursite.com/2020/08/06/%E5%9F%BA%E4%BA%8E%E9%AA%A8%E6%9E%B6%E7%9A%84%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E7%9A%84%E8%A7%A3%E7%BC%A0%E5%92%8C%E7%BB%9F%E4%B8%80%E7%9A%84%E5%9B%BE%E5%8D%B7%E7%A7%AF/"/>
    <id>http://yoursite.com/2020/08/06/%E5%9F%BA%E4%BA%8E%E9%AA%A8%E6%9E%B6%E7%9A%84%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E7%9A%84%E8%A7%A3%E7%BC%A0%E5%92%8C%E7%BB%9F%E4%B8%80%E7%9A%84%E5%9B%BE%E5%8D%B7%E7%A7%AF/</id>
    <published>2020-08-06T14:24:32.000Z</published>
    <updated>2020-11-04T16:25:18.286Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Disentangling-and-Unifying-Graph-Convolutions-for-Skeleton-Based-Action-Recognition"><a href="#Disentangling-and-Unifying-Graph-Convolutions-for-Skeleton-Based-Action-Recognition" class="headerlink" title="Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition"></a>Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</h1><blockquote><p><a href="https://arxiv.org/pdf/2003.14111.pdf%E2%80%8Barxiv.org" target="_blank" rel="noopener">paper: https://arxiv.org/pdf/2003.14111.pdf​arxiv.org</a><br><a href="https://github.com/kenziyuliu/ms-g3d%E2%80%8Bgithub.com" target="_blank" rel="noopener">code: https://github.com/kenziyuliu/ms-g3d​github.com</a></p></blockquote><h1 id="基于骨架的动作识别的分离和统一的图卷积"><a href="#基于骨架的动作识别的分离和统一的图卷积" class="headerlink" title="基于骨架的动作识别的分离和统一的图卷积"></a>基于骨架的动作识别的分离和统一的图卷积</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>基于骨架的动作识别算法广泛使用时空图对人体动作动态进行建模。</p><p>为了从这些图中捕获稳健的运动模式，长期和多尺度的上下文聚合与时空依赖建模是一个强大的特征提取器的关键方面。</p><p>然而，现有的方法在实现以下方面存在局限性。</p><p>​    (1)多尺度算子下的无偏差长期关节关系建模</p><p>​    (2)用于捕捉复杂时空依赖的通畅的跨时空信息流</p><p>在这项工作中，我们提出了</p><p>​    (1)一种简单的分解（disentangle）多尺度图卷积的方法和</p><p>​    (2)一种统一的时空图卷积算子G3D。</p><p>所提出的多尺度聚合方法理清了不同邻域中节点对于有效的长期建模的重要性。</p><p>所提出的G3D模块利用密集的跨时空边作为跳跃连接，用于在时空图中直接传播信息。</p><p>通过结合上述提议，我们开发了一个名为MS-G3D的强大的特征提取器，在此基础上，我们的模型在三个大规模数据集NTU RGB+D60，NTU RGB+D120和Kinetics Skeleton 400上的性能优于以前的最先进方法。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDEuc2JpbWcuY24vMjAyMC8wOC8wNi9vUVVsZS5wbmc?x-oss-process=image/format,png" alt="image1"></p><blockquote><p>图1.（a)骨架图序列的空间和时间分解的建模导致间接信息流。 (b)在这项工作中，我们建议用统一的时空图卷积来捕捉跨时空关联。 (c)在不同的时空邻域(在不同距离的黄色、蓝色、红色，为了清晰而部分着色)上的节点特征的分离对于在时空域中进行有效的多尺度学习至关重要</p></blockquote><p>​    人体动作识别是许多实际应用中的重要任务。特别是，基于骨架的人体动作识别涉及从人体的骨架表示而不是原始的RGB视频预测动作，并且在最近的工作中看到了有意义的结果证明了它的优点。对比RGB表征，骨架数据只包含2D 或3D 人体关键关节的位置，提供了高度抽象的信息，并且没有环境噪声（例如背景杂波，光照条件，衣服）使得动作识别算法可以专注于动作的稳健特征。</p><p>​    早期的基于骨架的动作识别方法将人体关节视为一组独立的特征，他们通过手工制作的或学习的这些特征的集合来建模空间和时间上的关节相关性。 然而，这些方法忽略了人体关节之间的内在关系，这种关系最好用人体骨架图来捕捉，人体骨架图中的关节为节点，而它们的自然连接(即“骨头”)为边。为此，最近的研究方法利用骨架时空图建立了动作的关节运动模式的模型，骨架时空图是一系列不相交、同构的不同时间步长的骨架图，承载着空间和时间维度上的信息。</p><p>​    为了从骨架图进行稳健的动作识别，理想的算法应该超越局部关节连接性，提取多尺度结构特征和长期依赖关系，因为结构上分离的关节也可以有很强的相关性。许多现有的方法都是通过使用骨架邻接矩阵的高次幂来实现这一目的: 直观地，邻接矩阵的幂来捕获每对节点之间的路径数，且行走的长度与幂相同；因此，邻接多项式通过使远邻可达来增加图卷积的感受野。然而，这种方法存在有偏权问题，即无向图上环的存在意味着边权重将偏向于更靠近的节点而不是更远的节点。 在骨架图上，这意味着邻接矩阵高次幂只能低效地捕捉远处关节的信息，因为聚集的特征将由局部身体部位的关节主导。 这是限制现有多尺度聚合器可伸缩性的一个严重缺陷。</p><p>​    鲁棒算法的另一个理想特征是利用复杂的跨时空关节关系进行动作识别的能力。 然而，为此，大多数现有的方法部署的仅空间和仅时间交错的的模块(图1(a)) ，类似于分解的3D 卷积。 典型的方法是首先使用图卷积提取每个时间步长的空间关系，然后使用循环神经网络或一维卷积层建立时间动态模型。 虽然这样的分解方法可以有效的长期建模，但它阻碍了跨时空的直接信息流，无法捕获复杂的区域时空关节依赖关系。例如，“站立”动作通常是上身和下身在空间和时间上的共同运动，上身的运动(向前倾)与下身未来的运动(站立)有很强的相关性。 分解方法建模可能无法有效地捕捉到这些用来做预测的有力线索。</p><p>​    在这项工作中，我们从两个方面解决了上述限制。 首先，我们提出了一种新的多尺度聚合方案，通过消除较远和较近邻域间的冗余依赖关系来解决有偏差的权重问题，从而理顺多尺度聚合下的特征(如图2所示)。 这就产生了更强大的多尺度算子，可以建模关节之间的关系，而不用考虑它们之间的距离。 其次，我们提出了 G3D，一个新的统一的时空图卷积模块，可以直接建模跨时空关节依赖关系。 G3D通过引入跨越“3D”时空域的图形边作为无障碍信息流的跳过连接来做到这一点(图1(B))，实质上促进了时空特征学习。值得注意的是，我们提出的解纠缠聚合方案增加了 G3D 的多尺度时空推理(图1(c))而没有受到有偏差权重问题的影响，尽管引入了额外的边。由此产生的功能强大的特征提取器，命名为 MSG3D，构成了我们最终模型架构的基石，在三个大规模骨架动作数据集(NTU rgb + d120、 NTU rgb + d60和 Kinetics Skeleton 400)上的性能优于最先进的方法。</p><p>这项工作的主要贡献概括如下：</p><p>(i)提出了一种解纠缠多尺度聚合方案，该方案消除了不同邻域节点特征之间的冗余依赖关系，使强大的多尺度聚合器能够有效地捕获人体骨架上的图形广义关节关系。<br>(ii)提出了一种统一的时空图卷积(G3D)算子，使得信息在时空中直接流动，从而实现高效的特征学习。<br>(iii)将解纠缠方案与 G3D 相结合，提供了一个强大的特征提取器(MS-G3D) ，具有跨时空的多尺度感受野。 时空特征的直接多尺度聚合进一步提高了模型性能。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h3><h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><p>​    为了从任意结构的图中提取特征，图神经网络(GNNs)得到了广泛的发展和探索。最近提出的GNN大致可分为频谱GNN和空域GNN。频谱GNN将输入的图形信号与图傅立叶域中的一组学习滤波器进行卷积。然而，由于特征分解的要求和固定邻接的假设，它们在计算效率和对新图的推广性方面受到限制。相反，空域GNN通常通过(1)选择具有邻域函数的邻居(例如，相邻节点)；(2)将来自所选择的邻居及其自身的特征与聚集函数合并(例如，均值池)；以及(3)将激活的变换应用于合并的特征(例如，MLP)，来执行针对每个节点的层级更新。在不同的GNN变体中，图卷积网络(GCN)最初是作为局部频谱卷积的一阶近似引入的，但它作为平均邻域聚合器的简单性迅速导致许多后续的空域GNN体系结构和涉及图结构数据的各种应用将其视为空域GNN基线。本文采用了GCN中的分层更新规则。</p><h4 id="多尺度图卷积"><a href="#多尺度图卷积" class="headerlink" title="多尺度图卷积"></a>多尺度图卷积</h4><p>​    多尺度空域GNNs也被提出用来捕捉非局部邻域的特征。 使用图邻接矩阵的邻接矩阵高次幂来聚合来自远处邻居节点的特征。Truncated Block Krylov network同样将邻接矩阵提高到更高的幂次，并通过不同隐层的密集特征串联来获得多尺度信息。LanczosNet利用邻接矩阵的低秩近似来加速大型图的幂运算。如第1节所述，我们认为邻接权重可能会因权重偏差而对远程建模产生不利影响，而我们提出的模块旨在通过解缠的多尺度聚合器解决这一问题。</p><h3 id="基于骨架的动作识别"><a href="#基于骨架的动作识别" class="headerlink" title="基于骨架的动作识别"></a>基于骨架的动作识别</h3><p>​    早期的基于骨架的动作识别方法侧重于下游分类器的手工制作特征和关节关系，忽略了人体语义连接的重要性。通过构造时空图和直接用GNNs建模空间关系，最近的方法的性能得到了显著提高，这表明人体骨架的语义对于动作预测的必要性。</p><p>​    图卷积的一个早期应用是ST-GCN，其中空间图卷积与交错时间卷积一起用于时空建模。李等共同的工作提出了一个类似的方法，通过提高骨架邻接矩阵到更高的幂次来引入多尺度模块。AS-GCN也使用邻接矩阵的幂进行多尺度建模，但它还额外生成人体姿势以增强空间图卷积。时空图路由（STGR）网络使用逐帧注意和全局自注意机制为骨架图添加额外的边。类似地，2s-AGCN引入了具有自注意的图形自适应性以及自由学习的图形残差掩码。它还使用具有骨架骨骼特征的双流集成来提高性能。DGNN同样利用了骨骼特征，但是它通过交替的空间聚合方案同时更新关节和骨骼特征。要提出来的是，上述这些方法主要集中在空间建模上；相比之下，我们提出了一种统一的方法，用于直接跨时空捕获复杂的关节相关性。</p><p>​    另一个相关的工作是GR-GCN，它在骨架图序列上每三帧合并一次，并在相邻帧之间添加稀疏边。虽然GR-GCN也应用了跨时空边，但跟我们的G3D模块有几个重要区别：</p><p>​    (1)G3D中的跨时空边遵循语义人体骨架，与GR-GCN中稀疏的、一刀切的图相比，G3D中的跨时空边自然是一种更可解释、更健壮的表示。底层图形也更容易计算。</p><p>（2）GR-GCN仅在相邻帧之间具有跨时空边，这使其无法推理超出三个帧的有限时间范围。</p><p>（3）G3D可以同时利用不同的窗口大小和膨胀从多个时间上下文中学习，这在GR-GCN中没有解决。</p><h2 id="MS-G3D"><a href="#MS-G3D" class="headerlink" title="MS-G3D"></a>MS-G3D</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><h4 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h4><p>​    人体骨架图被表示为 <script type="math/tex">G=(V，E)</script>，其中<script type="math/tex">V=\{v1，.，vn\}</script>是表示关节的N个节点的集合，E是表示由邻接矩阵<script type="math/tex">A \in \mathbb{R}^{N \times N}</script>捕获的骨架的边集，其中如果边从<script type="math/tex">v_i</script>指向<script type="math/tex">v_j</script></p><p>初始 <script type="math/tex">A_{i,j} = 1</script>  否则为0。</p><p>因为G是无向图所以矩阵A是对称的。作为图序列的动作具有节点特征集</p><script type="math/tex; mode=display">\mathcal{X}=\left\{x_{t, n} \in \mathbb{R}^{C} \mid t, n \in\right.\mathbb{Z}, 1 \leq t \leq T, 1 \leq n \leq N\}</script><p>表示为特征张量<script type="math/tex">\mathbf{X} \in \mathbb{R}^{T \times N \times C}</script>其中</p><script type="math/tex; mode=display">x_{t, n}=\mathbf{X}_{t, n,:}</script><p>是节点<script type="math/tex">v_n</script>总共T帧的第t帧的C维特征向量 。因此，输入动作在结构上由A和在特征上由<script type="math/tex">\mathbf{X}_{t} \in \mathbb{R}^{N \times C}</script><br>适当地描述，其中是时间t处的节点特征。 </p><script type="math/tex; mode=display">\Theta^{(l)} \in \mathbb{R}^{C_{l} \times C_{l+1}}</script><p>表示网络第L层的可学习权重矩阵。</p><h4 id="图卷积网络"><a href="#图卷积网络" class="headerlink" title="图卷积网络"></a>图卷积网络</h4><p>在特征向量X和图A定义的骨架输入上，可以将GCN的分层更新规则应用于时间t处的特征，如下所示：</p><script type="math/tex; mode=display">\mathbf{X}_{t}^{(l+1)}=\sigma\left(\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{X}_{t}^{(l)} \Theta^{(l)}\right)</script><p>式中，<script type="math/tex">\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}</script> 是骨架图，添加了自循环以保持自身特征，<script type="math/tex">\tilde{\mathbf{D}}</script>是<script type="math/tex">\tilde{\mathbf{A}}</script>的对角度矩阵，<script type="math/tex">\sigma(\cdot)</script> 是激活函数。 公式<script type="math/tex">\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{X}_{t}^{(l)}</script>可以直观地解释为来自直接邻域的近似空间平均特征聚集，随后是激活的线性层。</p><h3 id="解缠多尺度聚合"><a href="#解缠多尺度聚合" class="headerlink" title="解缠多尺度聚合"></a>解缠多尺度聚合</h3><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDIuc2JpbWcuY24vMjAyMC8wOC8wNi9vUW5DRC5wbmc?x-oss-process=image/format,png" alt="image2"></p><blockquote><p>Figure2: 图解有偏加权问题和所提出的分离聚合方案。颜色越深，表示对于中心节点的权重越大（红色）。左上角：越近的节点从邻接矩阵获得更高的权重，这会降低远程建模的效率，特别是在聚合多个尺度时。左下：我们提出的解缠结聚合模型在保持自身特征的同时，对每个邻域的关节关系进行建模。右：可视化相应的邻接矩阵。 为了视觉清晰，省略了节点自环</p></blockquote><h4 id="有偏加权问题"><a href="#有偏加权问题" class="headerlink" title="有偏加权问题"></a>有偏加权问题</h4><p>在公式1的空间聚集框架下，现有的方法使用邻接矩阵的高次幂来聚集时间t的多尺度结构信息，如：</p><script type="math/tex; mode=display">\mathbf{X}_{t}^{(l+1)}=\sigma\left(\sum_{k=0}^{K} \widehat{\mathbf{A}}^{k} \mathbf{X}_{t}^{(l)} \Theta_{(k)}^{(l)}\right)</script><p>其中<script type="math/tex">K</script>控制要聚合的尺度。在这里, <script type="math/tex">\widehat{\mathbf{A}}</script> 是<script type="math/tex">\mathbf{A}</script> 的归一化形式，例如[19]使用堆成归一化拉普拉斯图 <script type="math/tex">\widehat{\mathbf{A}}=\mathbf{L}^{\text {norm }}=\mathbf{I}-\mathbf{D}^{\frac{1}{2}} \mathbf{A} \mathbf{D}^{\frac{1}{2}}</script>；[21] 利用随机游走归一化邻接 <script type="math/tex">\widehat{\mathbf{A}}=</script> <script type="math/tex">\mathbf{D}^{-1} \mathbf{A} ;</script> 更一般的说，可以使用GCNs中的 <script type="math/tex">\widehat{\mathbf{A}}=\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}}</script>。很容易易看出</p><script type="math/tex; mode=display">\mathbf{A}_{i,j}^{k}=\mathbf{A}_{j, i}^{k}$$ ，给出了$$v_{i}$$ 和$$v_{j}$$ 之间长度为k的路径数，因此  $$\widehat{\mathbf{A}}^{k}\mathbf{X}_{t}^{(l)}</script><p>正在根据此类步数执行加权特征平均。然而，很明显，由于循环遍历，到更近的节点的长度为k的路径比到实际的k跳邻居的数量多得多。这会导致权重偏向局部区域以及度较高的节点。GCNS中的节点的自环允许更多可能的路径(因为总是可以在自环上循环)，从而放大偏差。参见图2。因此，在骨架图上进行多尺度聚合时，聚合特征将以局部身体部位的信号为主导，从而使用具有较高次幂无法有效捕获长期关节依赖关系。</p><h4 id="解开邻域关系"><a href="#解开邻域关系" class="headerlink" title="解开邻域关系"></a>解开邻域关系</h4><p>为了解决上述问题，我们首先将k邻接矩阵<script type="math/tex">\tilde{\mathbf{A}}_{(k)}</script>定义为</p><script type="math/tex; mode=display">\left[\tilde{\mathbf{A}}_{(k)}\right]_{i, j}=\left\{\begin{array}{cl}1 & \text { if } d\left(v_{i}, v_{j}\right)=k \\1 & \text { if } i=j \\0 & \text { otherwise }\end{array}\right.</script><p>其中 <script type="math/tex">d\left(v_{i}, v_{j}\right)</script> 是<script type="math/tex">v_{i}</script></p><p>和 <script type="math/tex">v_{j}</script>之间跳数最短的距离,<script type="math/tex">\tilde{\mathbf{A}}_{(k)}</script> 是 <script type="math/tex">\tilde{\mathbf{A}}</script> 到更远领域的一般化, 其中<script type="math/tex">\tilde{\mathbf{A}}_{(1)}=\tilde{\mathbf{A}}</script> and <script type="math/tex">\tilde{\mathbf{A}}_{(0)}=\mathbf{I}</script>。在式1中的空间聚集下，在<script type="math/tex">\hat{\mathbf{A}}_{(k)}</script>中包含自循环对于学习当前关节与其k跳邻居之间的关系以及在没有k跳邻居时保持每个关节的自身信息至关重要。考虑到N很小，因此可以很容易地计算出<script type="math/tex">\tilde{\mathbf{A}}_{(k)}</script> ，例如，使用图幂之差，如</p><script type="math/tex; mode=display">\tilde{\mathbf{A}}_{(k)}=\mathbf{I}+\mathbb{1}\left(\tilde{\mathbf{A}}^{k} \geq 1\right)-\mathbb{1}\left(\tilde{\mathbf{A}}^{k-1} \geq 1\right)</script><p>（将邻接矩阵k次幂得到的矩阵中元素大于1的元素全都变为1）</p><p>将式2中的<script type="math/tex">\widehat{\mathbf{A}}^{k}</script>替换为 <script type="math/tex">\tilde{\mathbf{A}}_{(k)}</script>，得出：</p><script type="math/tex; mode=display">\mathbf{X}_{t}^{(l+1)}=\sigma\left(\sum_{k=0}^{K} \tilde{\mathbf{D}}_{(k)}^{-\frac{1}{2}} \tilde{\mathbf{A}}_{(k)} \tilde{\mathbf{D}}_{(k)}^{-\frac{1}{2}} \mathbf{X}_{t}^{(l)} \Theta_{(k)}^{(l)}\right)</script><p>其中 <script type="math/tex">\tilde{\mathbf{D}}_{(k)}^{-\frac{1}{2}} \tilde{\mathbf{A}}_{(k)} \tilde{\mathbf{D}}_{(k)}^{-\frac{1}{2}}</script>  是<script type="math/tex">k</script> 邻接矩阵的归一化。</p><p>​    与之前的情况不同，可能的长度k的路径数主要取决于长度k − 1的路径数，在式4提出的分离公式通过去除较远邻域对较近邻域权重的冗余依赖来解决有偏权重问题。 此外，具有较大k的尺度在多尺度算子下以相加的方式聚合，使得具有较大k值的长期建模保持有效。所得的k邻接矩阵也比其对应高次幂的矩阵稀疏（请参见图2），从而可以更有效地表示。</p><h3 id="G3D：统一的时空建模"><a href="#G3D：统一的时空建模" class="headerlink" title="G3D：统一的时空建模"></a>G3D：统一的时空建模</h3><p>​    大多数现有工作将骨架动作视为一系列不相交的图，其中特征是通过仅空间（例如GCN）和仅时间（例如TCN）模块提取的。我们认为，这种分解的方法对于捕获复杂的时空关节关系不太有效。显然，如果一对节点之间存在牢固的连接，则在逐层传播期间，该对节点应包含彼此的显著特征部分以反映这种连接。然而，当信号通过一系列局部聚合器(GCNS和TCNs)在时空中传播时，随着从越来越大的时空感受野聚集冗余信息时，信号会被削弱。如果观察到gcn没有执行加权聚合来区分每个邻居，那么问题就更明显了。</p><h4 id="跨时空跳跃连接"><a href="#跨时空跳跃连接" class="headerlink" title="跨时空跳跃连接"></a>跨时空跳跃连接</h4><p>​    为了解决上述问题，我们提出了一种更合理的方法来允许跨时空跳跃连接，这种连接很容易用时空图中的跨时空边来建模。（参数过大和提取的特征过于泛化）让我们首先考虑输入图序列上一个大小为<script type="math/tex">\tau</script>的滑动时间窗口，在每一步中，它都会得到一个时空子图<script type="math/tex">\mathcal{G}_{(\tau)}=\left(\mathcal{V}_{(\tau)}, \mathcal{E}_{(\tau)}\right)</script>,其中<script type="math/tex">\mathcal{V}_{(\tau)}=\mathcal{V}_{1} \cup \ldots \cup \mathcal{V}_{\tau}</script>是窗口中<script type="math/tex">\tau</script>帧的所有节点集的并集。通过将<script type="math/tex">\tilde{\mathbf{A}}</script>平铺到块邻接矩阵<script type="math/tex">\hat{\mathbf{A}}_{(\tau)}</script>来定义初始边集合 <script type="math/tex">\mathcal{E}_ {(\tau)}</script>，其中（每个node都和所有frames的对应邻居节点直接相连）</p><script type="math/tex; mode=display">\tilde{\mathbf{A}}_{(\tau)}=\left[\begin{array}{ccc}\tilde{\mathbf{A}} & \cdots & \tilde{\mathbf{A}} \\\vdots & \ddots & \vdots \\\tilde{\mathbf{A}} & \cdots & \tilde{\mathbf{A}}\end{array}\right] \in \mathbb{R}^{\tau N \times \tau N}</script><p>直观地说，每个子矩阵<script type="math/tex">\left[\tilde{\mathbf{A}}_{(\tau)}\right]_{i, j}=\tilde{\mathbf{A}}</script>意味着<script type="math/tex">\mathcal{V}_{i}</script> 中的每个节点通过将逐帧的空间连通性（即所有<script type="math/tex">i</script>的空间连通性为<script type="math/tex">\left[\tilde{\mathbf{A}}_{(\tau)}\right]_{i, i}</script>）外推到时域，在帧<script type="math/tex">j</script>处连接到自身及其1跳的空间邻居。因此，<script type="math/tex">\mathcal{G}{（\tau）}</script>内的每个节点都与自身及其跨所有<script type="math/tex">\tau</script>帧的1跳空间邻居紧密相连。在<script type="math/tex">\mathbf{X}</script>上使用相同的零填充滑动窗口构造<script type="math/tex">T</script>个窗口，可以很容易地得到 <script type="math/tex">\mathbf{X}_{(\tau)} \in \mathbb{R}^{T \times \tau N \times C}</script> 。利用式1，因此我们得出了用于<script type="math/tex">t^{\text {th }}</script> 时间窗口的统一的时空图卷积算子：</p><script type="math/tex; mode=display">\left[\mathbf{X}_{(\tau)}^{(l+1)}\right]_{t}=\sigma\left(\tilde{\mathbf{D}}_{(\tau)}^{-\frac{1}{2}} \tilde{\mathbf{A}}_{(\tau)} \tilde{\mathbf{D}}_{(\tau)}^{-\frac{1}{2}}\left[\mathbf{X}_{(\tau)}^{(l)}\right]_{t} \Theta^{(l)}\right)</script><h4 id="膨胀窗口"><a href="#膨胀窗口" class="headerlink" title="膨胀窗口"></a>膨胀窗口</h4><p>​    上述窗口结构的另一个重要方面是不需要是相邻帧。通过每<script type="math/tex">d</script>帧选取一帧并重用相同的时空结构<script type="math/tex">\mathbf{A}_{(\tau)}</script>，可以构造具有<script type="math/tex">\tau</script>帧和<script type="math/tex">d</script>膨胀率的膨胀窗口。同样，我们可以获得节点特征<script type="math/tex">\mathbf{X}_{(\tau, d)} \in \mathbb{R}^{T \times \tau N \times C}</script>（被忽略的话<script type="math/tex">d=1</script> ），执行式6中的逐层更新。膨胀窗口允许更大的时间感受野而不增加<script type="math/tex">\tilde{\mathbf{A}}_{（\tau）}</script>的大小，类似于空洞卷积如何保持恒定的复杂性。</p><h4 id="多尺度G3D"><a href="#多尺度G3D" class="headerlink" title="多尺度G3D"></a>多尺度G3D</h4><p>​    我们也可以将所提出的解缠多尺度聚合方案（式4）整合到G3D中，直接在时空域进行多尺度推理。因此，我们从式6推导出MS-G3D模块为：</p><script type="math/tex; mode=display">\left[\mathbf{X}_{(\tau)}^{(l+1)}\right]_{t}=\sigma\left(\sum_{k=0}^{K} \tilde{\mathbf{D}}_{(\tau, k)}^{-\frac{1}{2}} \tilde{\mathbf{A}}_{(\tau, k)} \tilde{\mathbf{D}}_{(\tau, k)}^{-\frac{1}{2}}\left[\mathbf{X}_{(\tau)}^{(l)}\right]_{t} \Theta_{(k)}^{(l)}\right)</script><p>其中<script type="math/tex">\tilde{\mathbf{A}}_{(\tau, k)}</script>和<script type="math/tex">\tilde{\mathbf{D}}_{(\tau, k)}</script> 的定义分别类似于<script type="math/tex">\tilde{\mathbf{A}}_{(k)}</script> 和 <script type="math/tex">\tilde{\mathbf{D}}_{(k)}</script>。值得注意的是，我们提出的解缠聚合方案补充了这一统一算子，因为G3D由于时空连通性而增加的节点度数可能会导致有偏权重问题。</p><h4 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h4><p>​    我们对G3D进行了更深入的分析，如下所示。</p><p>​    （1）它类似于经典的三维卷积块，其时空感受野由<script type="math/tex">d</script>,<script type="math/tex">\tau</script>和<script type="math/tex">\tilde{A}</script>定义。</p><p>​    （2） 与3D卷积不同，G3D的参数由<script type="math/tex">\Theta_{(\cdot)}^{(\cdot)}</script>得出独立于<script type="math/tex">\tau</script>或<script type="math/tex">\left |\mathcal{E}{（\tau）}\right |，</script>使得它在大的<script type="math/tex">\tau</script>通情况下不太容易过拟合。(3)G3D中密集的跨时空连接需要在<script type="math/tex">\tau</script>上进行权衡，因为较大的<script type="math/tex">\tau</script>值带来了更大的时间感受野，代价是由于更大的邻域而牺牲了更一般的特征。此外，越大的<script type="math/tex">\tau</script>意味着<script type="math/tex">\tilde{\mathbf{A}}_{(\tau)}</script>平方的扩大，因此多尺度聚合的运算量越大。另一方面，较大的膨胀率<script type="math/tex">d</script>以时间分辨率(较低的帧率)为代价带来更大的时间覆盖。因此必须小心地平衡<script type="math/tex">d</script>,<script type="math/tex">\tau</script>。（4）G3D模块旨在捕获复杂的区域时空关系，而不是由因数分解模块可以更经济地捕获的长期依赖关系。因此，我们观察到，当G3D模块使用长期的因式分解模块增强时，性能最佳，我们将在下一节讨论这一点</p><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDEuc2JpbWcuY24vMjAyMC8wOC8wNi9vUXhMTi5wbmc?x-oss-process=image/format,png" alt="image-20200708002432414"></p><blockquote><p>Figure 3:架构概述：“TCN”、“GCN”、前缀“MS-”和后缀“-D”分别表示时间卷积块和图卷积块，以及多尺度和解缠聚集(第3.2节)。r个STGC块(b)中的每一个都使用了多分支设计，以同时捕获长期的和区域时空依赖关系。虚线模块：包括额外的G3D分支、1×1卷积层和跨步时间卷积，根据情况权衡模型的性能/复杂性。</p></blockquote><h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p>​    最终的模型架构如图3所示。在高层次上，它包含r个时空图卷积(STGC)块的堆栈，用于从骨架序列中提取特征，随后是全局均值池化层和Softmax分类器。每个STGC块部署两种类型的路径，以同时捕获复杂的区域时空关节相关性以及长期的时空依赖性：（1） G3D路径首先构造时空窗口，对其进行解纠缠的多尺度图卷积，然后用一个全连接层对其进行折叠将窗口特征读出。额外的虚线G3D分支(图3(B))表明该模型可以同时从不同的<script type="math/tex">\tau</script>和<script type="math/tex">d</script>的多个时空上下文中学习；（2） 因式分解路径通过长期、仅空间和仅时间的模块增强了G3D分支：第一层是一个多尺度的图卷积层，能够用最大K（最长关节间距离）对整个骨架图进行建模；随后是两个多尺度时间卷积层，以捕获扩展的时间上下文(下面讨论)。来自所有分支的输出被聚集为STGC块输出，该STGC块输出在典型的r=3块体系结构中分别具有96、192和384个特征通道。批归一化和ReLU添加到除了最后一层以外的每一层末尾。除第一个块外，所有STGC块均使用步幅为2的时间卷积和滑动窗口在时间维度上进行下采样。</p><h4 id="多尺度时间建模"><a href="#多尺度时间建模" class="headerlink" title="多尺度时间建模"></a>多尺度时间建模</h4><p>​    G3D所使用的时空窗口<script type="math/tex">\mathcal{G}{(\tau)}</script>本身是一个封闭的结构，这意味着G3D必须伴随时间模块进行跨窗口信息交换。许多现有工作在整个架构中使用具有固定大小为<script type="math/tex">k_t\times1</script>的卷积核的时间卷积对时间建模。我们用多尺度学习增强香草时间卷积层，如图3（c）所示。为了降低额外分支所带来的计算成本，我们采用了瓶颈设计，将卷积核大小固定为3×1，并使用不同的膨胀率，而不是更大的卷积核来获得更大的感受野。我们还使用残差连接来促进训练。</p><h4 id="自适应图"><a href="#自适应图" class="headerlink" title="自适应图"></a>自适应图</h4><p>​    为了提高执行同类邻域平均化的图卷积层的灵活性，我们给每个<script type="math/tex">\tilde{\mathbf{A}}_{(K)}</script>和<script type="math/tex">\tilde { \mathbf {A}}_{ ( \tau, k) }</script>添加一个受[33，32]启发的简单的、可学习的、无约束的残缺掩码图<script type="math/tex">\mathbf{A} ^ {\text{res}}</script>，以动态地加强、削弱、添加或删除边。例如，公式4更新为</p><script type="math/tex; mode=display">\mathbf{X}_{t}^{(l+1)}=\sigma\left(\sum_{k=0}^{K} \tilde{\mathbf{D}}_{(k)}^{-\frac{1}{2}}\left(\tilde{\mathbf{A}}_{(k)}+\mathbf{A}_{(k)}^{\mathrm{res}}\right) \tilde{\mathbf{D}} _{(k)}^{-\frac{1}{2}} \mathbf{X}_{t}^{(l)} \Theta_{(k)}^{(l)}\right)</script><script type="math/tex; mode=display">\mathbf{A}^{\text{res}}$$被初始化为0左右的随机值，并且对于每个$$k$$和$$\tau$$是不同的，使得每个多尺度上下文（空间或时空）选择最适合的掩码。还要注意的是，由于$$\mathbf{A}^{\text{res}}$$针对所有可能的动作进行了优化，这些动作可能具有不同的用于特征传播的最佳边集，因此预计它会给出较小的边校正，并且当图结构具有重大缺陷时可能是不够的。特别是，$$\mathbf{A}^{\text{res}}$$仅部分缓解了偏向加权问题(参见第4.3节)。#### 关节-骨骼双流融合​    受到[33，32，34]中的双流方法的启发，以及可视化骨骼和关节可以帮助人类识别骨骼动作的直观，我们使用了一个双流框架，其中具有相同架构的单独模型使用被初始化为远离身体中心的相邻关节矢量差的骨骼特征来训练。来自关节/骨骼模型的softmax得分相加以获得最终预测得分。由于骨架图是树，我们在身体中心添加一个零骨骼向量，以从N个关节获得N个骨骼，并重用A来定义连通性。## 实验### 数据集#### NTU RGB+D 60 和 NTU RGB+D 120.​    NTU RGB+D60[31]是一个大规模的动作识别数据集，包含56,578个骨骼序列，超过60个动作类别，采集自40个不同的对象和3个不同的摄像机视角。每个骨架图包含N=25个身体关节作为节点，其在空间中的3D位置作为初始特征。动作的每一帧包含1到2个对象。作者建议报告两种情况下的分类准确性：(1)交叉对象(X-Sub)，将40名对象分为训练组和测试组，分别产生40,091个和16,487个训练和测试样本。(2)交叉视图(X-View)，从1号摄像机收集的18,932个样本全部用于测试，其余37,646个样本用于训练。NTU RGB+D120扩展了NTU RGB+D 60，在60个额外的动作类别中增加了57,367个骨骼序列，总计113,945个样本，超过120个类别，来自106个不同的对象和32个不同的摄像机设备。作者现在建议将交叉视图的设置替换为交叉设备(X-Set)设置，其中从一半相机设备中收集的54,468个样本用于训练，其余59,477个样本用于测试。 在交叉对象方面，从53名受试者中挑选出63,026个样本用于训练，其余50,919个样本用于测试。#### Kinetics Skeleton 400​    Kinetics Skeleton 400数据集是由OpenPose姿态估计工具箱从Kinetics 400视频数据集改编而来。 它包含总计400个类别的240,436个训练骨架序列和19,796个测试骨架序列，其中每个骨架图包含18个身体关节，以及它们的2D空间坐标和来自OpenPose的预测置信度分数作为初始关节特征[50]。 在每个时间步长，骨架数量上限为2，总体置信度分数较低的骨架将被丢弃。 按照[15，50]中的会议，报告了Top-1和Top-5的精确度。### 实现细节​    除非另有说明，否则所有模型的r = 3并以SGD进行训练，其动量为0.9，批大小为32，初始学习率为0.05（可以按批量大小线性扩展）对于50、60和65个训练迭代， 对于NTU RGB + D 60、120和Kinetics Skeleton 400，分别在{30，40}，{30，50}和{45，55}个时期，受到LR衰减的0.1个学习率减少。最终模型的“权重衰减”设置为0.0005，并在组件研究期间进行相应调整。通过重放动作将所有骨架序列填充到T=300帧。 在之后，使用归一化和转化对输入进行预处理。不使用数据增强来进行公平的性能比较。### 组件研究​    我们在最终的架构中分析各个组件及其配置。除非另有说明，性能报告为仅使用关节数据的NTU RGB+D 60交叉对象设置的分类精度。#### 解缠的多尺度聚合![image-20200709001329657](https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDIuc2JpbWcuY24vMjAyMC8wOC8wNi9vQVBqVS5wbmc?x-oss-process=image/format,png)> 表1：具有不同K的STGC块的单个路径上的多尺度聚集的准确性(%)。 “掩码”是指残缺掩码$$\mathbf{A}^{\text{res}}$$。 如果K&gt;1，则GCN/G3D为多尺度(MS-)。​    我们首先通过在稀疏和稠密图上验证其在不同尺度数下的有效性来证明提出的解缠多尺度聚集方案的有效性。在表1中，我们使用STGC块（图3（b））的单独路径，分别称为“GCN”和“G3D”，后缀“-E”和“-D”表示邻接矩阵高次幂和解缠。这里，最大$$K=12$$是来自NTU RGB+d60的骨架图的直径，对于G3D模块，我们设置$$\tau=5$$。为了保持一致的归一化，我们在公式2 $$\widehat{\mathbf{A}}=\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}}$$中为GCN-E和G3D-E设置了。我们首先观察到，分解的因式分解 比简单邻接矩阵幂在K=4时可带来高达1.4%的增益，支持邻域分解的必要性。在这种情况下，残缺掩码$$\mathbf{A}^\text{res}$$部分校正了权重不平衡，将最大差距缩小到0.4%。然而，在G3D路径上的同一组实验中，窗口图$$\mathcal{G}{(\tau)}$$比空间图$$\mathcal{G}$$的密度更大，显示G3D-E和G3D-D之间的精度差距更大，这表明存在更严重的有偏加权问题。具体地说，即使添加残缺掩码，我们在K=12时也会看到0.8%的性能差距。这些结果验证了所提出的解缠聚合方案在多尺度学习中的有效性；它不仅在空间域中提高了不同数字尺度上的性能，而且在时空域中更是如此，它补充了所提出的G3D模块。一般来说，空间GCN比时空G3D模块从大K中获益更多；对于最终的体系结构，我们分别为MS-GCN和MS-G3D块分别设置$$K \in\{12,5\}$$。 #### G3D的有效性![image-20200709001552265](https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDIuc2JpbWcuY24vMjAyMC8wOC8wNi9vQW94ZC5wbmc?x-oss-process=image/format,png)> 表2：各种设置下的模型精度。 MS-GCN和MS-G3D分别使用$$K\in\{12，5\}$$。 $$†$$输出通道在折叠窗口层(图3(D),$$C_{mid}$$ 到 $$C_{out}$$)加倍，而不是在图形卷积($$C_{in}$$ 到 $$C_{mid}$$)，以维持相似的预算。​    为了验证G3D模块捕获复杂时空特征的有效性，我们使用其单独的组件逐步构建模型，并在表2中显示其性能。我们使用来自2S-AGCN的关节流作为控制实验的基线，并且为了公平比较，我们用MS-TCN层替换其规则的时间卷积层，得到了参数量变少的改进。首先，我们观察到，由于MS-GCN中强大的分离聚集作用，因式分解途径本身就可以优于基线。然而，如果我们简单地将因式分解的路径放大到更大的容量(更深和更宽)，或者复制因式分解的路径以从不同的特征子空间中学习并模仿STGC块中的多路径设计，我们观察到的收益是有限的。相反,当添加G3D路径时，我们在相似或更少的参数下观察到一致更好的结果，验证了G3D提取复杂的区域时空相关性的能力，这些相关性以前通过以因式分解的方式建模空间和时间依赖而被忽略。#### 探索G3D配置​    表2还比较了各种G3D设置，包括不同的$$\tau$$，$$d$$值和STGC块中G3D路径的数量。首先，我们观察到所有的配置一致地优于基线，证实了MS-G3D作为一个健壮的特征提取器的稳定性。我们还发现$$\tau$$=5的结果稍好一些，但在$$\tau$$=7时，由于局部时空邻域过大，聚集的特征变得过于通用，因此抵消了较大时间覆盖的好处。扩张率d具有不同的影响：(1)当$$\tau=3$$时，$$d=1$$的性能低于$$d \in \{2,3\}$$，证明需要更大的时间上下文；(2)更大的d具有边际效益，因为其更大的时间覆盖以时间分辨率为代价(从而使骨骼运动粗糙)。因此，当$$d=(1，2)$$的两条G3D路径组合时，我们观察到更好的结果,不出所料，当时间分辨率通过设置$$\tau=(3，5)$$保持不变时，我们获得了最好的结果。#### 跨时空连接![image-20200709002127999](https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDIuc2JpbWcuY24vMjAyMC8wOC8wNi9vUWJjbi5wbmc?x-oss-process=image/format,png)> 表3：比较图连接性设置$$(\tau=3，d=2)$$。​    为了证明在公式5中定义的$$\mathcal{G}{(\tau)}$$中需要跨时空边，而不是简单的、类似网格的时间自边（G3D也适用于此），我们对比了表3中的不同连接方案，同时固定了架构的其他部分。前两个设置指的是修改块邻接矩阵$$\tilde{A}_{(\tau)}$$，使得（1） 保留主对角线上的块$$\tilde{A}$$，将超对角线对角线上的块设置为$$I$$，其余设置为$$0$$；</script><p>\tilde{\mathbf{A}}_{(\tau)}=\left[\begin{array}{ccc}<br>\tilde{\mathbf{A}} &amp; \cdots &amp; I \<br>\vdots &amp; \ddots &amp; \vdots \<br>O &amp; \cdots &amp; \tilde{\mathbf{A}}<br>\end{array}\right] \in \mathbb{R}^{\tau N \times \tau N}</p><script type="math/tex; mode=display"></script><p>\left[\begin{array}{cccc}<br>\tilde{\mathbf{A}} &amp; I &amp; I&amp; \cdots I \<br>O &amp; \tilde{\mathbf{A}} &amp; I&amp; \cdots I \<br>O &amp; O &amp; \tilde{\mathbf{A}}&amp;  \cdots I \<br>\cdots&amp; \cdots&amp;\cdots&amp;\cdots I\<br>O &amp; O &amp; O &amp; \cdots  \tilde{\mathbf{A}}<br>\end{array}\right]</p><script type="math/tex; mode=display">(2)除主对角线的$$\tilde{A}$$的外的所有块均设置为I。直观地说，第一种方法生成“3D网格”图形，第二种方法在帧上包含额外密集的自边。 显然，虽然所有的设置都允许统一的时空图形卷积，但作为跳过连接的跨时空边对于有效的信息流是必不可少的。</script><p>\tilde{\mathbf{A}}_{(\tau)}=\left[\begin{array}{ccc}<br>\tilde{\mathbf{A}} &amp; \cdots &amp; I \<br>\vdots &amp; \ddots &amp; \vdots \<br>I &amp; \cdots &amp; \tilde{\mathbf{A}}<br>\end{array}\right] \in \mathbb{R}^{\tau N \times \tau N}</p><script type="math/tex; mode=display"></script><p>\left[\begin{array}{cccc}\tilde{\mathbf{A}} &amp; I &amp; I&amp; \cdots I \I &amp; \tilde{\mathbf{A}} &amp; I&amp; \cdots I \I &amp; I &amp; \tilde{\mathbf{A}}&amp;  \cdots I \\cdots&amp; \cdots&amp;\cdots&amp;\cdots I\I &amp; I &amp; I &amp; \cdots  \tilde{\mathbf{A}}\end{array}\right]</p><p>$$</p><h4 id="关节-骨骼双流融合"><a href="#关节-骨骼双流融合" class="headerlink" title="关节-骨骼双流融合"></a>关节-骨骼双流融合</h4><p>​    我们在NTU RGB+D60数据集上验证了我们在关节骨骼融合框架下的方法（表5中）。与[33]相似，我们在融合关节和骨骼特征时获得了最佳性能，这表明我们的方法可以推广到其他输入模式。</p><h3 id="与最新技术相比"><a href="#与最新技术相比" class="headerlink" title="与最新技术相比"></a>与最新技术相比</h3><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDIuc2JpbWcuY24vMjAyMC8wOC8wNi9vUWxZaC5wbmc?x-oss-process=image/format,png" alt="image-20200709002241972"></p><blockquote><p>表4：在NTU RGB+D 120骨骼数据集上的分类精度与最新方法的比较。</p></blockquote><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDIuc2JpbWcuY24vMjAyMC8wOC8wNi9vUVM5YS5wbmc?x-oss-process=image/format,png" alt="image-20200709002353026"></p><blockquote><p>NTU RGB+D60骨架数据集分类精度与最新方法的比较。</p></blockquote><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly93eDIuc2JpbWcuY24vMjAyMC8wOC8wNi9vUXJGTS5wbmc?x-oss-process=image/format,png" alt="image-20200709002414727">    </p><blockquote><p>表6：Kinetics Skeleton 400数据集上的分类精度与最新方法的比较。</p></blockquote><p>​    我们将我们的完整模型(图3(A))与表4、5和6中的最新水平进行比较。表4比较了非图和基于图的方法。 表5比较了非图方法、具有空间边和具有时空边[8]的基于图的方法。 表6比较了单流和多流方法。在所有三个大型数据集上，我们的方法在所有评估设置下都优于所有现有的方法。 值得注意的是，我们的方法首次应用多路径设计从骨架序列中学习远程时空相关性和复杂的区域时空相关性，结果验证了我们方法的有效性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    在这项工作中，我们提出了两种改进基于骨架的动作识别的方法：一种是去除不同邻域之间冗余依赖的解缠多尺度图卷积聚集方案；另一种是G3D，它是一种统一的时空图卷积算子，它直接从骨架图序列中建模时空依赖关系。 通过耦合这些方法，我们得到了MS-G3D，这是一个功能强大的特征提取器，它捕获了以前被因式分解方法建模忽视的多尺度时空特征。 在三个大规模数据集上的实验表明，我们的模型相比现有的方法有相当大的优势。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Disentangling-and-Unifying-Graph-Convolutions-for-Skeleton-Based-Action-Recognition&quot;&gt;&lt;a href=&quot;#Disentangling-and-Unifying-Graph-Conv
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
      <category term="ActionRecognition" scheme="http://yoursite.com/tags/ActionRecognition/"/>
    
  </entry>
  
  <entry>
    <title>使用Dockerfile创建Ubuntu+Pytorch+CUDA 镜像</title>
    <link href="http://yoursite.com/2020/07/02/%E4%BD%BF%E7%94%A8Dockerfile%E5%88%9B%E5%BB%BAUbuntu+Pytorch+CUDA%20%E9%95%9C%E5%83%8F/"/>
    <id>http://yoursite.com/2020/07/02/%E4%BD%BF%E7%94%A8Dockerfile%E5%88%9B%E5%BB%BAUbuntu+Pytorch+CUDA%20%E9%95%9C%E5%83%8F/</id>
    <published>2020-07-02T15:50:32.000Z</published>
    <updated>2020-10-19T16:34:58.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用Dockerfile创建Ubuntu-Pytorch-CUDA-镜像"><a href="#使用Dockerfile创建Ubuntu-Pytorch-CUDA-镜像" class="headerlink" title="使用Dockerfile创建Ubuntu+Pytorch+CUDA 镜像"></a>使用Dockerfile创建Ubuntu+Pytorch+CUDA 镜像</h1><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><ol><li>安装Docker     参考<a href="https://blog.csdn.net/LuffysMan/article/details/89393965" target="_blank" rel="noopener">ubuntu安装docker</a></li><li>安装NVIDIA Container Toolkit   参考<a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">NVIDIA/nvidia-docker</a></li><li>准备好Python-3.6.9.tar.xz</li><li>从nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04 镜像基础上搭建</li><li>安装openssh-server、python、pytorch</li><li>run镜像时加上参数—gpus all —ipc=host<h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BASE IMAGE</span></span><br><span class="line"><span class="keyword">FROM</span> nvidia/cuda:<span class="number">10.0</span>-cudnn7-devel-ubuntu16.<span class="number">04</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># LABEL MAINTAINER</span></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"ltobenull@gmail.com"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SHELL</span><span class="bash"> [<span class="string">"/bin/bash"</span>,<span class="string">"-c"</span>]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /tmp</span></span><br><span class="line"><span class="comment"># copy安装文件</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> Python-3.6.9.tar.xz /tmp</span></span><br><span class="line"><span class="comment"># 设置 root 密码</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'root:password'</span> | chpasswd \</span></span><br><span class="line"><span class="bash"><span class="comment"># 安装openssh-server 并配置</span></span></span><br><span class="line">  &amp;&amp; apt-get update &amp;&amp; apt-get -y install openssh-server \</span><br><span class="line">  &amp;&amp; sed -i <span class="string">'s/UsePAM yes/UsePAM no/g'</span> /etc/ssh/sshd_config \ </span><br><span class="line">  &amp;&amp; sed -i <span class="string">'s/PermitRootLogin prohibit-password/PermitRootLogin yes/g'</span> /etc/ssh/sshd_config \</span><br><span class="line">  &amp;&amp; mkdir /var/<span class="keyword">run</span><span class="bash">/sshd \</span></span><br><span class="line"><span class="bash"><span class="comment"># 安装python依赖包</span></span></span><br><span class="line">  &amp;&amp; apt-get -y install build-essential python-dev python-setuptools python-pip python-smbus \</span><br><span class="line">  &amp;&amp; apt-get -y install build-essential libncursesw5-dev libgdbm-dev libc6-dev \</span><br><span class="line">  &amp;&amp; apt-get -y install zlib1g-dev libsqlite3-dev tk-dev \</span><br><span class="line">  &amp;&amp; apt-get -y install libssl-dev openssl \</span><br><span class="line">  &amp;&amp; apt-get -y install libffi-dev \</span><br><span class="line"><span class="comment"># 安装python 3.6.9</span></span><br><span class="line">  &amp;&amp; mkdir -p /usr/local/python3.<span class="number">6</span> \</span><br><span class="line">  &amp;&amp; tar xvf Python-<span class="number">3.6</span>.<span class="number">9</span>.tar.xz \</span><br><span class="line">  &amp;&amp; cd Python-<span class="number">3.6</span>.<span class="number">9</span> \</span><br><span class="line">  &amp;&amp; ./configure --prefix=/usr/local/python3.<span class="number">6</span> \</span><br><span class="line">  &amp;&amp; make altinstall \</span><br><span class="line"><span class="comment"># 建立软链接</span></span><br><span class="line">  &amp;&amp; ln -snf /usr/local/python3.<span class="number">6</span>/bin/python3.<span class="number">6</span> /usr/bin/python3 \</span><br><span class="line">  &amp;&amp; ln -snf /usr/local/python3.<span class="number">6</span>/bin/pip3.<span class="number">6</span> /usr/bin/pip3\</span><br><span class="line"><span class="comment"># 安装pytorch</span></span><br><span class="line">  &amp;&amp; mkdir ~/.pip &amp;&amp; echo -e <span class="string">'[global] \nindex-url = https://mirrors.aliyun.com/pypi/simple/'</span> &gt;&gt; ~/.pip/pip.conf \</span><br><span class="line">  &amp;&amp; pip3 install torch===<span class="number">1.2</span>.<span class="number">0</span> torchvision===<span class="number">0.4</span>.<span class="number">0</span> -f https://download.pytorch.org/whl/torch_stable.html \</span><br><span class="line"><span class="comment"># 清理copy的安装文件</span></span><br><span class="line">  &amp;&amp; apt-get clean \</span><br><span class="line">  &amp;&amp; rm -rf /tmp/* /var/tmp/*</span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">22</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/usr/sbin/sshd"</span>, <span class="string">"-D"</span>]</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用Dockerfile创建Ubuntu-Pytorch-CUDA-镜像&quot;&gt;&lt;a href=&quot;#使用Dockerfile创建Ubuntu-Pytorch-CUDA-镜像&quot; class=&quot;headerlink&quot; title=&quot;使用Dockerfile创建Ubuntu
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
</feed>
